{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ffmpeg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sentence(df:pd.DataFrame) -> pd.Series:\n",
    "\tseries = df.iloc[0]\n",
    "\tseries['w'] = ' '.join(df['w'])\n",
    "\tseries['e'] = df.iloc[-1]['e']\n",
    "\tseries['a'] = df['a'].mean()\n",
    "\t# series['min'] = df['a'].min()\n",
    "\treturn series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/metadata.txt'): os.remove('data/metadata.txt')\n",
    "\n",
    "transcripts = {}\n",
    "with open('data/metadata.txt', 'ab') as f:\n",
    "\tfor lec in [1]: # chain(range(1,21),[10.5,18.5]): # [1]:\n",
    "\t\tdata = pd.read_json(\n",
    "\t\t\tf'Text/Lec{lec}.json',\n",
    "\t\t\torient='records',\n",
    "\t\t)\n",
    "\t\tdata.drop(columns=['i','t'], inplace=True)\n",
    "\t\tdata[['s','e']] /= 1000\n",
    "\n",
    "\t\tsents = sent_tokenize(' '.join(data['w']))\n",
    "\t\tindices = np.fromiter((sent.count(' ')+1 for sent in sents), int, len(sents)).cumsum() # end of sentence indices\n",
    "\t\t# for i,(s,e) in enumerate(zip([0]+indices[:-1], indices)):\n",
    "\t\t# \tassert sents[i] == ' '.join(data['w'][s:e])\t# double check correctness\n",
    "\t\tlabels = np.zeros(len(data), int)\n",
    "\t\tlabels[indices[:-1]] = 1 # exclude last index (which is the length)\n",
    "\t\tlabels = labels.cumsum() # sentence labels [1,2,...] for each word\n",
    "\n",
    "\t\ttranscript = data.groupby(labels).apply(join_sentence)\n",
    "\t\tsent_lens = transcript['w'].str.count(' ')+1 # count by space +1 (ignores contractions, numerals, etc.)\n",
    "\t\ttranscript = transcript[sent_lens/(transcript['e']-transcript['s']) > 1] # >= 60 words per minute (avoiding long pauses)\n",
    "\t\t# transcript = transcript[sent_lens >= 4] # >=4 words (avoiding short sentences)\n",
    "\t\ttranscript = transcript[transcript['a'] >= 70] # >=70% accuracy (avoiding low accruacy sentences)\n",
    "\t\ttranscripts[lec] = transcript\n",
    "\t\ttranscript.reset_index(inplace=True)\n",
    "\t\tnp.savetxt(f,(f'Lec{lec}-'+transcript.index.astype(str)+('|'+transcript['w'])*2),fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lec in chain(range(1,21),[10.5,18.5]):\n",
    "\t# mp4 (AAC) -> wav (PCM)\n",
    "\tffmpeg.input(f'Video/Lec{lec}.mp4').output(f'data/Video/Lec{lec}-all.wav', ar=22050).overwrite_output().run()\n",
    "\n",
    "\t# clips from the whole wav\n",
    "\tfor thread in tqdm([\n",
    "\t\tffmpeg.input(f'data/Video/Lec{lec}-all.wav')\n",
    "\t\t\t.output(f'data/wavs/Lec{lec}-{t.Index}.wav', ss=t.s, to=t.e)\n",
    "\t\t\t.overwrite_output()\n",
    "\t\t\t.run_async()\n",
    "\t\tfor t in transcripts[lec].itertuples()\n",
    "\t], desc=f'Lec{lec}', leave=False): thread.wait() # wait until all is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import wavfile\n",
    "# wavfile.read('wavs/Lec1-1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r data.zip data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
