Lec10-1|Do you guys still have questions about this? Basically, these three guys are at the top of a convolutional network.
Lec10-2|So there's no fully connected thing here, right? And global average pooling usually doesn't work quite as well as having a fully connected layer.
Lec10-3|But what it does do is you have all of these guys, the six-by-six patch, listening to the convolutions below it, to the feature map below it.
Lec10-4|And each one is only connected to one of the outputs.
Lec10-5|And through, so through the magic of backprop.
Lec10-7|So that means essentially, since the six-by-six patch, there's only a fixed weight of one-thirty sixth from every dot here to this dot here.
Lec10-8|Okay? And so that would be the average of these and through the magic of backprop, because this is taught to be a cat unit.
Lec10-9|All of these guys become cat units.
Lec10-10|That's how they become.
Lec10-11|So you have a plane here for every category.
Lec10-14|Then because these are trying to detect cats where they're most active is where the cat is in the image.
Lec10-17|Some people asked me after class, please ask questions during class.
Lec10-20|And then, okay, how about this clicker question? I'll bet you know the answer to this.
Lec10-21|After last, last week, after Tuesday.
Lec10-25|I don't know why it's happening today.
Lec10-29|Here's usually up to 60, some Everybody voted.
Lec10-31|Yeah, it looks like it is okay.
Lec10-32|Going, going, going gone.
Lec10-36|And we talked about it last time.
Lec10-38|And I said either use soccer ISOC and I decided to dissect.
Lec10-39|I'm going to try, I made a little animation.
Lec10-40|Let's see if this helps.
Lec10-41|So this is a one-by-one convolution.
Lec10-44|So Casey is calling for help, but he doesn't really need it.
Lec10-45|Let's see if we can get them distracted.
Lec10-51|If you want that, you can have it.
Lec10-55|So that's gonna give you one number.
Lec10-56|And it's blue, meaning we repeat that for the first row.
Lec10-57|And we get now 56 numbers, not 64 numbers, and the second row and all the way up to the top row.
Lec10-60|And eventually if we do this 32 times, we get a 32 by 56 by 56 block.
Lec10-61|Does that make sense now? Yeah.
Lec10-63|That's what it says right here.
Lec10-64|When each filter has a size of one by one by 64.
Lec10-65|Okay? So you could do it ten times instead of 32, and that would give you a ten by 56 by 56 block.
Lec10-68|From, I mean, fewer features, they're going to probably mean poorer performance.
Lec10-69|But but you can control the amount of stuff.
Lec10-83|I left my keys at Pet Smart yesterday, so you're not alone.
Lec10-86|This is called the inception module.
Lec10-87|And by notice the number of filters here goes up.
Lec10-88|So you can have lots of one-by-one filters and a smaller number of three-by-three and a smaller number of five-by-five.
Lec10-91|You have fewer of them, but this way by having all these one-by-one convolutions here again, you can control the number of numbers and control how much computation you're doing.
Lec10-92|And that was what allowed them.
Lec10-93|To end up with a pretty deep network, 22 layers, but with only twice the number of numbers as AlexNet.
Lec10-94|So the width of these modules, meaning how many features you are computing, starts small and gets bigger.
Lec10-95|But that makes some sense, right? Because you can have relatively universal local features to recognize this thing.
Lec10-97|Because they're relatively universal.
Lec10-98|They're useful for all sorts of things, are features are useful for all sorts of environments.
Lec10-100|It used to be you would train a whole new system.
Lec10-101|If you're doing outdoor images versus indoor images, we have light adaptation in our retina and we are able to recognize lots of different things.
Lec10-104|And again, it's a lot like VGG net and that they just keep repeating the same structure over and over again, sometimes with more features than before.
Lec10-105|Yeah, chew on that.
Lec10-108|This is a one to one computation.
Lec10-109|So all the weights here are one fixed.
Lec10-111|And that allows the gradient to pass back through the network.
Lec10-112|And so you can have a very deep network.
Lec10-113|And it turns out the deeper you are, the better it works.
Lec10-114|Okay? Which is a little disappointing.
Lec10-116|You don't have to be really smart to add more layers.
Lec10-120|Residual connections were state of the art at the time.
Lec10-122|And so that can be learned.
Lec10-123|And so you can wait how much it's gonna be a copy and how much it's this residual thing.
Lec10-124|And we came up with so that was already around.
Lec10-125|And you can do this with transformers as well.
Lec10-126|But we came up with a really simple idea which was we called it reads zero, where you have a residual network like this with an Alpha there and you start out alpha at zero.
Lec10-127|So all you have is the, is the one-to-one connections.
Lec10-128|And the physics student whose idea this was trained to 10,000 layer network in three days on a laptop using that idea.
Lec10-130|You initialize it to zero and then you train.
Lec10-133|Yes, you have to make the size match.
Lec10-134|Or where they're dashed lines here is there's one where it went from 2506512.
Lec10-136|So you can change the size by having a fixed linear weight matrix.
Lec10-137|Okay? So it could like average or it could expand.
Lec10-143|Okay, so now we want to talk about visualizing what these filters are.
Lec10-145|So I could just take the activations of the hidden layer and multiply them back, some backpropagating the activations now to the PCA layer, and then use that to reconstruct what I get.
Lec10-148|That's why it's an approximation.
Lec10-149|But it gave us these kind of global, weird-looking receptive fields of the units.
Lec10-151|So we're gonna do some things that sort of mix those ideas.
Lec10-152|There are two ways to visualize.
Lec10-153|One is to project activations back to pixel space, and that's essentially what we did.
Lec10-155|Okay, so here's some references for those that like to read up.
Lec10-157|We're going to take this feature maps somewhere in the network.
Lec10-160|We want to know what that guy is responding to in the input.
Lec10-163|So again, we're propagating activation backwards through the network.
Lec10-165|We've got feature maps that have weights, right? F is our filter, let's call them.
Lec10-166|They become rectified with electrified linear function and then their max pooling.
Lec10-168|And those are called switches.
Lec10-172|That'll still be zero.
Lec10-174|It's a linear function.
Lec10-175|Then we use the transpose of the weights to propagate the activation back just like when you propagate the deltas back.
Lec10-177|So here's a visualization of that.
Lec10-178|So here's a feat rectified feature map.
Lec10-179|And it's got four places where it's maxed.
Lec10-186|You don't have those anymore? I mean, you could keep those.
Lec10-187|But I don't believe they do.
Lec10-188|They just take the numbers from above and multiply them through the.
Lec10-193|Notice that these numbers, these, the heights of these bars, aren't exactly the same as the heights of these bars.
Lec10-194|So it's taking the activation from above that's computed by using the, the, the way the transpose of the weights.
Lec10-195|So we start again.
Lec10-196|We want to know what that feature map is doing.
Lec10-197|And we start by taking that guy.
Lec10-199|That's where we start.
Lec10-200|We know what that number was.
Lec10-201|And then we're going to use this process of inverting the rectified linear function and on pooling the max pools and multiplying by the transpose, etc.
Lec10-202|This is what they do.
Lec10-203|And this was again Rob Fergus doing this.
Lec10-204|Okay, So this is, this is AlexNet.
Lec10-206|These are the layer one filters.
Lec10-207|There are 11, I think the original AlexNet used 11 by 11.
Lec10-208|So these are just the weights.
Lec10-209|There's no mystery here.
Lec10-210|You're just plotting the weights like you guys did.
Lec10-211|And you get these Gabor Filters.
Lec10-212|Again, a good war is a sine wave restricted by a Gaussian envelope.
Lec10-214|They roughly match what is seen and v1.
Lec10-215|These are pretty low frequency sine wave.
Lec10-216|This is a higher frequency sine wave.
Lec10-218|So it's capturing more details.
Lec10-219|But these were just learned.
Lec10-221|That's again, what your retina sends back to.
Lec10-222|Your cortex, is the difference between red and blue receptors, and red and green receptors and blue and yellow ones.
Lec10-224|And we think that probably for telling ripe fruit better in the trees.
Lec10-225|So that there's no mystery here and there's no, This is just the weights to the first layer.
Lec10-226|Then what we're gonna do is take the validation set from ImageNet, the 50,000 examples we're going to push each image through the network.
Lec10-227|And poor guy, you want to help them.
Lec10-231|And then this thing I've just explained to you is called deacon net to project back to pixel space using the pooling switches particular to that activation.
Lec10-232|Okay, the other thing we're gonna do is keep track of the original, excuse me, patch in the image that drove this filter.
Lec10-236|And these are the, I think these are the actual patches that drove it.
Lec10-237|And if you look at these, like Yeah, it's hard to tell here, but okay.
Lec10-241|You see that this is angled this way and this guy is looking at edges in the image.
Lec10-243|Nine patches that drove these filters the most.
Lec10-244|Some of them don't look exactly like they match, but I guess they do.
Lec10-246|this one here is angled this way and it go corresponds to this guy angled this way.
Lec10-249|And you can see that there's starting to be some structure already.
Lec10-250|There's things that look kind of like round things, maybe eyes.
Lec10-251|And these, as you remember, as we go deeper in the net, we have larger and larger receptive fields.
Lec10-253|And this is the computed thing using DICOM net.
Lec10-254|And you see there's kind of looks like a monkey face right there.
Lec10-256|So it's a kind of view of invariances.
Lec10-257|These are what these are what you computed by DICOM net.
Lec10-258|And these are the actual patches that drove them.
Lec10-259|So if you go back and forth here, this is why this file is so huge.
Lec10-260|You can see that there's quite a bit of variability in what's driving these.
Lec10-262|Alright? And then you go, again, the receptive fields get bigger as you go up.
Lec10-263|And this is somehow the label on this has gone, but this is layer three.
Lec10-264|And you see they're starting to get bigger and they're capturing larger fractions of the image.
Lec10-270|It's not so much the red is the edges That's putting together.
Lec10-272|They're starting to get a little bit more categorical.
Lec10-273|Then layer four, top nine.
Lec10-275|And now we're seeing this.
Lec10-277|I used to say 120, but I think there's 117 actually different dog breeds.
Lec10-281|So these are getting more and more complicated here.
Lec10-282|This is obviously going to feed into the keyboard category so that the organization of the features is becoming more tuned to what you're trying to map them into.
Lec10-284|And now we're getting all these white dogs up here.
Lec10-285|I'm getting a lot of keyboards over here, et cetera.
Lec10-287|So you can take a guy here and figure out what part of the image is driving him.
Lec10-288|By working your way backwards.
Lec10-290|So we took all we I say we but it wasn't me.
Lec10-291|You take all 57, 50,000 holdout set Images, validation set images for ImageNet.
Lec10-292|Run each one through the network for a particular feature and a particular layer.
Lec10-293|You're going to have a feature map and you record what the maximum activation was.
Lec10-294|Out of those 50,000.
Lec10-295|We're picking the nine that drove that maximally.
Lec10-298|The receptive fields for higher layers are bigger, not smaller.
Lec10-299|They get bigger as you go up.
Lec10-300|So that's in front of your temporal lobe.
Lec10-301|You've got to, you've got to Jennifer Aniston neuron.
Lec10-302|If you've watched the Jennifer Aniston movie lately.
Lec10-304|And it makes sense because we're trying to, at the end, what we want the network, what the magic of backprop pass to do is the last layer is a Softmax.
Lec10-305|It's a linear classifier.
Lec10-307|So what backprop has to do is make everything linearly separable by the last layer before the softmax.
Lec10-309|So backprop is the, gives you the partial derivative of the error with respect to the weights.
Lec10-310|But you can also use backprop to compute the partial derivative of an output with respect to the input.
Lec10-311|So you're trying to see what makes this guy go uphill, like the cat neuron.
Lec10-313|And maximize and keep backpropagating the input, adding to the input pixels.
Lec10-315|You have to add some more constraints.
Lec10-316|Like you want neighboring pixels to be correlated.
Lec10-317|But I keep backpropagating from the banana output to maximize that output.
Lec10-318|Changing the input to maximize that output.
Lec10-320|This is called Google Deep Dream.
Lec10-326|Alright, so that's two ways to visualize things.
Lec10-327|Deep Dream is not all that useful, but it does make cooler.
Lec10-328|Okay, that's all for that.
Lec10-329|Any questions, two ways of doing it.
Lec10-332|This particular feature the most.
Lec10-333|You can also do this with cats and find out what cats like to look at.
Lec10-334|You show them a lot of record from a neuron.
Lec10-336|You can do this with psychology sophomores as well.
Lec10-337|Student who's committee I was on had showed white noise images to human subjects, which are always sophomores in psychology and told them that there was a face in the image.
Lec10-338|And about half of them had faces in them.
Lec10-339|There were no faces.
Lec10-340|But they would thinking that there were, they would push the button about half the time when they thought they saw a face.
Lec10-341|And you take all those images and add them together and you get two big black spots here and a black spot here.
Lec10-342|That's the internal representation of a face for a UCSD psychology software.
Lec10-344|So what this is, is, here's this panda.
Lec10-345|And I'm going to take the sign of the gradient to go basically uphill.
Lec10-346|I can have the output.
Lec10-347|So this is the input.
Lec10-348|This is the output nematode.
Lec10-349|And we're taking the gradient of that and creating this image.
Lec10-350|So it's like Google Deep Dream.
Lec10-351|And we're figuring out how we would change the pixels to make it a nematode.
Lec10-352|And adding a tiny amount of that to the given image.
Lec10-353|And to us it looks the same.
Lec10-356|We don't care exactly what the output is, but you can do this for any category and make it think it's that category.
Lec10-358|You're just taking the sign of the gradient of the panda output with respect to the cost function.
Lec10-359|And then adding a tiny amount of that to the input.
Lec10-360|And it's similar, they are similar.
Lec10-361|Other techniques, these are targeted.
Lec10-362|So it's trying to make a adversarial image that you can add to this school bus.
Lec10-363|Add to that, add to this, add to this, add to this, and make it say ostrich.
Lec10-364|This may seem like a property of deep networks, but it's actually a known property of linear networks.
Lec10-365|So in a linear network, it's especially simple.
Lec10-371|Take the weights for an eight and you just add a tiny bit of them to the two.
Lec10-372|Because you've got 28 by 28 image, so you've got about 900 numbers.
Lec10-373|You just have to add a tiny bit of each weight.
Lec10-375|And it'll turn on the aid.
Lec10-376|So it doesn't have to be a deep non-linear network.
Lec10-377|It's actually the linear part that makes this adversarial examples possible.
Lec10-380|We backpropagate that gradient to the input and get some pixels.
Lec10-381|And then we just add a small amount of those pixels to the input image.
Lec10-384|So notice these are different for different inputs.
Lec10-386|All right, you're good.
Lec10-390|So you could, e.g.
Lec10-391|it turns out you can take a stop sign and add some white rectangles to it and make it think make a self-driving car.
Lec10-392|I think it's a speed limit sign.
Lec10-394|Of what the consequences of that is in the real-world.
Lec10-397|Yeah, in this case.
Lec10-398|But what's weird and surprising about all this is that these examples tend to generalize to different continents that weren't, that you didn't have access to.
Lec10-399|They tend to screw up other convolutional networks like you can take VGG 19, compute these adversarial examples, give it to, to Google.
Lec10-400|And Google will make mistakes too.
Lec10-402|So here's a clicker question.
Lec10-403|I'm fine tuning the classic this a thought question.
Lec10-406|I won't show you the let's see.
Lec10-407|Does it show up again? Yes.
Lec10-409|I won't show you that.
Lec10-410|I'll let you figure this out.
Lec10-413|With the adversarial examples on the previous slide, using the correct labels should improve performance.
Lec10-414|Improve the performance on the training set.
Lec10-415|Both a and B.
Lec10-420|You stop taking the class.
Lec10-421|You don't get to vote.
Lec10-422|Yes, you can have kidding.
Lec10-427|So we could discuss this.
Lec10-428|I guess we should discuss it.
Lec10-430|Talk to your neighbor and see what you think.
Lec10-433|We re to try again.
Lec10-437|We're up to 41.
Lec10-438|Okay, I think more people voted last time, but there's some more going going going gone.
Lec10-440|Let's see what we said.
Lec10-445|In fact, one of my ex-students does this all the time.
Lec10-447|She's at Santa Barbara now.
Lec10-448|The system Prof google.
Lec10-449|But she worked on fixing networks by training them on adversarial examples.
Lec10-452|What I think we ought to do is be more biologically inspired because they don't fool us.
Lec10-455|It's a good question.
Lec10-456|It is kind of like augmenting your training set.
Lec10-457|To add these to the training set.
Lec10-458|It seems possible it could improve performance, but it's not directly trying to do that, right? It's trying mainly improved performance on these adversarial examples.
Lec10-459|There's a whole arms race going on with people coming up with new ways to do this.
Lec10-461|And it turns out there's an attack that beats that, kind of surprisingly.
Lec10-462|So there's this whole arms race.
Lec10-463|Somebody who will come up with a new way to fool a continent.
Lec10-464|Then a few months later somebody will come up with a fix for that.
Lec10-465|But I didn't really answer your question.
Lec10-466|And that's because I'm not totally sure what the answer is.
Lec10-472|So it's possible that could actually help.
Lec10-473|So I could be wrong here.
Lec10-476|This one thing in the middle here, added to all these things on the left gives the results.
Lec10-478|What they do is they find where the hyperplane is, it separates out these and then they try and move the other way.
Lec10-479|And they keep doing that with multiple images.
Lec10-480|And they end up with this weird thing here that screws up all these images.
Lec10-481|And these also do generalize well to other networks that were different than the network that was trained on, created on.
Lec10-484|Whatever that is becomes a griffin.
Lec10-489|Okay, So I'm not gonna go into depth on that.
Lec10-493|And you get this image.
Lec10-494|You can see it's a little orange in the middle.
Lec10-496|These are images that don't look like anything.
Lec10-497|So the title of the paper is high confidence predictions for unrecognizable images.
Lec10-498|And there's a link to it down there.
Lec10-500|And down here what they're doing is using, using genetic algorithms to combined basis images in different ways to fool the network.
Lec10-501|Thinks this is a baseball and you can see why it might think that it looks like the stitches on a baseball.
Lec10-502|And thinks this penguin, etc.
Lec10-504|Although it doesn't look like any of these are doing that.
Lec10-506|And then you combine their parameters and do some, some mutation.
Lec10-507|So that's crossover and mutation like genetic algorithms.
Lec10-513|I'm not sure about that.
Lec10-514|I do know that the better these, the more accurate these networks are, the better they predict activations and monkey cortex for the same images.
Lec10-515|So you can, these are, I think, did I say this already in here that this already the best model we have, the temporal lobe.
Lec10-517|And you can learn a linear, just use linear regression between layers of the network activations and monkey neural recordings and learn a mapping between those.
Lec10-518|And it turns out earlier layers in the network are better for earlier layers in the monkey, and later layers are better for later layers.
Lec10-519|And the better the network is at, the more accurate the network is on ImageNet, the better the predictions it makes.
Lec10-520|You learn that in a big set of images.
Lec10-521|And then you show the monkey in the network and new image, neither one's seen before.
Lec10-522|And the network does a pretty good job of predicting the monkey activations.
Lec10-523|And they also are good at predicting fMRI on human heads for similar things.
Lec10-525|So we could talk about what are some possible defenses? What do you think? Yeah.
Lec10-526|Just run a network.
Lec10-529|Yeah, that might work, but the activations are still voting for something, right? So he's thinking will translate the image a little bit.
Lec10-530|And the activations are no longer in the right place, but the activations were put there, particularly to drive a particular output in a targeted attack.
Lec10-533|Noise is not calculus.
Lec10-535|People have tried this or their ideas.
Lec10-539|And we already yeah.
Lec10-540|Rotation of the image.
Lec10-545|They're not aligned with where they were supposed to be input.
Lec10-549|So you're taking multiple crops essentially and having them vote for whatever.
Lec10-550|Having multiple crops and then integrating over those.
Lec10-551|And that helps to be more biologically plausible, might be a good idea.
Lec10-552|Yeah, there's some other ideas.
Lec10-555|The Adam optimizer is pretty popular.
Lec10-558|So this is just a graph of what happens when you start with a large learning rate and slowly, slowly make it smaller.
Lec10-559|Remember what's happening here is you're making big jumps across the bottom of the bowl.
Lec10-560|You lower the learning rate, you make smaller jumps.
Lec10-561|And that's going to have diminishing returns.
Lec10-563|I don't know if anybody uses these.
Lec10-566|You want that because it's got identify different images differently.
Lec10-567|And the hidden units are fairly sparsely activated, right? So this hidden unit activates for these two images, but not much for other ones.
Lec10-568|So that's good training result.
Lec10-569|This would be bad, right? So here these hidden units are highly correlated across a whole bunch of images.
Lec10-570|So they're not going to help distinguish among the images.
Lec10-571|And you want, you want.
Lec10-573|And what you want is something that looks structured, not too noisy, not to simulate, similar to one another.
Lec10-574|No structure at all.
Lec10-575|So, but I highly recommend Andre neuropathies blog for how to train a neural network.
Lec10-576|Like this, is first thing to say is become one with your data.
Lec10-579|And in their report they talked about how it was digits.
Lec10-580|They weren't they didn't look at the data.
Lec10-582|You can visualize the features.
Lec10-584|But I think the most important thing here is the bottom one.
Lec10-586|If you've got your network setup properly, you train on a small subset of the data.
Lec10-587|You ought to be able to drive the error down to zero.
Lec10-588|That's a simple way to just check that you've got everything.
Lec10-589|Everything is groovy and you're ready to train on a bigger amount of the data.
Lec10-590|This takes a lot less time because you're training on a small subset might finish in 10 min instead of 10 h.
Lec10-591|So that's a good thing to do.
Lec10-592|If it's not working.
Lec10-593|If training diverges, your learning rates probably too high.
Lec10-599|It could be built-in, but we didn't, in this case, rotation, not-so-good.
Lec10-600|We looked at the structure of several deep networks.
Lec10-601|We looked at how to visualize the features.
Lec10-602|We looked at adversarial examples and gave you some training tips.
Lec10-603|And these, these are from his original.
Lec10-604|I didn't make these up.
Lec10-605|These are from Rob's initial thing.
Lec10-608|Okay? So, so far we've been doing.
Lec10-611|They have a prior and the network structure that makes them good for images.
Lec10-612|What do we do about things like words? Sentences? Originally the first thing to do with sentences is to use recurrent networks.
Lec10-614|Again, these chapters and dive into deep learning might be off by one.
Lec10-615|But go ahead and look at those.
Lec10-616|So times important for everything.
Lec10-617|You know, time is God's way of keeping everything from happening at once.
Lec10-618|And so how do we represent time in these networks? There are two approaches.
Lec10-619|One that's coming back into favor recently, mapping time into space.
Lec10-620|And what I mean by that is, if I have a sentence like the cat sat on the mat, I would have the cat sat on the mat as input to my network.
Lec10-623|It's not a feed-forward network.
Lec10-624|You're not like an amoeba stimulus response, stimulus response.
Lec10-626|Your brain is in a new state.
Lec10-627|Hopefully your weights are changing.
Lec10-628|I am changing your brain right now, hopefully in a good way.
Lec10-629|So the first thing to think about is mapping time into space.
Lec10-632|And it would pronounce the end.
Lec10-633|And then you'd shift that over by one and pronounce the a, shift that over by one and pronounce the end.
Lec10-634|And again, you've got this fixed width problem.
Lec10-636|If you need to know stuff that came way before.
Lec10-637|Like there was quite a breeze.
Lec10-638|John, love to stand in the wind.
Lec10-639|How do you know it's wind and not wind? You have to know.
Lec10-640|A long time ago he was talking about the breeze.
Lec10-643|It's all these natural language.
Lec10-645|They're just trained to predict the next word.
Lec10-646|And feed forward networks generalize this by adding one or more layers of hidden units.
Lec10-647|So that works for simple problems.
Lec10-648|Doesn't work very well for arbitrary length items, but transformers have changed all that.
Lec10-650|It's got a bunch of fancy stuff.
Lec10-651|But you just replicate that for every different word in the sentence.
Lec10-652|So you can have arbitrary length sentences.
Lec10-653|That surprised me because I like to think about how the brain works and we don't generate new brain.
Lec10-654|When the sentences or longer.
Lec10-655|It took me awhile to get on board with those.
Lec10-656|But you have the same weights everywhere and since you're just, and then they interact with each other through these attention mechanisms.
Lec10-658|But the original stuff was mapping time into state.
Lec10-659|And that what that means is you have memory of what previously happened by simply keeping the activations of the units around.
Lec10-660|So this is one of the original early versions.
Lec10-662|You have the output layer connected to itself, and the output feeds back on the hidden layer.
Lec10-665|Meanwhile, the hidden layer is changing.
Lec10-666|And that's the network state.
Lec10-667|So we're mapping time into state.
Lec10-668|So historically in the '80s, two variants were commonly used.
Lec10-671|And he just copied the output, the output feed into the hidden layer.
Lec10-672|And the way he did that was by simply copying the output back to the input and then having a set of weights that went into the hidden layer.
Lec10-673|And then Jeff Elman, also UCSD.
Lec10-674|He's passed now, but he was a giant in our field.
Lec10-675|He copied back the hidden unit activations and learn to set of weights this way.
Lec10-676|So he called them simple recurrent networks.
Lec10-677|Everyone else called the mailman nets.
Lec10-678|And this is how it was actually implemented.
Lec10-681|But this is, this is an approximation to backpropagating all the way back through time.
Lec10-682|This is called BP t0, t1 for backpropagation through time, one step.
Lec10-684|So you get an error at the output.
Lec10-685|You propagate the deltas back to here.
Lec10-686|And now I change these weights and these weights and these weights.
Lec10-687|But this one-to-one copy back stays there.
Lec10-690|And you can't predict the next word.
Lec10-691|You can have a good idea.
Lec10-692|Like I like my coffee with cream and dog.
Lec10-694|So you can use these to predict the next word.
Lec10-695|You could use them to generate a sentence.
Lec10-697|And now you're generating the words.
Lec10-698|You can use them to recognize sequences.
Lec10-699|So you can imagine you have a finite state machine that recognizes the variables in your programs and says Yes, Okay, That's a legal variable.
Lec10-700|You can do stuff like that.
Lec10-702|And you need the video because you can't always tell whether an action is from a single frame.
Lec10-703|And then there's sequence transformation.
Lec10-704|Speech to text or English to French, were using a deep network right now that's translating my sentences into these words at the bottom.
Lec10-705|And you can also imagine, you may not know this yet, but you can have them learn to program.
Lec10-707|So when I'm walking.
Lec10-708|There's an oscillation going on in my brain where I'm tensing one muscle and then releasing, tensing and releasing, tensing and releasing.
Lec10-712|So they're like cows of the sea.
Lec10-713|They get some food in their stomach and then they go under a rock and they chew it up.
Lec10-715|So they can oscillate, which is good for motor control.
Lec10-716|They can settle to a stable state.
Lec10-718|They're intelligent and they're really old.
Lec10-720|But you're accessing a memory from some features of that memory.
Lec10-721|You settled on or he settled on Ronald Reagan.
Lec10-722|And they can be behave systematically and they're doing that in spades right now.
Lec10-723|And they can behave chaotically, which may be useful actually.
Lec10-726|Because the on is the unit in most sciences like electron and physics and neuron and neuroscience and person and psychology.
Lec10-730|So we might want to turn as we want to model sequences, we might want to turn a sequence of sand pressures into a sequence of word identities.
Lec10-731|And elements approach is useful when there's no separate target sequence.
Lec10-733|He used these little tiny networks, but he was doing exactly what they're doing today.
Lec10-736|I'm going to, you're starting to wiggle a little bit.
Lec10-738|Today is your programming assignment is due.
Lec10-739|I'm gonna, it's gonna be in a weird place.
Lec10-741|That's the room they could find.
Lec10-742|And I will review.
Lec10-743|I'll basically go through all the slides that we've seen so far really fast.
Lec10-744|Or try and give the log of my presentation so far.
Lec10-745|And I think the room is 122, but I'll put it up on Piazza starting at 07:00.
Lec10-746|I will record it.
Lec10-747|I'll put it on Zoom and I'll record it so you can watch it later.
Lec10-748|If you have to go bowling tomorrow night or something.
Lec10-749|Then the actual midterm is in this room, 01:00 Saturday.
Lec10-750|You can bring a page of notes 8.5 by 11 with notes on one side.
Lec10-751|And you can write really small, but you can't bring a magnifying glass.
Lec10-752|And it could be typed or it could be handwritten.
Lec10-753|Um, and there tomorrow night when I go over it, I'll probably go over an old exam and we'll see how that goes as well.
Lec10-754|So usually goes it's scheduled 7-9, but usually we go to ten and then I'm about to drop dead and I go home.
Lec10-756|So see you tomorrow.
Lec3-10|If you're following along at home on the podcast.
Lec3-11|We just did a clicker question to try it out.
Lec3-12|So I haven't said anything really.
Lec3-15|And the logistic regression uses the logistic function.
Lec3-16|And we motivated that logistic function by thinking of two categories as Gaussians with equal distributions, equal variances.
Lec3-20|And with a little manipulation, we got that down to a logistic function where it's a function of the log ratio of the probability of class one, class two as the argument to the logistic.
Lec3-23|Okay? And then we did this little thing.
Lec3-25|A lot of things are Gaussian.
Lec3-27|But a lot of things, we can assume our data is Gaussian.
Lec3-28|And we need to learn the weights.
Lec3-29|And there's no closed form formula for that.
Lec3-31|And last time I just talked about how many things that had been learned by gradient descent.
Lec3-32|So today what I'm gonna do is derive learning rule, assuming the objective function.
Lec3-33|We're trying to, using gradient, the schema for gradient descent, assuming the objective function is squared error, mean squared error.
Lec3-34|And in fact, that's what we did back in the 80s.
Lec3-36|So we're going to start with the mean squared error, plug it in, take the derivative with respect to the weights.
Lec3-39|So a is the weighted sum of the inputs to the network that's down at the bottom here.
Lec3-44|We're trying to go downhill in our objective function to get us to get to a minimum.
Lec3-46|And anything, any other, any questions about this? This is just, this should look very familiar.
Lec3-48|So I'm going to not do this on the board, I'll do it on the slides.
Lec3-50|And it's the average of the derivative for every pattern.
Lec3-51|Okay? Because the derivative of the sum is the sum of the derivatives.
Lec3-55|Remember? And just do it this way because the notation is simply alright.
Lec3-56|So we're going to just look at one component of that sum.
Lec3-57|And so taking the derivative of this using the chain rule, the two comes down, cancels with this two.
Lec3-58|We get T minus Y and then the derivative of t minus y with respect to the parameters.
Lec3-59|So that's just the chain rule that should look familiar as well.
Lec3-60|And again, the derivative of the sum is the sum of the derivatives.
Lec3-61|So we've got these two components here.
Lec3-62|This is all in the slides that are posted on Piazza.
Lec3-64|Might be better to train, follow, but okay, go ahead, keep writing things down.
Lec3-66|Then we just have this one component here.
Lec3-68|And because g of a is a, then it's just this.
Lec3-69|Right now it isn't.
Lec3-77|That's just the chain rule.
Lec3-79|At that point, whatever a is, if it's minus four, will be a slope of near zero.
Lec3-80|If it's plus four or greater, It's a slope of near-zero in-between.
Lec3-82|Okay? Because we'll see why that is in a moment.
Lec3-84|And that should look familiar because we did that for linear regression.
Lec3-85|So we've got this, we take this derivative and move it inside.
Lec3-87|So that's just x sub I.
Lec3-88|And right, and because the other component to the sum, while they include variables, when you're doing partial derivatives, you assume all the other variables are constant.
Lec3-90|Gradient descent becomes this guy, which that's batch gradient descent.
Lec3-91|So you go through all the patterns, you compute this number for each pattern.
Lec3-92|You sum it up, you divide by the number of patterns to get an average.
Lec3-93|And then you have a step size Alpha that, and now you're going downhill and the error with respect to that parameter.
Lec3-95|So it's minus, minus is a plus.
Lec3-97|You just computed g of a, then you were just a subtraction and a multiplication away from the derivative.
Lec3-98|The gradient ends up looking like that.
Lec3-99|And that is what we did for many years in the eighties because we didn't know any better.
Lec3-100|And you'll see why.
Lec3-101|This is not a good idea in the next lecture after this one.
Lec3-102|But what's wrong with this? It looks a lot like the delta rule.
Lec3-103|So what could be problematic with this, this rule? It turns out it's slow.
Lec3-108|Then it goes up, then it's flat again.
Lec3-110|So that's what made it slow.
Lec3-112|We did that and it worked better, but we didn't know why.
Lec3-114|And our friend in row four there, give us the answer.
Lec3-117|And the learning rule you drive in your homework should have no slope term in it.
Lec3-118|It's just T minus Y times x psi.
Lec3-119|What's going on is that MSE is the wrong objective function for logistic regression.
Lec3-120|It's perfect for linear regression, but not logistic regression.
Lec3-121|And the right one is cross entropy, which leads to the delta rule.
Lec3-123|Why that's a good thing to do.
Lec3-126|So God, now it's talking to me again.
Lec3-127|If I can get that to stop.
Lec3-131|And it replaces that threshold function with the sigmoid, which allows us to go from a bang-bang classifier to one that gives us a probability of category membership.
Lec3-132|So in particular, if the weighted sum of the inputs is zero, meaning that the ratio of the Log likelihoods are equal.
Lec3-133|If you think about it as two Gaussians, then the classifier can output a 1.5, meaning it has no idea.
Lec3-134|That seems like a good, good thing to do.
Lec3-135|And again, for logistic regression, there's no closed form formula.
Lec3-137|We have to use gradient descent to learn the weights.
Lec3-138|And yeah, okay, Here's another clicker question.
Lec3-142|Because I haven't set things up.
Lec3-143|And the answer is not E, The Joker.
Lec3-144|And most of you got that trick.
Lec3-146|There's about 70 of you that need to buy a clicker if you want to get the credit for participation.
Lec3-147|It doesn't matter if you're a writer on.
Lec3-148|If you get that credit, I'll bump your grade up.
Lec3-149|So you should get a clicker.
Lec3-151|Going, going, going gone.
Lec3-155|The picture you should have in your mind for a softmax regression is you have multiple outputs and still a single layer of weights from the input to the output.
Lec3-157|Okay? And usually have as many outputs as you have categories.
Lec3-159|And then we're going to use a different activation function that turns the outputs into probabilities.
Lec3-160|And that's so again, first thing we do is we calculate the weighted sum of the inputs.
Lec3-161|And that's very small on my screen, should be a little bigger.
Lec3-162|And you doing big in that.
Lec3-163|Ambiguity is a great word.
Lec3-164|One of my post-docs came up with it means to make bigger.
Lec3-165|Again, we're going to use x for the bias, but now there's two indices on the bias weight depending on each output is going to have a separate bias that typically will end up being the frequency of that output.
Lec3-166|So if one output is more frequent than the others, you'll have a higher bias on that.
Lec3-167|Then we can compute that very efficiently.
Lec3-168|Just doing matrix vector multiplication.
Lec3-169|And then our activation function is this one which is called the softmax activation function.
Lec3-170|And notice that it doesn't matter whether a is positive or negative.
Lec3-171|This output is still positive.
Lec3-172|And furthermore, since the sum of all of them is one, it, it can be a probability distribution.
Lec3-173|Okay? So that's called the softmax distribution.
Lec3-174|This is generally a good thing if your output categories are mutually exclusive.
Lec3-177|It kept switching back and forth, didn't know what to do and it killed the woman.
Lec3-178|So let that be a lesson to you.
Lec3-179|You might want to use the logistic outputs any way.
Lec3-180|And that way the outputs are independent of one another.
Lec3-181|And you can have both a bicycle and pedestrian.
Lec3-183|Tagging the image with some words that describe the image.
Lec3-184|If you're just trying to categorize it as one thing or another than softmax is good.
Lec3-185|So if you go to work for Waymo, remember that? Okay.
Lec3-186|So now we can remember, we can think of the logistic as being the probability of category one given the input.
Lec3-187|Now we can think of this as the probability of category k given the input, which in which the numerator is e to the h k.
Lec3-188|And this is called a softmax and it sets up a competition between the units.
Lec3-190|And I'm going to make you smaller.
Lec3-191|Sort of like what we used to call a winner-take-all.
Lec3-192|The biggest, a sub k will be the biggest output.
Lec3-193|And if you have to categorize it, you typically will pick the largest output.
Lec3-194|So you can have, when you're training a network like this, you have a loss that's going down.
Lec3-195|But you can also measure correctness or probability of correct by taking the maximum output and saying that's your answer.
Lec3-196|And then you can use that to say if you're right or wrong and get a percent correct.
Lec3-197|That's what you have to do in your homework.
Lec3-198|And again, this can also be trained by the delta rule.
Lec3-199|I won't derive that because you don't have to do it for homework.
Lec3-200|We've seen four kinds of neural networks so far, linear networks, which is linear regression.
Lec3-201|And you get, you get it turning, producing a line.
Lec3-202|Depending on the weighted sum of the inputs.
Lec3-203|We've seen perceptrons, which produces this off on response.
Lec3-206|That's just a slight change in conceptualization.
Lec3-208|We have a smooth output that gives us a probability instead of a category decision.
Lec3-211|Even though I proved it to them in email.
Lec3-212|Alright, so here's another clicker question.
Lec3-214|The weight matrix W has a number of entries equal to the number of nodes in the network.
Lec3-215|The number of edges in the network graph, the number of nodes plus the number of edges.
Lec3-216|You can't say the relationship depends on the problem you're solving.
Lec3-217|And we still got about 15 more people to vote.
Lec3-219|There's 30 clickers in the room.
Lec3-223|So I would make you turn to your neighbor, but this kind of a silly question.
Lec3-225|Ta came up with it.
Lec3-228|So what is the answer class? They're ready, set, go.
Lec3-230|What's wrong with a, the number of nodes in the network.
Lec3-234|And you can think of each row of the weight matrix as being the weights to one of the outputs.
Lec3-235|So you really need d plus one times c to label all of these, to have a variable for all these weights.
Lec3-237|And you can say, If you, if you know what the dimension of the input and the dimension of the output is, does that make sense to everyone? There were quite a few people who said a, C and even dy.
Lec3-241|Remember if you have a question.
Lec3-242|There's probably ten other people in the class with the same question.
Lec3-243|So go ahead and ask it the only, as my friend Rich Sutton says, the only stupid question is the one that wasn't asked.
Lec3-247|Rectified linear units is a commonly used function in deep networks.
Lec3-248|But you can also use it in shallow networks.
Lec3-252|The nice thing about leaky ReLu, which is this green one, is that you also get negative outputs when when you're instead of zero output.
Lec3-253|So at zero outputs does, is it doesn't contribute anything.
Lec3-254|But leaky ReLu, tends to work a little better.
Lec3-257|It's symmetric around zero, which is nice.
Lec3-258|It has negative outputs and positive outputs.
Lec3-259|Okay? And there, there are plenty more there.
Lec3-260|Swish, which I didn't talk about, which is the logistic function times a.
Lec3-261|I think it is.
Lec3-266|They could even be softmax units.
Lec3-267|But that might not be a good idea.
Lec3-268|Then you get this weighted sum of the inputs.
Lec3-269|You apply an activation function, and then you multiply that times the hidden to output weights.
Lec3-270|And then you apply an activation function to that.
Lec3-272|So in summary, there's theoretical ways to motivate perceptrons.
Lec3-274|I didn't talk about the motivation for perceptrons, but there is one linear regression, logistic regression and softmax regression that lead to the activation functions.
Lec3-276|Some activation functions are motivated by how well they work in a neural network empirically.
Lec3-277|And whereas we did just have last fall a conference called deep math.
Lec3-278|Still, a lot of this area is irritating Lee empiric, empirical.
Lec3-279|There's not yet a lot of theory about deep networks.
Lec3-281|For networks like this one.
Lec3-282|How white and friends who was in the Economics Department here proved that network like this could arbitrarily approximate any Borel measurable function up to any degree of accuracy if you just added enough hidden units.
Lec3-284|It's just means a countable number of discontinuities.
Lec3-287|And we'll see how deep networks do stuff in a little bit.
Lec3-288|And finally I showed that this guy is also this guy and this guy is not Fenton area in the code.
Lec3-292|Otherwise, I'll go on to the next lecture.
Lec3-294|So I gotta complaint years ago from some undergrads who said this color scheme hurt their heads.
Lec3-295|And I just haven't changed all of them over to black and white yet.
Lec3-296|So I'm sorry if I'm about to hurt your head.
Lec3-298|And I think those who'd be good things to read.
Lec3-299|So I'm going to talk about first, What's maximum likelihood? Why does it lead to some squared error for regression? How does it lead to cross entropy for logistic regression? How does it lead to cross entropy for multinomial regression or softmax regression.
Lec3-300|And probably the next lecture I'll talk briefly about some other approaches.
Lec3-301|Or maybe I'll get to it today.
Lec3-303|Alright, Some people don't know.
Lec3-304|So the main idea here is what we really want to know is given some data, what parameters of our model are the most likely ones? That would be great if we could do that.
Lec3-309|And typically, the, the idea of maximum likelihood is that we have no reason a priori to prefer one set of weights over some other set of weights.
Lec3-310|And if we treat them as equal, That's called an uninformative prior.
Lec3-311|So it's a flat distribution.
Lec3-312|All of the weights have equal probability even if there 1,472.167.
Lec3-314|So it's not a great prayer.
Lec3-315|It turns out that if you include a certain prior, you get some regularization techniques.
Lec3-319|But that's what it's called.
Lec3-320|Okay? So we want our network to make the observed data as likely as possible.
Lec3-322|So imagine we model our data as the Gaussian.
Lec3-323|So this is a 1D Gaussian.
Lec3-324|And this is the probability of one point given a mean and a standard deviation.
Lec3-325|This is just a formula for the Gaussian distribution in one-dimension.
Lec3-328|That is, that's not always the case, but if we get one data point, if that has some, gives us some information about the next data point, they're not independently identically distributed.
Lec3-329|So your grade on programming assignment one might be predictive of your grade on programming assignment two, and that wouldn't be independently identically distributed.
Lec3-330|So what maximum likelihood says in this case is that we should pick the mu and the sigma that maximizes the likelihood.
Lec3-331|So again, if I didn't finish this, if these are independent, then the, then the likelihood that is the probability of all of the data is just the product of the probabilities of all of the data.
Lec3-333|Independent variables, if you can.
Lec3-334|So your brain is probably deriving relatively independent variables from your input.
Lec3-335|If we add to model the distribution of the world, the variables interacted a lot.
Lec3-337|If you have two variables, you need three numbers.
Lec3-339|And the fourth number is just one minus the other three.
Lec3-340|But if I have three variables, then I need seven numbers.
Lec3-341|And if I have four variables, I need 15 numbers and it goes up exponentially.
Lec3-342|It'd be really good if our brains extracted independent variables from the world.
Lec3-343|Because otherwise, if every, if all the pixels interacted, then we would need brains the size of Manhattan, to represent the world.
Lec3-346|And so we want the mean and the standard deviation that maximizes the likelihood of the data.
Lec3-349|This one doesn't work very well either.
Lec3-350|But this one makes the data the most likely.
Lec3-351|We want the mean and the standard deviation of that Gaussian.
Lec3-352|And in this case, there's a fairly simple way to do that.
Lec3-353|How is it? How do we do it? Yeah, but why? Why is that the maximum likelihood solution we want to derive the formula for the maximum likelihood mean.
Lec3-354|Yes, you have to take the derivative and set it equal to zero.
Lec3-355|And then look for the check that it's the maximum.
Lec3-356|So usually it's easier to minimize the negative log-likelihood.
Lec3-357|So because it gives you the same answer.
Lec3-358|So we want mu and sigma that maximizes the likelihood.
Lec3-359|But that's also going to maximize the log of the likelihood because logs monotonic.
Lec3-366|Okay, so all three of these are equivalent.
Lec3-367|The sigma and mu max of this, the Min of this domain of this and the Min of this.
Lec3-368|So the last bit here assumes a Gaussian.
Lec3-370|It turns out when you do that, you get the empirical mean, which is very satisfying.
Lec3-373|But that's another class, not this one.
Lec3-374|You can try this out on your own at home.
Lec3-375|It won't hurt anything.
Lec3-379|Also maximizes the log-likelihood true.
Lec3-380|Now, let's try this one though.
Lec3-386|A little peer instruction here.
Lec3-387|Your neighbor has a 42% chance of being wrong.
Lec3-391|Nice day in the neighborhood.
Lec3-395|Looking worse, better, better, better.
Lec3-396|I'm giving reinforcement to those of you as you vote.
Lec3-400|It was up to 76 and it got to 72.
Lec3-406|However, in maximum likelihood it's the same as, as the other things.
Lec3-407|So it's a little bit of a trick question.
Lec3-409|Since in maximum likelihood, we assume that the probability of the weights given the data or the maximum of the probability of the data given the weights.
Lec3-411|In applications of neural nets.
Lec3-412|We have inputs and outputs.
Lec3-414|So what this line is showing here is that joint distribution, this comma here means, and in most notations.
Lec3-415|So the likelihood of the data as the joint probability of each data point and its target.
Lec3-416|And we can use the standard rules of probability to factor that into the probability of the target given the input times the probability of the input.
Lec3-417|Okay? That's the likelihood of the data.
Lec3-418|And taking the negative log, we get this guy.
Lec3-419|So we took the sum product, turned it into a sum, we put a negative sign out front.
Lec3-420|And we have these two things.
Lec3-421|When we take a log, we get a sum instead of a product.
Lec3-422|Okay? Now, we're doing neural nets, right? So we have an input, we get an output.
Lec3-423|There's no modeling here of the probability of the data.
Lec3-424|And the probability of the data is a constant.
Lec3-425|Anyway, the weights in the network don't affect that.
Lec3-428|The noise that we assume it's been corrupted by is zero mean Gaussian noise.
Lec3-431|But the red line is the underlying, the assumed underlying deterministic function.
Lec3-432|And the data that we actually see though, is sampled from that with this red point here being the mean.
Lec3-433|If we have zero mean Gaussian distribution, then the distribution of the points around this mean is going to follow this distribution.
Lec3-434|So think of that Gaussian lumped, they're coming out of the board.
Lec3-436|We're assuming zero mean Gaussian noise.
Lec3-440|So this is just one example.
Lec3-441|So this is x naught.
Lec3-442|The value of this line of this function at that point is h of x naught.
Lec3-443|But what we actually see is a corrupted version of that, with that being the mean.
Lec3-445|It doesn't matter down here.
Lec3-446|Some other x, up here at some other x.
Lec3-448|So the mean of the Gaussian that we're sampling from to get the data is the value of this red function at that point.
Lec3-455|In other words, the probability of the target given the input is this Gaussian distribution around h of x n.
Lec3-456|Again, this is kind of small on my screen.
Lec3-457|So again, we're assuming there's this underlying deterministic function h.
Lec3-458|And so the target is the actual H plus some epsilon noise.
Lec3-459|In other words, the noise is t minus h of x.
Lec3-460|So that's what we get in the numerator of this Gaussian.
Lec3-461|So the likelihood of all of the data then is the product of those likelihoods.
Lec3-462|And we get this n-dimensional while this n-dimensional Gaussian.
Lec3-464|And so if this is Gaussian with mean x, and then this is what the distribution looks like.
Lec3-465|We've just given a model of the distribution of the data.
Lec3-466|And that's critical and maximum likelihood is how we model the distribution of the data.
Lec3-467|Here we're modeling it as a Gaussian around a deterministic function.
Lec3-469|I haven't changed anything yet.
Lec3-471|Okay, It's not anything mysterious.
Lec3-472|It's just saying it's essentially depends on W on the weights because that what parameterizes our function y.
Lec3-474|That's looking a little familiar.
Lec3-475|And this log of the variance, square root of the variance to the nth power.
Lec3-476|But our network doesn't say anything about sigma.
Lec3-477|So the second term, we can just remove this because we're not modeling the variance here, we're just modeling the deterministic underlying function.
Lec3-478|So this is not dependent on the weights at all, so we can just ignore that.
Lec3-480|So the minimum of this is the minimum of this.
Lec3-481|Okay, they're the same minimum.
Lec3-483|So when we're doing regression, if we assume the targets are Gaussian distributed, and we're trying to maximize the likelihood of the data by minimizing the negative log-likelihood.
Lec3-485|It's supposed to be spooky that we got that.
Lec3-489|Probably find it on YouTube.
Lec3-491|Okay, so that's how maximum likelihood leads to some squared error for regression.
Lec3-494|And I sneakily replaced that with y r function because that's what we're trying to model.
Lec3-496|H being the underlying deterministic function.
Lec3-497|We're trying to model that underlying deterministic function with our network.
Lec3-498|So I snuck it in here.
Lec3-499|And I don't think I said anything about it at that point.
Lec3-506|Shaking his head, yes.
Lec3-511|And we have to have some functional form of that probability.
Lec3-512|We have to, we have to have some assumption about the distribution of the data to do this.
Lec3-513|And here, we're assuming that P is a Gaussian distribution.
Lec3-514|So the weights, so this is an assumption.
Lec3-515|We can have a different assumption and we'd get a different objective function.
Lec3-516|We wouldn't get squared error, we'd get something else.
Lec3-517|So this could be exponential distribution or something else and we would get a different, we wouldn't get squared error anymore.
Lec3-518|But, you know, a lot of things are Gaussian, right? There are presumably a lot of independent random variables that somehow determine are what we see.
Lec3-519|And the sum of many independent random variables is a Gaussian, it turns out.
Lec3-520|So it's the highest entropy distribution.
Lec3-522|We made this Gaussian assumption.
Lec3-523|That's why we, and this is the product of individual Gaussians for every data point.
Lec3-524|So the probability, the likelihood of the data is the product of the probabilities of each data point, where we assume the data is independently identically distributed.
Lec3-525|So the independent part says we can just multiply them together.
Lec3-526|The identically distributed part is they all follow this Gaussian, but the same sigma.
Lec3-528|It's not necessarily a true assumption.
Lec3-529|Must assumptions are wrong.
Lec3-530|I assume that, you know, when I walk across in front of the class that the floor is going to hold me up.
Lec3-533|Or if we make that assumption, then we get square root error.
Lec3-534|Does that help? Okay.
Lec3-537|And again, the important thing in maximum likelihood is how you model the probability distribution of the data.
Lec3-538|And here we decided it was a Gaussian with an underlying deterministic function h.
Lec3-539|And so we're trying to model that.
Lec3-540|And in fact, this will be the, if the data follows what we think, this will be the curve that's closest to all the data.
Lec3-542|Even though she hasn't settled to a stable state.
Lec3-546|So in logistic regression, it should output the probability of category one.
Lec3-547|We want the network to produce the probability that the input is in category one.
Lec3-548|Even though it's called logistic regression, we're trying to fit this probability function.
Lec3-550|Sorry, I shouldn't have done that.
Lec3-551|So what is the probability distribution of a coin flip? Bernoulli, right? So it's as if we have a coin flip.
Lec3-552|So that the bright distribution, the correct distribution for the probability of the category given the input is a Bernoulli distribution.
Lec3-553|So again, we're picking what probability distribution we're going to use to model the data.
Lec3-556|So this here is an exponent and it's either one or zero.
Lec3-558|This will go to one.
Lec3-559|Because anything to the zero power is one.
Lec3-560|So we just get y of n.
Lec3-562|So this will become one.
Lec3-563|And we get one minus y su pen, which is what we want.
Lec3-564|We want the two probabilities sum to one.
Lec3-566|So now I just said that.
Lec3-568|So it's the product of all of these probabilities.
Lec3-569|And again, we're assuming all points are IID.
Lec3-571|Which should look familiar.
Lec3-573|That's the cross-entropy function for one variable or 11 target.
Lec3-575|Okay, we could just plug and play.
Lec3-577|And then we wrote down the likelihood.
Lec3-579|So this is what we want to minimize.
Lec3-581|Well, this will be zero.
Lec3-582|Okay? This reaches a minimum whenever t soup n equals y su pen.
Lec3-583|So when T of n is one and wine and is one, the expression would be zero.
Lec3-586|Let's see, we've got 3 min.
Lec3-587|What is cross-entropy anyway? What's entropy? If you haven't, if you're not from ECE, may not have had anything on entropy yet.
Lec3-588|But this is 6.10 section of Bishop and it's a lot of fun, I think, but nerd fun.
Lec3-590|So think about these distributions.
Lec3-591|These are all Gaussian distributions.
Lec3-592|If I tell you the value of a variable in the blue distribution, which is the really peaky one.
Lec3-593|I'm not telling you as much compared to if I tell you the value of variable with the yellow distribution.
Lec3-594|Right? So high entropy means a flatter distribution generally.
Lec3-595|Okay? That makes sense.
Lec3-601|Suppose I'm trying to send three words over a channel.
Lec3-602|One word, Bob is twice as frequent as the other two.
Lec3-603|So Bob's 50% of the time, Ted is 25% of the time, Alice is 25% of the time.
Lec3-605|So I have one word, Bob, that's twice as frequent as the other two.
Lec3-606|So I want a short message for that one.
Lec3-609|So I could use one bit for Bob to bits for Ted analysis here is 0.01.
Lec3-610|Now half the time I'm sending one bit and half the time and sending two bits, on average 1.5 bits.
Lec3-611|If you take minus p log p of that, you get one-and-a-half due to, due to the entropy of the distribution tells me how many bits I need to encode it optimally.
Lec3-612|And then cross entropy, that's 320.
Lec3-614|That looks like that.
Lec3-615|So here I've got the probability of T log probability.
Lec3-616|Why? This is optimal if the two distributions are the same.
Lec3-619|And we'll look at that next time.
Lec3-622|I'm giving a little extension for people in that situation.
Lec2-7|How about that? Now? Makes about that.
Lec2-12|Podcast streams there it is.
Lec2-14|For those of you listening at home.
Lec2-15|In this podcast, I'm going to the board so you won't see anything, but all of the derivations are in the slides as well.
Lec2-16|But by going to the board, I have to slow down.
Lec2-18|So keep an eye on me.
Lec2-21|So we have big N data points.
Lec2-22|And y n is the output of the linear regression, which we can.
Lec2-23|So that's the sum squared error.
Lec2-24|If we put a big one over n and the front of it, its mean squared error, but it doesn't change anything.
Lec2-26|I've just substituted for y of n, the sum over j of w and w j x j.
Lec2-29|Can you all see that in the back? Big enough? Okay.
Lec2-31|So again, for linear regression, you may have more than one input variable.
Lec2-32|And then you'll have a scalar output variable.
Lec2-33|And all this is, is lay your regression written as a neural network.
Lec2-34|So you take the weighted sum of all of these and you get an output.
Lec2-35|So that's just what I've written here.
Lec2-38|And this is just a written expression for the same thing as this.
Lec2-39|So we think of the w's is the synaptic strengths between the neurons.
Lec2-40|Maybe I'll turn this down a little bit.
Lec2-41|Or maybe it's just my hearing aids that are making it sound tinny.
Lec2-43|You still hear me in the back.
Lec2-46|Head shakes are good.
Lec2-47|The Indians are shaking their heads.
Lec2-48|No, but they mean yes.
Lec2-50|I can do that too.
Lec2-51|Alright, so that's the thing we're trying to minimize.
Lec2-52|And again, as I said last time, if you put a square root over this, essentially what we're trying to do is for the whole training set is to get the outputs.
Lec2-53|Have the minimum Euclidian distance to the targets.
Lec2-57|You may not have.
Lec2-58|If you have thousands of examples are 1 million examples.
Lec2-59|You're trying to invert a matrix that's like 1 million by the dimensionality of the input.
Lec2-62|And again, if you look at this formula for the sum squared error, if you multiply this out, you're going to get w squared terms.
Lec2-64|So essentially, we're trying to minimize a quadratic over these w's.
Lec2-65|And if you just look at one weight, if you just do projection into the dimension for one weight, you get a parabola.
Lec2-69|And if you have two weights, then this is a bowl you're trying to minimize.
Lec2-70|And that's why it's called, That's why Europe's used to have all of its workshops.
Lec2-74|Day of skiing going doing gradient descent on the mountain, and then 4-6 or whatever, you'd have more talks that soon went away.
Lec2-78|And in statistics a big grant was $10,000 and neural nets a big grant was $1 million.
Lec2-79|And the statistician's have their meetings in Las Vegas and we had our ski resorts anyway.
Lec2-80|So we're starting here and you project this back down here.
Lec2-81|And you change the weight by that amount.
Lec2-82|And you keep doing that and you get down here.
Lec2-83|The problem is that you've got a learning rate.
Lec2-85|And if your learning rate is big enough, eventually, you're gonna do.
Lec2-86|If you've got a big enough change, you're going to end up over here.
Lec2-87|And then you do it again.
Lec2-88|You're going to end up over here.
Lec2-89|And then you end up over here and you waffle back-and-forth.
Lec2-90|If you watch your error go downhill, typically you'll get to someplace where it starts going like that.
Lec2-91|Then what you do is lower the learning rate.
Lec2-92|And you reach some equilibrium down here.
Lec2-94|And eventually you get to the bottom.
Lec2-95|So having too high of a learning rate can make you not really reached the minimum.
Lec2-96|And for a long time what we did was by hand turn down the learning rate.
Lec2-99|So that's the idea.
Lec2-101|What we're gonna do now is derive what this update should be for some squared error.
Lec2-105|So keep an eye on me.
Lec2-107|Okay, any questions so far? Okay.
Lec2-108|So let's do a little bored magic here.
Lec2-110|And the sum of those two is the gradient.
Lec2-111|It's just another name for slope when you have multiple dimensions.
Lec2-115|So I'm just gonna do this for one pattern, meaning one of these ends.
Lec2-116|Because when I do that, I can just sum up the results, right? So to make it simpler on the board, I'm just going to go sum over j equals zero to d w j x j.
Lec2-118|And I want to know what's this with respect to wi.
Lec2-120|So well, what is this? I'm going to use the chain rule.
Lec2-121|And I get t minus the sum j equals zero to d w j x j.
Lec2-123|And now there's still a one-half, but now it's times two because I brought down the two.
Lec2-126|Okay, If you're wondering.
Lec2-128|Okay? And now I'm going to just make it simpler.
Lec2-130|So derivative of T with respect to wi minus the derivative of the sum from j equals zero to d of w j x j with respect to wi.
Lec2-131|And changing w t is a constant.
Lec2-132|So changing w i, that should be an eye isn't going to change that.
Lec2-133|So we're not Jimi Hendrix, like, we don't care if six turns out to be nine.
Lec2-136|And I ain't going to bother you.
Lec2-137|So this becomes zero.
Lec2-138|And now we've got what's the derivative of this guy? I'll go over here.
Lec2-141|Alright, I already made one mistake.
Lec2-142|Anybody notice? It's got to be a minus sign in front of here.
Lec2-144|When you're doing partial derivatives, you treat all the other variables as if they're constants.
Lec2-149|I guess you can't see it behind this.
Lec2-150|Let me write it up here again.
Lec2-160|We make it bigger.
Lec2-162|And if this is positive, then we are going to lower the weight from that input and vice versa for x being negative.
Lec2-171|And your gradient is gonna go uphill instead of downhill and your network will get worse instead of better.
Lec2-176|So I could do this for each pattern as it comes along.
Lec2-178|Again, I would sum this over all the, all the data.
Lec2-180|And now I've gone through all the patterns in the data.
Lec2-181|And I change the weight once.
Lec2-182|That's called batch gradient descent.
Lec2-183|Because you go through all the data before you change the weights.
Lec2-184|If I just change the weights for each pattern as it comes, That's an online learning rule and that's called stochastic gradient descent because typically what you do is you randomize the order of the patterns.
Lec2-185|And each pattern is gonna go in a different direction.
Lec2-187|On average, it's going to head in the direction of the true gradient.
Lec2-188|But this is often faster than batch gradient descent because a lot of datasets that we work with are redundant.
Lec2-191|And the other reason to use m-nest is that when we visualize the weights, you'll be able to see something that makes sense.
Lec2-192|Last quarter for the undergrads, we did we did a redo.
Lec2-193|I forgot the name of the dataset right now, but we did a ten-dimensional or ten category dataset that was.
Lec2-197|So it wasn't very useful to visualize the weights, but I want you to get the idea that the weights correspond to what a particular neuron is interested in.
Lec2-198|So if you think about the incoming weights as a vector and the incoming pattern is a vector.
Lec2-199|You're gonna get the maximum activation when they're lined up.
Lec2-200|And you're gonna get the minimum activation when they're in opposite directions and you're gonna get zero when they're at right angles.
Lec2-202|You'll have as many weights as you have pixels.
Lec2-203|And so you can plot them as an image.
Lec2-204|And what you'll see is like a ghostly version of 1234 up to 90, up to nine.
Lec2-206|I think I've said everything.
Lec2-207|I want to say now.
Lec2-210|And so if you just process half the data and change the weights, the gradient, the weight change will be almost identical to if you do all of them.
Lec2-214|That is the sound that the activation makes when it hits the neuron, F0 MP.
Lec2-215|It's a technical term that only I use.
Lec2-216|And so you get an output, you change the weights for that input.
Lec2-217|And then you get another pattern from the dataset.
Lec2-218|You give that to the network, you change the weights for that one.
Lec2-221|And so stochastic gradient descent means you don't go down that true downhill thing.
Lec2-222|You go down it for one pattern and then another pattern and then another pattern.
Lec2-223|And on average, you're going to end up going in the right direction.
Lec2-224|Because a lot of datasets are redundant.
Lec2-225|You can learn, you can learn a lot about the digits before we even get to the end of the 60,000 examples.
Lec2-228|I'm sorry, I usually without replacement.
Lec2-229|So you randomize the order of the data.
Lec2-230|You run through it.
Lec2-231|You randomize it again, you run through it.
Lec2-237|Now I'm in a new place, and now I'm going to change the weights for the next pattern from that new place.
Lec2-238|So they're not identical.
Lec2-241|I think that's the word I'm looking for are asymptotically.
Lec2-243|There was another question.
Lec2-245|Yeah, it doesn't make the minimum many different, right? It's just a mathematical convenience.
Lec2-246|I could've had one-third there and then I'd have to carry this 0.666 all over the place.
Lec2-248|Will it come back? Yes.
Lec2-253|Let me see if I can get to the interactive demo.
Lec2-257|I can start in different places.
Lec2-260|But anyway, it depends on where you start in this.
Lec2-266|But the point is, for linear regression, it doesn't matter where you start.
Lec2-268|But in general, we're going to have many non-quadratic gradient cost functions.
Lec2-272|And so someplace there'll be good and we'll have a high thing in some places will be bad.
Lec2-273|And generally it's smooth in-between.
Lec2-274|So you get this surface over the weights that you're trying to go downhill in.
Lec2-275|And in that case, it depends a lot on where you start.
Lec2-276|So generally we start with my hearing myself.
Lec2-280|But typically wherever you start, you're going to hit a local minimum.
Lec2-281|The nice thing about stochastic gradient descent is because of the noise in the gradient.
Lec2-282|You're not going down the true gradient.
Lec2-283|You can sometimes get around local minima that way.
Lec2-285|Okay? So again, in regression, what we're trying to do is fit a function to some data points.
Lec2-286|In linear regression, what we're trying to do is fit a line or in two-dimensions the plane, or in three-dimensional and higher hyperplane.
Lec2-287|And the great thing about it is there's a closed form formula for it, but that's never going to happen again.
Lec2-288|So this is the update rule and what's not in my board here is alpha.
Lec2-289|Alpha is the learning rate and that's the thing.
Lec2-290|It says, how far are we going to go in the direction of the gradient.
Lec2-292|So if you want to do online learning, you could do stochastic gradient descent where you update the weights for each pattern.
Lec2-293|Okay? I said that.
Lec2-294|Then finally, as most of you probably know already, there's an in-between version that's kind of a blend between the two called mini-batch gradient descent, where you have smaller, you have a sample from the data.
Lec2-295|And you hope that that sample is representative of the function as a whole.
Lec2-297|For linear regression, since we're taking the gradient.
Lec2-298|Having a learning rate, we just knew exactly.
Lec2-303|And you have to do it in a batch way.
Lec2-304|Otherwise you'll be going off and never, never land.
Lec2-305|And you search along that line for where the minimum is.
Lec2-306|And that gets you there faster.
Lec2-309|And then sum them up when we're done.
Lec2-310|Okay, That's it for this lecture.
Lec2-311|But don't go away.
Lec2-315|And they're the first non-linear neural network that you could train.
Lec2-316|And believe it or not, you can still use the delta rule for them.
Lec2-318|Consider what you can represent with a perceptron.
Lec2-320|So this is Frank Rosenblatt with a neural network in front of him.
Lec2-321|And the perceptron has a single layer of weights, just like linear regression, except that you take the weighted sum of the inputs and then you compare it to a threshold.
Lec2-322|And if it's higher than the threshold, then the output is one.
Lec2-324|And so this is a kind of, you can use this for classification and say, I can train this thing.
Lec2-326|And it's a picture of me.
Lec2-328|And so every pixel has a weight connected to it that goes to the output.
Lec2-329|And essentially, what I'm doing is every time I give the network a picture of me and it doesn't fire, I add my pixels to the weights.
Lec2-331|The only difference here is that now t is only either one or zero.
Lec2-332|And what that's gonna do, if I add my pixels to the weights, it's going to make the weight vector pointing in the direction of me.
Lec2-333|And so I'm going to get a good inner product.
Lec2-335|And I bang it on the head and I subtract that picture view from the weight.
Lec2-336|So it's going to make the weights pointed away from your picture.
Lec2-337|So I keep adding pictures of me and subtracting pictures of you.
Lec2-338|And again, because there's as many weights as there are pixels, I can plot the weights as an image.
Lec2-339|And what I'll get is a ghostly looking picture of me, which is really the difference between me and everyone else.
Lec2-340|These, this is the, the activation function.
Lec2-341|You take the weighted sum of the inputs, compare it to this threshold.
Lec2-342|And you can compute simple things like some Boolean functions like OR, AND, and NOT.
Lec2-343|Okay, I said that.
Lec2-345|Another way of looking at it is like this.
Lec2-346|So that's the activation function.
Lec2-347|If it's bigger than the threshold, you get a one.
Lec2-348|Otherwise you get a zero.
Lec2-349|And that's called a binary threshold unit.
Lec2-351|Okay? Perceptron with no binary, you don't have to get out your clicker because I don't have a clicker setup.
Lec2-353|Who thinks that's true? Raise your hand.
Lec2-354|Who thinks that's false? Who doesn't know? Okay, is it a linear regression model? Remember regression, you're trying to fit a function to some data.
Lec2-355|Whereas the perceptron only as a zero or one output.
Lec2-356|So it's not regression, it's classification.
Lec2-358|How about this? Oh wait, I'm sorry.
Lec2-359|I didn't read it right.
Lec2-360|The perceptron with no binary threshold unit is simply a linear.
Lec2-368|We've got a threshold.
Lec2-375|So if I give it 000 times one is 00 times one is 00 isn't bigger than or equal to one.
Lec2-377|But if I have zero times one is 0.1 times one is 11 is greater than or equal to one.
Lec2-385|This will become clearer as we go.
Lec2-395|Because these are trainable.
Lec2-396|It's the first trainable neural network.
Lec2-398|It can learn to compute, which is pretty cool.
Lec2-402|You can't do it with a single layer perceptron.
Lec2-403|Okay? And actually I can prove that you can't do it.
Lec2-404|It's a quick proofs and even doable in class time.
Lec2-407|So that says the threshold has to be non-negative.
Lec2-408|The second line of the truth table says, I want zero times W1 plus one times W2 be greater than or equal to the threshold.
Lec2-409|So W2 has to be bigger than or equal to the threshold.
Lec2-410|Likewise, the third line says that W1 has to be greater than or equal to the threshold.
Lec2-412|So we've got the threshold is positive.
Lec2-413|Each weight is bigger than or equal to that.
Lec2-415|Okay? So the goal was to make a neurally inspired machine that could categorize inputs and learn to do this from examples.
Lec2-419|Unfortunately, not everything was computable as Minsky and Papert in 1969 book called Perceptrons showed that it couldn't do XOR and there were a lot of problem.
Lec2-420|So what that meant was you couldn't tell whether your inputs are the same or different.
Lec2-422|He was at Cornell at that time as I was, but I didn't know who he was or I didn't know anything about neural nets back then, we're mainly majoring in sex drugs, rock and roll and ending the war.
Lec2-423|And so sex drugs, rock and roll and demonstrations.
Lec2-424|He died while I was there in mysterious boating accident on Chesapeake Bay on his birthday.
Lec2-429|So there's a set of inputs, the design matrix, there's a set of outputs.
Lec2-431|You get an output.
Lec2-432|If they don't match, you change the weights and the threshold so it'll get closer to producing the target next time.
Lec2-434|Let's choose or I'll yeah, okay.
Lec2-436|So let's convert the learning rule.
Lec2-438|And so that's the bias term.
Lec2-439|The bias is the opposite of threshold.
Lec2-440|A threshold tells you.
Lec2-441|By the higher bias means it's going to turn on more in the absence of any other input.
Lec2-442|A higher threshold means it's going to turn on less than the absence of any other input.
Lec2-443|So now we've got a nice form.
Lec2-444|It's just an inner product now between the input and the weights.
Lec2-445|And we compare that to zero.
Lec2-447|And this is an English version of the rule, which I think is intuitive.
Lec2-448|So if the output is one and it should be zero, I want to lower weights from active inputs.
Lec2-449|And by active inputs in this case, I mean inputs that are one.
Lec2-452|Okay, and for W, the weight, the inputs always active.
Lec2-454|So now let's randomly present these patterns to the network, apply the learning rule, and continue until it doesn't make any mistakes.
Lec2-457|I'm going to pick the first row of patterns to start with.
Lec2-458|True and false are represented by one or zero.
Lec2-462|So you'll be more, pay more attention now.
Lec2-463|So the output is one and it should be zero.
Lec2-464|The weights we start everything out at zero.
Lec2-465|And so I was on and I should be off.
Lec2-466|So I want to lower weights to active inputs.
Lec2-467|Then how much should I learn them? Again, that's the learning rate.
Lec2-468|So I'm going to use one to make it easy.
Lec2-472|Now I'm going to randomly pick another pattern.
Lec2-474|And in this case the output is zero because one times zero is 00 times zero is zero.
Lec2-475|X times one times minus one is minus one.
Lec2-476|So the output is zero.
Lec2-478|So I want to raise weight stacked of inputs.
Lec2-479|That will be X1 and W0, W1 and W2.
Lec2-481|I've just added one to the ones that were active.
Lec2-483|And I'm gonna get the same mistake.
Lec2-487|Now I'm going to randomly pick another pattern.
Lec2-489|And if I give this to the network, what's the output? What's the output? In this case, I've got a network.
Lec2-490|That's my network, that's my input.
Lec2-491|What's the output? It's zero, yeah, but it's supposed to be one.
Lec2-492|So what should I write? In this row? The new weights will be, I need three numbers.
Lec2-494|So I'm just going to change this guy and this guy.
Lec2-496|Okay? Now I'm gonna give it zero-zero again.
Lec2-499|Acetylcholine needs to be raised even higher.
Lec2-501|Because zero is greater than equal to zero.
Lec2-502|So say, I've told you three times now you're supposed to be off in that situation.
Lec2-505|So if you remember that the threshold is the opposite of the bias and the bias the opposite of the threshold.
Lec2-506|For, if this was Theta, we'd have 111.
Lec2-507|So I am clairvoyant.
Lec2-508|I knew what she was going to say.
Lec2-510|I didn't change them in advance.
Lec2-511|Do, do, do, do.
Lec2-515|We gave it a set of input outputs that it was supposed to fit its error correction learning.
Lec2-516|We only corrected it when it was wrong.
Lec2-517|We never praised it when it was right.
Lec2-518|So it's not very psychologically plausible.
Lec2-520|I did it that way, so it converge in class time.
Lec2-524|It's just the delta rule.
Lec2-527|So if I'm this is that we're assuming one.
Lec2-530|If our inputs are 0.0, the only thing that changes is the bias.
Lec2-532|If X is inactive, we don't change it because there's zero here.
Lec2-533|If we were at zero and it should be one, then we raise the weight by this rule.
Lec2-534|And if it's inactive, there's no change.
Lec2-535|What about the bias? The bias is a weight from a unit that's always one.
Lec2-536|So x zero is always one.
Lec2-538|Okay? So what if we get it right then T and Y are the same and we don't change the weights.
Lec2-539|Okay? And I should say X doesn't need to be binary.
Lec2-540|We could have images.
Lec2-541|And in fact, there's a story that may be apocryphal.
Lec2-542|We don't know for sure that the army was obviously very interested in this.
Lec2-543|And so Frank trained a neural network to recognize images that had tanks in them.
Lec2-544|And it was very good.
Lec2-548|The perceptron would take longer to converge.
Lec2-550|How many people think a, B, C, D.
Lec2-552|So we decided this by majority rule.
Lec2-553|Democracy here, except I get to, I get veto power.
Lec2-554|So the perceptron will never converge.
Lec2-555|Because in that case, you're actually changing the weights in the opposite direction of what you should change them in, right? Okay.
Lec2-560|Here I'm simply rewriting this as.
Lec2-563|And where does it change? What it says? Where does it go 0-1? It goes 0-1 when W T X equals zero.
Lec2-564|And we call that the decision boundary in two-dimensions.
Lec2-566|So y of x equals zero is a d minus one dimensional hyperplane, or in 2D it's a line.
Lec2-569|Okay? And so the, the line there is the decision boundary.
Lec2-570|So everything on this side will be off and everything on the other side will be on.
Lec2-572|So I've showing the weight vector there, and notice it's orthogonal to the decision boundary.
Lec2-573|So why is it perpendicular? So just take two points on this line.
Lec2-574|Call them XA and XB.
Lec2-576|Okay? So that means that their difference is zero.
Lec2-579|So the w zeros cancel out.
Lec2-581|That's a line segment on the decision boundary is zero.
Lec2-583|So the weight vector determines where the angle of the dividing line.
Lec2-584|And it turns out that the bias term tells you how far along that angle it goes so that the weight vector without the bias says where the dividing hyperplane is.
Lec2-585|The bias says Where, Right? And I'm not going to prove that because you have to do it for your homework.
Lec2-590|perceptron can decide if it's picture me versus a picture of you.
Lec2-591|So category C1 would be Gary and category s2 would be you.
Lec2-592|So this is a little slightly different interpretation.
Lec2-593|Instead of the output being one or zero, we're making a decision about whether it's in category one or category two.
Lec2-594|If we think about it that way, it makes generalization of more than two categories simpler.
Lec2-595|And again, there's no activation function here because we're just measuring the weighted sum of the inputs and then we're making a decision based on whether it's bigger than or equal to zero or not.
Lec2-596|So now we can do multiple categories.
Lec2-598|And when we think about it this way is a decision rule.
Lec2-599|We just pick the biggest output.
Lec2-600|So we pick category CK if the kth output is bigger than everyone else.
Lec2-601|So that's, that's how we think about multiple outputs.
Lec2-603|But it's, we're not doing that here.
Lec2-604|So now we're gonna get decision boundaries that look like this.
Lec2-608|Hi For this line here represents where y of k, y k of x equals y, j of x.
Lec2-610|And so that's where the, the output is the same for the two.
Lec2-611|That's the decision boundary between J and K.
Lec2-614|What about that one? Why I is bigger than everyone else.
Lec2-615|And in fact, every region is convex if XA and XB are in the region.
Lec2-617|And Here's a hint on how to prove it.
Lec2-622|Is this a multilayer perceptron, a single layer perceptron, a linear regressor.
Lec2-623|Or Frank Rosenblatt who says a Who says be.
Lec2-624|Okay, I don't think we need to go on.
Lec2-626|So in summary, perceptrons, a single layer neural network put zero or one.
Lec2-627|So it can be thought of as a classifier.
Lec2-629|So when we say a linear classifier, this essentially what we mean.
Lec2-630|Something like this, where we're going to smooth things out.
Lec2-631|We're not going to have a bang-bang binary threshold unit.
Lec2-632|Soon we're going to have logistic units.
Lec2-633|But logistic, the output was the logistic.
Lec2-634|That is, if it was a smooth activation function that went 0-1, it would still be a line between the good guys and the bad guys.
Lec2-639|Essentially it's still a linear boundary between off and on.
Lec2-640|It's just this ramp.
Lec2-643|This is Jeff Hinton, Bengio.
Lec2-645|And Hinton was a postdoc at UCSD just before I was I missed him.
Lec2-654|We've got ten more minutes, right? 310.
Lec2-657|And that same rule applies to perceptrons.
Lec2-658|Despite what my obstinate friends Steve Hanson says, I keep telling him It's the same rule and he says, No, it's not.
Lec2-661|I used to say that you couldn't get the perceptron rule by gradient descent.
Lec2-662|And one of my students said, yes you can.
Lec2-663|I said No, you can't.
Lec2-665|I learned that from Sanjay Gupta.
Lec2-669|So I said all this.
Lec2-671|So what happens if we try to use mean squared error for logistic regression? We're going to start with the mean squared error objective function.
Lec2-672|We're going to take the derivative with respect to the weights, plug-in the result into the rule for gradient descent and out pops a bad learning rule.
Lec2-673|It's actually the learning rule we used for a long time.
Lec2-676|Some of us knew that there was a better way to do it, but we didn't know why it took awhile to figure out why.
Lec2-678|So this is a monotonic activation function.
Lec2-679|If you go to equals 1/1 plus x minus x.
Lec2-680|This is what it looks like at around minus four, so it's pretty much off and around plus four.
Lec2-686|And you can interpret this as a probability of being in category one.
Lec2-687|Okay? And we're gonna see why that is in another lecture.
Lec2-688|So that's our activation function now.
Lec2-689|And it's inactivation function of the weighted sum of the inputs.
Lec2-691|So it's still linear because g is monotonic and you get this, this ramp up.
Lec2-692|So let's motivate this rule, this activation function.
Lec2-693|By imagining we have two categories that are Gaussian distributed.
Lec2-694|Almost everything in the world is Gaussian, right? With equal variance.
Lec2-698|And they like to play with pens and paper and they took the midterm.
Lec2-699|And it was all true, false, so they could get a score out.
Lec2-700|And so this, the distribution of the monkey is the PDF of the monkey grades.
Lec2-701|This is the PDF of your grades.
Lec2-703|It must have been a pretty hard test because the average is about 50.
Lec2-705|Can we tell if the person was a monkey or a person and if you've done any probability, you know what's coming.
Lec2-706|So we have two Gaussian probability density functions.
Lec2-707|And we can apply Bayes rule to get the probability of a human or monkey given the grade.
Lec2-709|So we use Bayes rule, the likelihood, the prior, and the probability of the data, etc.
Lec2-711|And because they have to sum to one, we can make the denominator the same as the sum of the two numerators.
Lec2-712|And this should all be pretty familiar.
Lec2-713|So I'm going to give us somewhat counter-intuitive derivation.
Lec2-714|But let's call these things a and b.
Lec2-715|So this is a, this is a, this is B.
Lec2-716|Okay? So let's divide through by a and we get 1/1 plus B over a.
Lec2-717|Do a funny math trick and make this e to the log of b over a.
Lec2-719|And then put a minus sign there and then it's the log of a over b.
Lec2-720|Plugging a and B back in.
Lec2-721|So this looks a lot like the logistic activation function, 1/1 plus e to the minus x.
Lec2-722|In this case, what x turns out to be this is, I'm sorry, I don't think I ever plot.
Lec2-723|I ever told you what the activation function was.
Lec2-725|But it's 1/1 plus e to the minus x, where x is the input.
Lec2-726|And what this shows is that if we have two Gaussian distributed categories, the probability of being in category one is this function where this is the log-likelihood of it being if they're equal probability.
Lec2-728|Again, it's the log-likelihood of the two categories.
Lec2-729|So that's motivation for the logistic activation function.
Lec2-730|So the probability of being in class one follows the sigmoid act.
Lec2-731|It's called the sigmoid because if you look at it like this, it looks like an S.
Lec2-732|Okay? And you'll see people talk about logits.
Lec2-733|That's the, even when they're not Gaussian.
Lec2-734|I did that already.
Lec2-735|So this allows us to treat the output is posterior probabilities, the probability of category C1 given the inputs.
Lec2-736|And there's a generalization to multi-dimensional Gaussians.
Lec2-737|And it turns out you can actually write what the weights ought to be in a closed form formula.
Lec2-739|So that's nice, but we can't assume that our data is Gaussian.
Lec2-740|We need to learn the weights.
Lec2-742|And then all bets are off.
Lec2-744|So what do we do? We have to do gradient descent.
Lec2-745|And gradient descent is like magic.
Lec2-746|I always talk about the magic of gradient descent.
Lec2-749|So what I'm gonna do next time on Tuesday is derived is compute, figure out what gradient descent is for Mean Squared Error and logistic regression.
Lec2-750|And we get a bad learning rule.
Lec2-753|Make sure it's kosher.
Lec2-758|Not to not to disrespect Martin Luther King.
Lec2-762|So I don't know.
Lec2-764|You were in the class.
Lec1-1|Okay, We're recording but we're not displaying.
Lec1-8|It's just not registering.
Lec1-20|The podcast screen that's working.
Lec1-25|Okay, So Brendan is here.
Lec1-28|And there is a deep neural net interpreting my voice and turning it into text.
Lec1-31|And you knew everybody in the field.
Lec1-32|Now, there's no way to know everyone in the field.
Lec1-33|But this is the place to come to learn about the basics of deep learning.
Lec1-34|We don't get into very advanced topics.
Lec1-35|We need some faculty to start doing some advanced deep learning class.
Lec1-36|But I'm not doing it because I'm semi retired.
Lec1-38|So they rehired me and I can get up to 43% of my salary teaching and doing research, and I get my pension as well.
Lec1-39|So I'm getting 130% of my salary retired, which is good, like having three months of summer salary all year round.
Lec1-40|And it's a computer science salary.
Lec1-41|I'm a cognitive scientist.
Lec1-43|You don't want to do it the other way around.
Lec1-44|And I'm mainly interested in trying to figure out how the brain works.
Lec1-45|And I do that by building neural networks that model portions of the brain in particular right now.
Lec1-46|In retirement, I got three grants last year.
Lec1-47|One of them, two of them are training grants, but one is a real research grant.
Lec1-48|We're collaborating with Japanese monkey neurophysiologists to try and model the primate visual system using anatomical constraints.
Lec1-51|Just thought I'd try that.
Lec1-56|I'm basically just going to go, give a little introduction, go over the syllabus, and maybe get a little bit into the class.
Lec1-57|So this is the graduate version of my undergraduate course.
Lec1-58|I used the same slides for both classes.
Lec1-59|The only difference is this one is more work.
Lec1-60|This is the this is the response on Reddit to somebody asked me about my undergrad class.
Lec1-62|They don't get a final project.
Lec1-63|You should probably not take anything else that is a heavy load.
Lec1-65|And then, yeah, so you should definitely see what this person is saying.
Lec1-66|It's always recommended to start on the day they come out.
Lec1-67|And you can study as you do them.
Lec1-68|So when I give a programming assignment, don't start on it three days before it's due because you'll die.
Lec1-73|I was hoping to make that bigger.
Lec1-75|It's considered one of the hardest classes in the department.
Lec1-76|So this is warning.
Lec1-78|So what we're going to talk about is some of the math behind deep learning.
Lec1-79|And you're gonna get practice by doing the programming assignments.
Lec1-80|The first two programming assignments you're going to do from scratch in NumPy.
Lec1-83|So you'll get some experience with that and then you'll be able to get that high paying job at Facebook, AI Research.
Lec1-86|So the first all of the assignment, while most of the assignments have a individual part that you're supposed to do by herself.
Lec1-87|Last quarter and the undergrad version, we had one guy who left the other person's name on the assignment they turned in.
Lec1-88|So don't don't cheat.
Lec1-90|And for the first assignment, usually we have about two people in a team doing it.
Lec1-91|And then the second assignment two and third, 13 or four, the fourth, 14 or five, etc.
Lec1-92|Bigger and bigger as we go.
Lec1-93|So the poor TA's have fewer things to grade.
Lec1-95|You should be able to understand some, maybe a quarter or a third of Europe's paper.
Lec1-96|And you're gonna get some practice writing narrative style papers because all of the programming assignment reports are done in Europe's format.
Lec1-98|Okay, So we're gonna go through the syllabus now.
Lec1-101|So I don't have quite so many today.
Lec1-102|Wait, what just happened? Okay.
Lec1-103|Where it is full-screen.
Lec1-108|So almost everything you're going to need to know is gonna be on Piazza Canvas is you know, anything.
Lec1-110|Piazza is the place.
Lec1-113|I give you my cell number.
Lec1-114|I don't know how many professors do that, but the main reason to give you my cell number is so that you can text me.
Lec1-116|I'm going to have office hours over Zoom.
Lec1-117|To avoid getting COVID from you.
Lec1-118|Monday is at four and Wednesdays at noon, that's tomorrow.
Lec1-119|I will have office hours on the two holiday Mondays.
Lec1-120|To get to me.
Lec1-121|My schedule is linked from there.
Lec1-123|I don't really do anything anymore.
Lec1-125|That's what the TAs are for.
Lec1-126|But I will be able to at times to debug your code.
Lec1-127|There's always some fraction of view that try and go uphill in the gradient instead of downhill.
Lec1-128|And you can't, it just gets worse than where you train it, which is not good.
Lec1-129|These are our TAs.
Lec1-130|Making Rohan the head TA, not because he's smarter than everybody else, but just that he's TA this class before, okay.
Lec1-132|There's going to be probably another wave of some new version of COVID.
Lec1-133|So I advise you to wear masks even though it's not required anymore.
Lec1-134|I'm far enough away.
Lec1-135|I think I can get away with not wearing a mask.
Lec1-137|You speak up a bit.
Lec1-138|You can wear a mask, right? I am I am obviously concerned about you as well.
Lec1-143|They're gonna be in class because remote ones promote cheating.
Lec1-145|I don't expect that, but it happens.
Lec1-147|It's it's, it's not good for you because it means you didn't learn the material.
Lec1-149|Maybe in your job someday when you don't know how to do something that you should have learned how to do.
Lec1-152|If you know what that means.
Lec1-153|And compute what and figure out what the update rule should be for the parameters in the model.
Lec1-154|So this, by having mid-terms on Saturday, they're scheduled already for one to 220.
Lec1-155|I think it is on like February 14 and March 14th, something like that.
Lec1-156|It's in there somewhere.
Lec1-157|I also I recommend reading the syllabus.
Lec1-159|Like how I grade, things like that.
Lec1-160|So please, I know it's eight pages long but, you know, have a cup of coffee, sit down, read it.
Lec1-165|And you would spend your time on the weekends figuring out some features you'd come in on Monday.
Lec1-166|Apply the features to the images bolt-on a linear classifier and be done with it.
Lec1-169|Of course, they are biased by the datasets we feed them, but they're only doing what we told them to do.
Lec1-172|Image generation, game-playing, speech recognition as it's happening.
Lec1-173|Right now in the recording.
Lec1-174|If, unless you've been living under a rock, you've been playing with chat.
Lec1-179|I hate, I was trying to compute my grades and I use Excel.
Lec1-183|Wrote it out, had documentation with bullet points for this input, you'll get that output, etc.
Lec1-185|So no, none of my genes are polluting his genome.
Lec1-187|The first time it came out, he got ideas for cowboy theme birthday party.
Lec1-191|So what are we covering here? So we started out relatively simply with linear regression.
Lec1-192|How many people have done linear regression already? Okay, almost everybody.
Lec1-198|Then the second programming assignment is backprop with one or two hidden layers.
Lec1-199|Again programmed by yourself.
Lec1-200|Well with your partner.
Lec1-202|It's okay with a car, you push the button, it turns on, you can drive it.
Lec1-204|Then we do convolutional neural network assignment.
Lec1-207|And then the fourth one is usually recurrent nets.
Lec1-208|You learn about vanilla recurrent nets and LSTM networks, things like that.
Lec1-209|Those two you can do with Py Torch.
Lec1-210|Back in the day, people had to actually implement convolutional neural networks from scratch in my class.
Lec1-212|You don't have to do that.
Lec1-214|So again, this is not an advanced deep learning class.
Lec1-215|If you already know how to implement an LSTM network and you run experiments with it.
Lec1-216|You probably don't need this class.
Lec1-217|But it's a good introduction.
Lec1-219|Then the last week or so, I cover kinda what happened last week in deep learning.
Lec1-222|And it has implementations of things in PyTorch and TensorFlow and hand rolled UCSD simulator as well.
Lec1-223|It's, it's better than a lot of things out there.
Lec1-224|There's the deep learning book.
Lec1-225|And I looked at how they explained principal components analysis in that book, there was absolutely no intuition, that was all equations, so I didn't get any farther in that book.
Lec1-226|So I highly recommend dive into deep learning instead.
Lec1-227|I also use Chris bishops, old book, neural networks or pattern recognition, which I think is maybe still the name of this class.
Lec1-228|It has very good introductory chapters.
Lec1-229|Of course, there wasn't deep learning back then, at least not much.
Lec1-230|I didn't train a five layer network once.
Lec1-231|And it's very good.
Lec1-232|Now use his, his notation as well.
Lec1-234|So it says the place to go.
Lec1-235|No Zoom, but I'm podcasting this so you can do that.
Lec1-236|I like to use clickers.
Lec1-237|You probably don't have a clicker.
Lec1-238|Please go buy a clicker, the web or the app for clickers.
Lec1-239|I have not been able to get it to work to record your participation.
Lec1-240|The only way I use clickers is I give you participation points.
Lec1-241|So it's a way to encourage you to come to class and just answer the questions.
Lec1-242|Um, and that helps me know if you're following along.
Lec1-243|And if I use Christine Alvarado has recommendations which is less than 80% of you get it right.
Lec1-244|Then I have you turn to your neighbor and discuss it for a few minutes, and then we try again.
Lec1-245|So have a neighbor.
Lec1-248|But you can get used clickers, almost any kind of clicker will work.
Lec1-249|You know that type one and type.
Lec1-250|I think they have different kinds.
Lec1-251|Now, the reason to come to class and use your clicker is that if you participate more than 75% of the time, I will bump your grade up at the end.
Lec1-256|Alright, so let's go to Piazza for a moment.
Lec1-258|You do get the lectures ahead of time.
Lec1-259|You get last year's lectures.
Lec1-260|Whereas it not happening.
Lec1-262|Okay? And I can't do anything now.
Lec1-269|He put me back there.
Lec1-274|I guess I'm not getting the Piazza today.
Lec1-275|Everybody else is trying to get to Piazza or something.
Lec1-277|All I wanted to say about that was that the oh gosh, how am I going to get back to where's the other window? Okay.
Lec1-278|So I'm not going to be able to get back to where it was trying to load the answer.
Lec1-279|It's not getting anywhere.
Lec1-283|Anyway, the main point of telling you about Piazza is that the slides are there.
Lec1-284|Now I can't go through the way.
Lec1-285|I have my own copy of the syllabus.
Lec1-286|I can go through these.
Lec1-290|The main point of telling you about Piazza is that the slides are there.
Lec1-291|They often still changing the slides minutes before class.
Lec1-292|And so I don't give you the lecture slides I'm lecturing you with just before class because I'm still editing them.
Lec1-293|I try, I'm trying to optimize my slides.
Lec1-295|Usually I'm doing small edits, so they're very similar.
Lec1-296|So you can look there for the sites.
Lec1-299|The programming assignments are 80% of your grade, the mid-term, so 10% each.
Lec1-300|And these four assignments are 60% and then the final project is 20%.
Lec1-301|So again, the first couple are done in pairs.
Lec1-302|Third, one in groups of three or four.
Lec1-303|Fourth, one in groups of four or five.
Lec1-305|So first we're gonna do logistic regression, softmax regression.
Lec1-306|We generally give you two weeks for these.
Lec1-307|So this Thursday I'm going to hand out the first assignment.
Lec1-310|And then fourth one is LSTM networks.
Lec1-311|And I'll step you through the process of your final project by having you give a few paragraphs of what it's gonna be about so we can check it and make sure that it's reasonable.
Lec1-312|Then I have you do a progress report.
Lec1-313|You should have your data maybe training networks.
Lec1-314|This is five days later than five days later than that.
Lec1-316|Um, and then we do project presentations.
Lec1-317|And the more teams there are, the bigger the teams are, the more time you have to present your project.
Lec1-318|Generally, you get about five-minutes in the 3 h that we have during the time slot for the final.
Lec1-319|So your team comes up here, you present your project and we we grade you not only on your project but your presentation.
Lec1-321|My advice is redundancy.
Lec1-323|How much you're going to tell him.
Lec1-324|Tell him then tell them what you told them.
Lec1-326|So after your presentation, then you can turn in the final thing that will grade.
Lec1-329|So everybody should code.
Lec1-330|Joe wrote the report, not going to get as many points as the people that actually did the work.
Lec1-331|So just writing the report is not good.
Lec1-332|You need to get in there.
Lec1-334|And that way and switch drivers during the assignment.
Lec1-335|So that way everybody gets a chance to code and nobody gets left behind.
Lec1-336|I want you to really understand how this stuff works.
Lec1-337|Okay? And in fact, I do one of my main when we grade the programming assignments.
Lec1-338|I'm the guy that grades the team contributions.
Lec1-339|So I look at that and then I write you a little note.
Lec1-340|If it doesn't look good for Joe who only wrote the report.
Lec1-341|And I say, I'm planning on only giving Joe 50% of the points.
Lec1-342|You can argue with me about why Joe should get more, but that's my plan.
Lec1-343|And then so keep an eye on that.
Lec1-344|We grade on grade scope.
Lec1-345|I think everybody does.
Lec1-346|You'll get those comments there.
Lec1-350|You don't have to do the readings, but it's recommended.
Lec1-351|I think people learn stuff better when they see it from different perspectives.
Lec1-353|Andre cardiomyopathy has a wonderful blog on neural nets.
Lec1-354|Lillian Lee, I think your last name is a really good podcast or not Peck, Sorry.
Lec1-358|So we start with perceptron.
Lec1-359|We start with single layer networks, linear regression, logistic regression, and softmax regression.
Lec1-361|I talk about representations.
Lec1-362|So again, your mantra for this quarter is backpropagation learns representations in the service of the task.
Lec1-364|Want you to meditate on that.
Lec1-365|Say it to yourself over and over again.
Lec1-366|Okay? And then, so we'll see some examples, not usually from deep networks, but for more shallow networks to see where things are more interpretable and see what neural networks learned.
Lec1-367|Okay, tricks of the trade.
Lec1-369|The book was called tricks of the trade.
Lec1-371|So I go over that.
Lec1-372|I talked about objective functions, how to improve generalization, convolutional networks.
Lec1-373|There's other readings you can go to the Stanford continents course and read their beautiful web pages with lots of Explanations.
Lec1-375|Then seven to 09:00 P.M.
Lec1-376|the night before I record the review session and it says seven to nine, but usually it goes till ten and I go home really tired.
Lec1-378|Then we start getting into recurrent networks.
Lec1-379|Attention networks, transformers, reinforcement learning, more reinforcement learning.
Lec1-381|So you may have heard of gans.
Lec1-382|The last class is kind of a disturbing and fun class, talking about the ethics of AI.
Lec1-383|How our datasets that we use are biased and end up with bias models.
Lec1-395|Last quarter we are average response time on Piazza was something like 20 min.
Lec1-396|I don't know if we'll be able to do that with three TAs this quarter.
Lec1-397|But if you have a question that's hanging around for 8 h or something like that, then text me and I'll make sure somebody answers it.
Lec1-398|I don't always monitor Piazza.
Lec1-401|There are a lot of resources on the web about neural nets.
Lec1-403|Geoff Hinton had a Coursera course.
Lec1-404|You can still find the lectures on YouTube.
Lec1-405|Andrew Ng, a Coursera course.
Lec1-406|I don't know if that link still works.
Lec1-408|The neural network playground is a lot of fun.
Lec1-409|You can build up to, I think, five or six hidden layer networks and train it to do for different tasks.
Lec1-410|See what kind of representations that learns.
Lec1-411|You can play around with different optimizers and things like that.
Lec1-412|We have a demo of face recognition that I will use in class.
Lec1-413|But it's a little bit fun.
Lec1-416|She has great explanations.
Lec1-417|Two-minute papers is fun.
Lec1-418|It's really more like five-minutes.
Lec1-422|This the most popular MOOC there is, I believe.
Lec1-424|So it's really useful thing to go.
Lec1-425|I highly recommend it.
Lec1-426|Barbara Oakley also wrote a really fun book.
Lec1-427|Evil genes, why? Why Hitler rose Rome fell.
Lec1-429|I read it about ten years ago.
Lec1-430|It's a lot of fun.
Lec1-434|And the way I do that is a well, usually it's the average grade, but maybe the median is more reliable statistic.
Lec1-436|So it's a linear shift up to 88.5.
Lec1-437|I have that number to all the grades and then round up to the nearest whole number.
Lec1-439|And if you end up with more than 50% in the class, I will pass you somehow.
Lec1-441|I think C plus is not really a passing grade for grad students if you're from India, right? You need to have certain number of credits.
Lec1-446|We have a fairly soft policy.
Lec1-447|You can please get it done on time.
Lec1-448|Don't ask for extensions.
Lec1-450|You get 10% off for a day late, 20% off for two days late, and 50% off for three days late and more than that 100%.
Lec1-451|However, if you get COVID or your roommate gets COVID and you have to take care of them or you have to fly home because your grandfather got COVID and it's dying.
Lec1-452|I will give you a break on that.
Lec1-453|This is Casey, by the way.
Lec1-454|He's not very well trained yet.
Lec1-456|Those are the ones that stand on top of the sheep.
Lec1-460|He has really weird eyes.
Lec1-461|At first I thought he was a demon from hell and they expected to grow to more heads, but he didn't.
Lec1-462|And now I've gotten used to them.
Lec1-465|So academic integrity, I'm sorry, I have to talk about this.
Lec1-468|I've never done this before.
Lec1-469|I finally gave in.
Lec1-470|They didn't turn it in or Turner in, then she cheated on the next slide.
Lec1-471|So I'm not doing that again.
Lec1-473|And you might get a slap on the wrist.
Lec1-474|One guy last quarter and undergrad is going to be suspended for spring quarter.
Lec1-475|I guess he's done it before, so it's not good.
Lec1-477|You can discuss the assignments with other people.
Lec1-481|That was seven savages joke.
Lec1-486|And what that is is you can talk about the homework with anyone else.
Lec1-487|You can discuss it.
Lec1-488|But you can't take any written notes and you have to watch an hour of Gilligan's Island or some equally insipid television show before you write anything down.
Lec1-489|And that should prevent you from turning in an alphabetic variant of somebody else's homework.
Lec1-493|Just the individual part.
Lec1-494|So that's what I do.
Lec1-497|There are no formal prereqs, but you should know what Bayes rule is.
Lec1-498|It'd be really great if you know how to take partial derivative.
Lec1-499|All those things are good to know in advance.
Lec1-501|If you have your own.
Lec1-502|We are committed to be to diversity here.
Lec1-503|I'm a lefty from the New Left, leftover from the New Left.
Lec1-515|If you haven't OASDI accommodation, let me know I used to be on that committee.
Lec1-516|One of the great things about retirement is I don't have to be on committees.
Lec1-517|And it's been estimated that some non, non insignificant fraction of graduate students are hungry.
Lec1-518|So here's some resources.
Lec1-519|There's the tritone food pantry where they have free food, etc.
Lec1-522|Discussion sections, what are they compulsory? Know, but you certainly should go because it's in the discussion sections where the TAs go over the programming assignments.
Lec1-524|But I wish they had done it more spread out.
Lec1-525|But yeah, and we will record those and provide the slides to those two.
Lec1-526|But they're highly recommended if you have questions about the assignment.
Lec1-535|They cover the material for the upcoming programming assignment.
Lec1-536|Usually after it's been assigned, right? Soon after it's been assigned.
Lec1-539|They'll be given by the same person.
Lec1-546|You know, you know how big the room is.
Lec1-548|Usually there's not a lot of people don't come, so there's usually plenty of room.
Lec1-556|I'm there should be because we're going to put out the assignment on Thursday.
Lec1-557|So Brendan, you want to volunteer to do the first assignment? Okay.
Lec1-562|They should be in the schedule of classes.
Lec1-565|Center to 163 to 44 to five.
Lec1-567|Do you have to be added to Piazza? It's linked from Canvas.
Lec1-568|And Canvas usually automatically adds you to Piazza.
Lec1-571|Any other questions? Comments, funny stories.
Lec1-574|I'm going to start right now.
Lec1-577|Well, not quite right now, but sorry.
Lec1-579|I try to be consistent.
Lec1-580|I tried switching to more modern notation.
Lec1-583|I still use T for the target, Y for the output.
Lec1-585|So why deep learning? I think you're all, you all know why deep learning.
Lec1-586|It's what's driving all of AI right now pretty much.
Lec1-587|And ever since we got deep networks, they've been able to do amazing things.
Lec1-589|Or our theoretician of neural nets.
Lec1-595|What's a Borel measurable function while it's any function you care about really.
Lec1-596|But nowadays with deep networks, one of the big problems in computer vision was lighting, pose things like that.
Lec1-597|And now neural nets can take my picture like this.
Lec1-598|And over many layers of processing, transform it into one place in the hidden unit, in the representational space and the network.
Lec1-600|You're probably using deep networks every day.
Lec1-601|Should understand how they work and especially if you want that high paying job at Google.
Lec1-602|Now, I know there's been a lot of layoffs and the tech industry, you're in the right place.
Lec1-603|You went back to school, right place to hang out while the tech industry is in a dip.
Lec1-604|But when you come out, they should be back.
Lec1-606|So what can deep learning you can do a lot of computer vision tasks.
Lec1-608|The only successful neural network company for a long time was Robert Hecht Nielsen's company.
Lec1-609|He was in ECE here.
Lec1-610|And he had a network that American Express used to tell when somebody else was using your credit card.
Lec1-611|And so there are more and more things that they can do these days.
Lec1-612|And as you probably know, there are a lot of things they can't do though.
Lec1-613|Yan lacunae and the Turing Award winners have ideas about how to get neural networks to reason.
Lec1-614|But right now they don't do that very well.
Lec1-616|They could pass the Turing test at this point, I think, at least to a naive observer.
Lec1-618|We haven't gotten to artificial general intelligence.
Lec1-621|Your brain is not one big, homogeneous bunch of jelly.
Lec1-623|It's got a hippocampus.
Lec1-624|If you don't have a hippocampus, you can't store any new information.
Lec1-626|So it can start to remember things that happened yesterday and learn new things.
Lec1-627|But right now, there isn't such a thing.
Lec1-629|I highly recommend the book, the alignment problem.
Lec1-630|I've been reading it today and other days recently.
Lec1-632|We want them to be aligned with our values.
Lec1-635|But that's another question.
Lec1-636|But if you tell a super intelligent AI cure cancer and it kills all life on the planet and it's cured cancer, that's great, but it's not really what we wanted.
Lec1-637|So we have to make sure that they're aligned with our needs and desires and wants.
Lec1-638|So if you took 258, you probably already know this, but there are several types of machine learning.
Lec1-639|Unsupervised learning where you learn some model of the data.
Lec1-640|Supervised learning where usually you learn a mapping from an input to an output.
Lec1-641|So you're approximating a function.
Lec1-642|Reinforcement learning is learning from your mistakes.
Lec1-643|That's reinforcement learning is probably the best attested learning system in the brain.
Lec1-644|Your basal ganglia are doing or have all the components do reinforcement learning.
Lec1-646|They learn about the environment by just being in it and acting in those two things, unsupervised learning and reinforcement learning, I think for the wave of the future.
Lec1-647|Actually there right now.
Lec1-648|And then there's imitation learning.
Lec1-649|So most, a lot of what this class is gonna be about as supervised learning.
Lec1-650|Where you have a set of input output patterns that you're trying to associate this input with this output.
Lec1-651|So you tell the network exactly what to do and when it doesn't, you bang it on the head and it changes its parameters to get closer to what you want.
Lec1-653|Classification is usually thought of as mapping to a target label.
Lec1-654|And regression is trying to fit some real valued function.
Lec1-655|Yeah, so that's the textbook story, but it's actually not always clear which, which these things are.
Lec1-657|autoencoders are neural networks to take an input and try and reproduce that input on its output.
Lec1-658|So we use supervised techniques for that, but the supervision comes from the data itself.
Lec1-659|We don't have anybody labeling things.
Lec1-661|And it told you about those.
Lec1-666|And what we're going to talk about now is linear regression.
Lec1-667|Linear regression is a neural network.
Lec1-668|It's a really simple neural network.
Lec1-669|There's a set of inputs and a set of outputs.
Lec1-670|And you assume that there's some underlying linear function that's been corrupted by Gaussian noise.
Lec1-671|And then it says go to the board.
Lec1-677|Now, this means that you basically, the output is, would be written as a unit in a neural network and it has a weight from the input and a bias way that says what you do in the absence of, thanks.
Lec1-681|But basically, what we have here is this weight and we're trying to approximate it.
Lec1-682|So I'll give a hat there.
Lec1-683|Then we have the height number coming in and there's this weight.
Lec1-684|And then we have a link from a unit that's always one and that's the bias or the officer.
Lec1-685|That's a neural network.
Lec1-686|It takes an input folk, you get an output.
Lec1-687|The activation function is the identity function.
Lec1-688|It's just h times w plus the bias.
Lec1-689|That is h, Sorry, W-hat 0.
Lec1-691|But that's a neural network.
Lec1-693|One step up from this is to add an activation function here, like logistic activation function.
Lec1-695|So that's a neural network.
Lec1-697|So again, you've got this set of inputs and we call that the, if you want to sound smart at a conference, you call that the design matrix and a set of targets.
Lec1-698|And so I call these input output patterns.
Lec1-699|This first row is a set of inputs for the, and this is also illustrating my notation.
Lec1-700|These numbers that look like exponent's are not exponent's.
Lec1-701|They are the pattern number which, which pattern we're looking at.
Lec1-702|So this might be a set of pixels.
Lec1-704|And we'd like to get as close to the targets as possible.
Lec1-705|And to make things simpler, we usually tack on a column of ones.
Lec1-706|And that means we can treat the bias as awake from a unit that's always one.
Lec1-707|And what we're trying to find are these weights that approximate the targets.
Lec1-708|So that's what we want.
Lec1-710|Then what's close? Close means that the weight vector that minimizes the sum squared error.
Lec1-711|Again here, n is indexing which pattern it is.
Lec1-712|J is indexing which input it is.
Lec1-713|D is the dimensionality of the input.
Lec1-714|So it's always d plus one because of the bias term.
Lec1-715|Now what function does this remind you of? That's not some squared error, but it's similar.
Lec1-716|What happens if we put a square root on top of this? We get Euclid, get Euclidean distance, right? So we're trying to minimize the distance between the output vectors and the target vectors.
Lec1-717|You're trying to get as close to them as possible.
Lec1-720|And so again, you're trying to fit a line.
Lec1-721|What? This line is, the line that's closest on average to all of these data points.
Lec1-722|Why did we do this? We do this because we want to generalize to new data.
Lec1-723|So we might have a kid here that isn't in our dataset that's 52 " tall.
Lec1-724|And we want to predict what is weight ought to be.
Lec1-725|If he's up here, he's a little overweight.
Lec1-726|If he's down here is a little underweight.
Lec1-729|And so you can't solve that perfectly.
Lec1-730|That's why you have to approximate it with something like sum squared error.
Lec1-731|I usually won't go this fast.
Lec1-732|I'm going this fast today just because I think that you've all seen this before.
Lec1-733|Basically it's just a reminder.
Lec1-734|So one way to solve this is to take this objective function, this thing we're trying to minimize, and take the derivative of it with respect to the weights and set it equal to zero.
Lec1-735|And try setting it equal to zero and solving.
Lec1-737|And because it's a quadratic, it's a quadratic.
Lec1-738|And what, what is it a quadratic in? What are my variables here? The w's are the variables.
Lec1-739|Some people get confused by the fact that there are x's there and they've always had x as being a variable.
Lec1-740|But x is, are given to you by God in the training set.
Lec1-741|They are constants and the targets are constants.
Lec1-743|And so you end up with a quadratic.
Lec1-744|So there's one minimum.
Lec1-745|So it has a single minimum.
Lec1-746|We can take the derivative with respect to the variable w and set it equal to zero and solve.
Lec1-747|And this is just some math to get there.
Lec1-748|But in the end, and I don't expect you to write this, answer this on a test.
Lec1-750|And it involves a matrix inverse.
Lec1-754|But it's only on that one side.
Lec1-755|It doesn't work on the other side.
Lec1-758|So it's actually better in these days to use something like gradient descent for this.
Lec1-759|Now, not all regression is linear.
Lec1-760|You can transform a variable like using a log.
Lec1-761|This is, I think Moore's law.
Lec1-762|One of the axes is logarithmic, so it's not really linear in that sense.
Lec1-763|And we could also make it nonlinear by using coefficients of a polynomial.
Lec1-764|Then that's called polynomial curve fitting.
Lec1-765|And it's still linear in the variables.
Lec1-766|Or we could use a neural network to fit the data.
Lec1-768|This is polynomial regression.
Lec1-769|Here using learning the coefficients of a polynomial whose done polynomial regression here.
Lec1-770|Okay, Still, that's the only a couple of you.
Lec1-771|But you can still use these techniques to do that.
Lec1-772|It's linear in the coefficients of the polynomial.
Lec1-773|So you might have a quadratic form like that, e.g.
Lec1-774|that you're trying to fit a function with.
Lec1-775|Okay, so that's the linear regression and it's 317.
Lec1-779|But that's what I'll talk about next time.
Lec1-780|And that'll be for a handful of you, your introduction to gradient descent.
Lec1-781|So see you on Thursday.
Lec5-2|If anybody sees a ride rolls around, I think I left my near for Casey last time.
Lec5-3|Doesn't seem to be where I left it.
Lec5-7|And then go on to talk about some representations that backprop is learned.
Lec5-9|Whereas if we start a new session, weird.
Lec5-11|Just check if it's plugged in all the way and that's the main thing.
Lec5-14|Again, we're still doing gradient descent.
Lec5-15|And again, this schema for gradient descent, we want to go downhill in our objective function to minimize it by changing the weight in the direction of the negative of the slope with respect to that way.
Lec5-18|And I start by applying the chain rule.
Lec5-19|We need to derive this guy.
Lec5-21|In this case, with respect to the weighted sum of the inputs to unit j.
Lec5-22|We're talking about unit j here and this the input and this the weight on that line, right? So it's the weight from I to j.
Lec5-23|Then the derivative of that weighted sum of the inputs with respect to the weight into that unit from Unit I.
Lec5-26|Just as before, we've done that a couple of times.
Lec5-27|And this is just replacing the net input with its expression.
Lec5-32|So now we have that.
Lec5-34|So this is starting to look a lot like the delta rule.
Lec5-36|We still have an input on that line.
Lec5-37|The way change is going to be proportional to that.
Lec5-38|If it's big, a big way change if it's small, small, we change.
Lec5-39|If it's negative, we'll have negative weight change depending on the sign of Delta.
Lec5-40|Then we define delta as minus the derivative of the gradient with respect to the weighted sum of the inputs.
Lec5-42|So that's what we have.
Lec5-44|And it's a minus, a minus.
Lec5-45|So that ends up being a plus.
Lec5-46|But I still need to figure some stuff out here.
Lec5-48|We know for the output units with the correct, with the right activation function and objective function that that becomes t minus y.
Lec5-49|We still need to figure out what happens with the hidden units.
Lec5-50|And for the hidden units.
Lec5-52|I read the the tech report with backprop in it, and this was where I ended up staring at it for about a week before it made sense.
Lec5-54|Here I'm applying the chain rule again over how the error goes down.
Lec5-55|How the error changes with respect to the act.
Lec5-56|Weighted sum of the inputs of the units downstream from me towards the output of the network.
Lec5-57|And then how they're weighted sum of their inputs changes as my input changes.
Lec5-60|And then how their input changes as my input changes.
Lec5-61|So this is the idea in pictures.
Lec5-62|Here I am unit j and we're trying to figure out what Delta is for this unit.
Lec5-63|And I'm connected downstream to all these other units by their weights.
Lec5-64|And so that's how does the error change as they, their input changes, and how does their input change? As my input changes? This would be the most confusing part to me anyway.
Lec5-65|And I majored in math.
Lec5-68|By the chain rule, their input changes, my output changes, my output changes as my input changes.
Lec5-69|This guy is just gonna be the slope of the activation function.
Lec5-70|So I pulled it out because it's out of scope of the sum now.
Lec5-71|And then I have to figure out this is a lot like that other one.
Lec5-73|I can't see this without my glasses on.
Lec5-74|Can see it there.
Lec5-75|The weighted sum of the inputs, the weighted sum of the inputs to those units times with respect to my output.
Lec5-77|Or I equals j, close as close.
Lec5-78|And that guy is the slope term of my act, my activation function.
Lec5-79|So the higher my slope, the more bang for the buck by changing my weights.
Lec5-80|So we figured out what this is.
Lec5-81|And again, it's the weighted sum of the deltas of everybody I'm connected to, that I'm projecting two times the weight I'm projecting to them with.
Lec5-82|And again, this is intuitive.
Lec5-83|If they have a big error and I have a big weight to them, I get a big delta.
Lec5-84|And if these are small, then I get a small deltas.
Lec5-85|So it depends on how much I'm affecting them.
Lec5-86|Right? And so this is how we compute the deltas.
Lec5-87|We figure out what the output delta is.
Lec5-89|But once we've computed the output deltas, then we propagate them backwards through the network.
Lec5-90|Just running the net work backwards to the next layer down, et cetera, et cetera.
Lec5-92|So again, this is the picture you should have in mind.
Lec5-93|I'm trying to figure out the Delta of this guy J.
Lec5-95|This is especially convenient with ReLU units, rectified linear units where the slope is either one or zero.
Lec5-97|There's no non-linear activation function.
Lec5-98|And what that means is that in a very deep network, I'm applying linear operations over and over and over again.
Lec5-99|If you've studied dynamical systems at all, you know that linear dynamical system is either going to blow up.
Lec5-100|We're going to get larger and larger.
Lec5-101|Activations are numbers, in this case deltas, or it's going to contract to zero.
Lec5-102|So those are called the exploding and vanishing gradient problem.
Lec5-103|Exploding gradient problem we can deal with relatively easily by just limiting the length of the weight of the gradient vector, the delta vector.
Lec5-104|The vanishing gradient problem is a little harder roller units have less of a vanishing problem because the for a network with the logistic units in it, the maximum slope of the logistic activation function is 0.25.
Lec5-107|So we couldn't train very deep networks back in the day.
Lec5-108|But with a ReLU unit, you've got a one or a zero, and it's less likely to completely disappear.
Lec5-109|But later we'll see some architectural changes that make it even better than that.
Lec5-112|We compute the delta at the output.
Lec5-113|We propagate it backwards through the network.
Lec5-115|That's online backprop where we presented one pattern and do that.
Lec5-116|If we are doing batch, then we can average that over the number of patterns.
Lec5-117|Or if we're doing mini-batch, we average it over the number of patterns in the mini-batch.
Lec5-125|So you initialize your initial random weights to have usually zero mean and unit standard deviation or the square root of the fan and standard deviation.
Lec5-126|We'll talk about that later.
Lec5-127|But it certainly can happen that you get a dead unit that doesn't respond to anything.
Lec5-128|Because then it's not going to change the output weights if it's always zero, the weights from that unit, because you're always multiplying by the slope which is zero if it's irrelevant.
Lec5-129|So you can have dead units.
Lec5-130|Okay? So now we're going to talk about the different kinds of representations backprop learns.
Lec5-131|And most of these, we're going to use shallow networks.
Lec5-133|It's a little harder to figure out what the representation of a unit is.
Lec5-136|But here we can.
Lec5-137|It's a little easier.
Lec5-138|Networks with two hidden units.
Lec5-139|Fully connected have learned XOR.
Lec5-141|And that's because now the error surface is not a bowl anymore.
Lec5-143|And one of the, one of the things that might learn is to have one of these units be or, and then in that case, turned on the output.
Lec5-144|And so that would fit three out of the four rows of the truth table for XOR.
Lec5-145|The fourth one corresponds to end.
Lec5-146|And so you can have one unit that only turns on if both inputs are on and then in that case turns off the output.
Lec5-149|In fact, depending on the initial random weights, backprop, we'll surprise you with a bunch of different possible solutions.
Lec5-150|So again, we're going down this error surface that's got, got hills and valleys and things like that.
Lec5-151|And depending on where you start that, where you start, this is a function of the weights of the network.
Lec5-152|So depending on your initial random weights, you'll start in different places on that surface.
Lec5-154|I hope is easy at this point.
Lec5-159|You have like a two-thirds chance that your neighbor is right.
Lec5-160|So find a neighbor.
Lec5-161|Maybe YouTube could talk.
Lec5-162|You don't have neighbors.
Lec5-169|Discuss it with your right hemisphere.
Lec5-172|Going, going, going gone.
Lec5-174|So far, so good.
Lec5-175|Still a few people looking better.
Lec5-176|It is not C or D or E.
Lec5-180|Hey, where people? Seven.
Lec5-182|Some of you haven't voted yet, but I'm going to close it out.
Lec5-183|Going, going, going gone.
Lec5-186|False, Right? It's linear.
Lec5-188|There is no activation function when you propagate the deltas backwards through the network, it's a weighted sum times the scalar, which is the slope term.
Lec5-191|Any questions from the nine of you that voted for a you don't have to admit it.
Lec5-192|You can just ask the question.
Lec5-194|Know, just getting a scalar number and multiplying it.
Lec5-197|So it's kind of pattern detector and that whatever features that learns is an internal representation.
Lec5-199|The third most important thing is that it learns internal representations.
Lec5-200|It's like real estate, location, location, location.
Lec5-202|As you'll see in your programming assignment.
Lec5-203|Um, that will give you on Thursday.
Lec5-204|You can, you don't need backprop to compute the slope.
Lec5-205|You can numerically approximate it.
Lec5-206|But that operation is order w squared, the number of weights squared, whereas this operation is linear in the weights.
Lec5-207|And it generalizes to recurrent networks.
Lec5-208|And that's good because your brains are recurrent network and we're trying to do artificial intelligence.
Lec5-211|We have activation flowing around in our heads and we should want a model that has recurrence in it.
Lec5-212|For the next k slides, where k is some medium-size integer, we'll look at various representations that backprop is learned and problems backprop is solved.
Lec5-213|And again, your mantra for the rest of the quarter is backprop, learns representations in the service of the task.
Lec5-214|So I want you, when you're having a mental mental enhancement moment and meditating, meditating on that.
Lec5-217|And so the bias is written inside the circle.
Lec5-218|So in the absence of any other input, as six makes it a logistic unit about 999.99, 92.2 is still positive.
Lec5-221|What, what Boolean function is that you give it 00.
Lec5-222|What would the output be? Yeah.
Lec5-229|If you give it 01.
Lec5-230|It's also on because six minus four is two, so it's still on.
Lec5-231|And if you give it 11, then you've got six minus eight and it's off.
Lec5-232|So it's got to truth table for a NAND.
Lec5-236|It only turns on for zeros, zeros.
Lec5-238|You remember in your digital architecture classes, we build things up at an ends and Norris, it only turns on for one thing when both are both are off.
Lec5-243|This one, I usually just show this slide and not with all these annotations.
Lec5-244|So last night I spent some time making this better.
Lec5-245|We're going to see it for a few slides.
Lec5-246|So this is symmetry.
Lec5-247|So we're checking here if the input which is down in the middle of this picture is symmetric around the middle.
Lec5-248|So this particular 1110000 is not symmetric.
Lec5-249|So the hidden units are here and here.
Lec5-251|The output does is it's one if the input symmetric and zero otherwise.
Lec5-252|And so it's got a high bias.
Lec5-253|Again, these are still, you can think of these as roughly logistic units.
Lec5-254|It's got a high bias, so it's going to assume you're symmetric unless proven otherwise.
Lec5-255|So the hidden units here are imbalanced detectors.
Lec5-256|They're off by default.
Lec5-257|But if they detect an imbalance here, at least one of them will turn on.
Lec5-258|And I'll, I'm just going to cut out the middle of this to illustrate the point.
Lec5-259|Okay? Actually I'm going to cut out the middle and leave the outside.
Lec5-260|So this is a sub, subnetwork of that network.
Lec5-261|So this is the input to hidden unit.
Lec5-262|This a hidden unit.
Lec5-263|If these are perfectly balanced, 1.1, then plus three minus three is zero and it stays off.
Lec5-264|Minus three plus three is zero and this stays off, right? But if, if it's 10, then we get one times three plus zero times three.
Lec5-265|So that turns on, turning off the output.
Lec5-266|This one doesn't turn on because it's minus three plus zero and you get minus four.
Lec5-267|What do you want to get that? Okay, take that.
Lec5-268|Any questions? Similarly, if it's 01, then the top one turns on.
Lec5-273|A logistic hidden unit will be pretty far on for like a two input.
Lec5-277|The bias tells you what it's gonna do in the absence of any other input.
Lec5-279|Yeah, the bias is also learned by backprop.
Lec5-280|Because remember the bias is a weight from unit that's always one.
Lec5-281|And so the input on that line is always one.
Lec5-282|So often the bias is what's learned first.
Lec5-284|You just take the weighted sum and apply that to logistic function as the input to logistics.
Lec5-285|So here it would be zero times minus three is 01 times three is 33 plus minus one is two.
Lec5-286|So this thing will be active and outputs like 0.8 or something like that.
Lec5-293|So does that make sense? Now? If it's 11, obviously, one times two minus three is minus 31 times three is 33, minus three is 00, minus one is minus one.
Lec5-295|And so it doesn't propagate activation here.
Lec5-298|Okay, And so the point of having this minus three plus three, plus three minus three is this one.
Lec5-299|It'll detect an imbalance on this side.
Lec5-300|Down here, this one will detect an imbalance on this side.
Lec5-301|And the inputs are only ones and zeros.
Lec5-304|I don't have any more duck.
Lec5-305|You have to like chicken now.
Lec5-309|And then there are these other weights.
Lec5-310|And now I've actually rip them large.
Lec5-311|So you can see them better from back of the room.
Lec5-318|And if this was minus three, well, I should do it up here.
Lec5-319|Minus three plus three.
Lec5-322|And I'm not the only one who calls it magic.
Lec5-323|I've seen other people do that too.
Lec5-326|But these are still biases.
Lec5-329|If it was 0100, we add 0.1 and get one.
Lec5-331|So where does this guy do? In that case? If it was 0101, so we're adding 1.1 and hopefully we're going to get to.
Lec5-335|So what does that unit do? Right? It's the carry.
Lec5-336|So it turns off this guy is trying to be turned on by these 21 times one is 11 times one, it's 12 minus one plus one is two, minus two is zero.
Lec5-338|And then it turns on.
Lec5-339|So it carries the 1/2 here.
Lec5-342|We end up with now this is the answer for 11.01, we get 100.
Lec5-346|So that this network arranged in this way with one hidden unit that's downstream from the other.
Lec5-347|It's a local minima about half the time.
Lec5-348|Because about half the time this guy learns to be the carry bit out of here.
Lec5-352|Well, essentially this is layer one and this is hidden layer 2.0.
Lec5-353|That's the activation of this unit.
Lec5-354|And this is the activation of this unit.
Lec5-355|And these are the activations of these units.
Lec5-356|Sorry, I should have said that.
Lec5-360|This the forest place.
Lec5-361|Nobody is connected directly to that.
Lec5-362|It has to be a ripple.
Lec5-363|So essentially it's the ripple carry.
Lec5-365|You can study this at home too.
Lec5-368|I I didn't do this.
Lec5-370|And again, it will fail completely if this one decides to be the carry out of here.
Lec5-371|And now this one doesn't know there was a carry out of here because it's not seeing the output of this guy.
Lec5-372|It only works if the guy in the first layer becomes the carry out of here.
Lec5-373|And then the guy in the second layer detects that and computes the carry out of there.
Lec5-374|Okay? So net torque, this was the beginning of neural network.
Lec5-379|Anyway, I can't remember the names of them right now because my brain is old.
Lec5-383|So this is net torque.
Lec5-385|It uses one-hot encodings of letters.
Lec5-387|There are like 26 input units here, plus some, some punctuation.
Lec5-389|here, the first unit in the input would be turned on and the rest would be zero.
Lec5-390|So it's a one-hot or localist encoding.
Lec5-393|And the job of the network is to take these inputs, go through this layer of hidden units, and then turn on an output corresponding to the phoneme for the central letters.
Lec5-394|So in this case, cat, it should turn on.
Lec5-396|So it's figuring out what the what the right phoneme is for the central letter in the context of three letters on either side of it.
Lec5-399|They took a deck talk speech synthesizer and they ripped out the rules and plugged in this network and you could hear it read.
Lec5-402|He used these battled logistic units, not even a softmax at the output, just a bunch of logistics.
Lec5-403|And it was trained on this text.
Lec5-404|So this is from Carter out and Jones.
Lec5-405|They transcribed, they were linguists and I guess this is what linguists do for fun.
Lec5-406|They took this transcription of a six-year-old boy talking.
Lec5-408|So this is a great training set.
Lec5-412|I'm going to play it for you.
Lec5-414|Learning corpus taken from Carter and Jones.
Lec5-421|You can't start a backprop net was zero weights, you have to have random weights.
Lec5-423|This is what it sounds like.
Lec5-424|The second recording gives the performance of the network.
Lec5-425|After 20 passes through a corpus of 500 words.
Lec5-427|So then they turn the page and use that as a test set.
Lec5-428|First recording, de novo learning new, I know your name, yeah.
Lec5-430|So the first thing one of these networks is going to learn is the bias at the output.
Lec5-431|And it learns essentially the average phoneme in English and ignores the input and just goes.
Lec5-433|Then it starts to pay attention to the input and starts to modulated speech.
Lec5-434|First it learns the soft kind of consonants like our kids do like mama.
Lec5-437|Yeah, while doing the line, line, line, line.
Lec5-443|I'm going to switch now.
Lec5-444|You got the point of that.
Lec5-445|To where he has it.
Lec5-446|Do what it did on the training set.
Lec5-448|After 10,000 training words.
Lec5-450|I walked over to Fraser.
Lec5-451|Sometimes we can't go from school because of a getaway every time she wants.
Lec5-452|She gets no reverse.
Lec5-453|I still can't breathe very well.
Lec5-455|That's why we can't run.
Lec5-461|And back in the day before deep learning, this often turned out to be a religious experience firmly, freshman seminar students.
Lec5-463|And I'm not going to show you the texts at first.
Lec5-464|I'm going to let you see if you can understand it.
Lec5-465|In the third recording, the previous network is tested on a new corpus, which is a continuation of the training corpus.
Lec5-467|I can stand in my family.
Lec5-469|I can get just need February night.
Lec5-470|He won't stop jumping or running a bathtub.
Lec5-473|Me just tell by 12.
Lec5-475|So he talks about trying to go to sleep with this baby in the house.
Lec5-477|So anyway, that's supposed to be a little bit of fun.
Lec5-479|Now the question is, I've got all of these hidden units.
Lec5-482|I think it's a, I think they have 70 hidden units.
Lec5-485|And they collect all those hidden unit vectors.
Lec5-487|Now we have a vector for every mapping that the network has to make.
Lec5-488|Now what we didn't do is cluster analysis, hierarchical cluster analysis.
Lec5-489|Who's done that before? Nobody don't.
Lec5-490|You do that in 25251 anyway.
Lec5-491|So what you do is you, now I've got these k vectors where k is some large integer.
Lec5-492|One for each mapping, I find the two, the two closest ones by either Euclidean distance or cosine.
Lec5-493|And I average those two together and I draw a little bit of NO down.
Lec5-497|Last about 30 s.
Lec5-500|I take the next two closest ones, average them together.
Lec5-501|Now I've got 68.
Lec5-502|And if I take two vectors that have been averaged together, I joined their little bits of tree.
Lec5-504|And you do that and you get this binary tree.
Lec5-505|And what's interesting about this is that it has all the vowels, a, e, I, 0, and sometimes even sometimes y.
Lec5-509|My obstinate friend Steve suggested this to them and then didn't get any credit for it.
Lec5-510|He's still upset about that.
Lec5-512|Geoff Hinton did this.
Lec5-513|You train a network to represent the relationships encoded in these family trees.
Lec5-514|So there are two isomorphic family trees.
Lec5-516|So Christopher married Penelope.
Lec5-517|They had two kids, Arthur and Victoria, Andrew and Christina, James.
Lec5-519|And there's the same relationships down here.
Lec5-521|So there are 2 468-101-2204 different people in these two trees.
Lec5-526|And by having, we used to call this a localist representation.
Lec5-528|What we're going to look at our Hinton diagrams, they're called where these six hidden units, we're going to look at their weights as black and white squares depending on the size of the weight to the people.
Lec5-531|And then he joined it here and here and this actually deep network.
Lec5-532|So it was kind of a big deal at the time to train a network that deep and get it to work.
Lec5-533|So what we're going to look at is these six hidden units here.
Lec5-534|I keep thinking I'm plugged in with us and none.
Lec5-536|This is the weight from Christopher and it's negative and big.
Lec5-537|This is the weight from Andrew to this hidden unit.
Lec5-538|This is one hidden unit here.
Lec5-539|It turns on for Andrew and turns off for Christopher.
Lec5-540|White means positive size of the square.
Lec5-541|The area of this square is proportional to the size of the weight.
Lec5-546|But anyway, he had a function in there for computing Hinton diagrams.
Lec5-549|Another way our language is biased.
Lec5-552|The bottom row is the corresponding Italians.
Lec5-555|So what feature of the input is this encoding? Give you a minute to talk to your neighbor, come up with an answer.
Lec5-558|Who thinks they have the answer? Raise your hand.
Lec5-559|I have the answer.
Lec5-566|This is one hidden unit.
Lec5-569|If Roberto comes on in the input, this unit stays off.
Lec5-571|It's what? It turns out.
Lec5-573|But yeah, it's nationality unit.
Lec5-574|It's turning on for the British, turning off for the Italians.
Lec5-579|What units should be on in the output, right? Only British, because it's a British family tree.
Lec5-580|Alright? So that's a good bit.
Lec5-581|Alright, it cuts the possible outputs in half.
Lec5-583|It probably has to do with how he did actually leave out a couple of patterns.
Lec5-584|So it might have to do with how frequently they showed up in the input.
Lec5-585|Okay? Okay, so it's picking out the British turning on for these guys, right? Turning off for these guys.
Lec5-589|It's going to turn off for Charlotte and Sophia and they'll find.
Lec5-591|For Collin and Charlotte.
Lec5-594|So this is heightened the tree.
Lec5-595|These are kinda grandparent units.
Lec5-596|So it's picking out the guys at the top of the tree.
Lec5-597|Okay, that makes sense.
Lec5-599|This looks kind of like an Italian detector, but we're talking about this guy now.
Lec5-600|It's on for Christopher and Roberto.
Lec5-601|It's off for Andrew and PRO.
Lec5-602|Right? So this is side of the tree.
Lec5-603|So this unit and codes in the United States at a wedding, the bride's family sits on one side of the church.
Lec5-605|So it's picking out the left side of the tree.
Lec5-608|Other combinations of these units will pick out other parts of the tree.
Lec5-611|Gender, yeah, so this is back when there were only two, but it's turning on for females.
Lec5-613|it would put out Penelope.
Lec5-616|Here's a clicker question.
Lec5-619|It's not that bad.
Lec5-624|Going 60, going, going gone.
Lec5-629|Nobody said a, which is your mantra actually, but B is true too.
Lec5-631|It's not black magic.
Lec5-635|Okay, switch to the demo.
Lec5-637|So it's trained by supervised learning.
Lec5-638|So you turn on one of the people inputs and one of the relation inputs, and you train it to turn on the correct output unit.
Lec5-640|The features in this case makes sense, so it's good for training purposes.
Lec5-642|All right, so let's switch to demo.
Lec5-646|And what we're trying to do is just separate the greens, the reds from the blues.
Lec5-647|And there's overlap in the two distributions, but you can move them around by playing with these numbers over here.
Lec5-648|And I can train it.
Lec5-649|And it quickly learns that separating line.
Lec5-650|But look at how much it's wiggling.
Lec5-651|What should I do? Reduce the learning rate, okay, I will do that.
Lec5-652|Now, we goes less.
Lec5-653|I'm going to reduce it again.
Lec5-655|Okay, and we can actually start it out with that low learning rate.
Lec5-656|And it very quickly gets there, but then it's got a struggle to get more of the red.
Lec5-658|Because the learning rate is too high.
Lec5-660|And PyTorch and TensorFlow have learning rate schedulers that will do that for you.
Lec5-661|But let's look at what happens if I start with too high a learning rate.
Lec5-663|Watch what happens, ready, set, go.
Lec5-669|I made it disappear.
Lec5-670|Just went way too high.
Lec5-674|So that was the two-dimensional input space, and that was the one-dimensional separating line.
Lec5-676|And this is the cafe dataset.
Lec5-677|It's California's facial expressions that we collected here at UCSD with the help of a trained facts coder, Facial Action Coding System.
Lec5-679|So each facial action is some independent.
Lec5-686|And for each face, these are the principal components, the first, second, third, etc.
Lec5-687|And what these sliders are is they are the projection of this face, this face onto this thing.
Lec5-688|So it's got a big negative projection.
Lec5-689|And I can twiddle it.
Lec5-690|And it changed her face completely to more male face.
Lec5-693|And so these are UCSD undergrads and grad students and they're making the six basic facial expressions.
Lec5-694|Happy, sad, afraid, surprise, disgust in an angry.
Lec5-695|That's supposed to be fear.
Lec5-697|We have two kinds of happy, happy with you're showing your teeth and happy not I can't do happy showing my teeth.
Lec5-699|Sad, neutral, surprised, angry.
Lec5-701|I think we also had two kinds of angry, angry with your mouth open and angry with your mouth closed.
Lec5-707|You did a lot of this work.
Lec5-709|He was great grad student.
Lec5-712|He had to leave because his wife had twins.
Lec5-713|This is him finding out his wife had twins.
Lec5-715|He quit, quit with a with a masters.
Lec5-718|And then they have an anti brain drain law.
Lec5-719|So they either had to pay back three times the amount or go teach in Thailand for 20 years.
Lec5-725|But basically we're going to train it to either recognize facial expressions or recognize identity or gender and trigger warning.
Lec5-726|There are only two genders in this dataset.
Lec5-732|My mind is going Sandra, I can feel it.
Lec5-733|But anyway, yeah, Hough transform.
Lec5-734|I don't know if that means probably not.
Lec5-735|Not in this class.
Lec5-740|Well, I can probably give you some advice on that.
Lec5-742|What's his name? No.
Lec5-744|He's in he's in cancer treatment.
Lec5-746|He's in he's working on neuropathy and diabetes and sorry, take skin samples and then we use deep learning to try and find that the epidermis dermis battery and then count how many nerves for crossing that because that's when you get feeling back.
Lec5-747|So he has a treatment for for neuropathy and run a clinical trial and now he's trying to automate the counting of nerves crossing that boundary, which should bring feeling back.
Lec5-752|Much smarter than I am.
Lec4-0|Chew on this one that sit, good boy, down.
Lec4-12|Let me check stream.
Lec4-17|So someone asked after class Y and please don't wait till after class to ask questions.
Lec4-18|Ask them during the class.
Lec4-21|And when I'm talking about is sending the value of a variable from the blue distribution versus the value of the variable from the yellow distribution.
Lec4-22|The battery, and this is almost dead, so only get to use it a little bit.
Lec4-23|So why is sending something from the blue distribution less informative than sending something from the yellow distribution.
Lec4-24|Somebody out there telling me, alright.
Lec4-29|So most of the values of the variables are gonna be right in here.
Lec4-30|Whereas from this distribution, most of you don't know roughly where the variables are.
Lec4-31|And I always say that variance is poor man's information.
Lec4-32|So the high-variance distribution is sending something from, which means it's high entropy, means that sending something in the value of a variable from this distribution is more informative because in the blue one, everything's concentrated right around zero.
Lec4-34|And then the formula for entropy is minus p log p base two.
Lec4-35|That's entropy and bits.
Lec4-36|If this was e, natural log e, then this would be in that my example was sending these three words over a channel.
Lec4-40|So we send Bob 50% of the time.
Lec4-42|So what's the optimal code to send those in the minimum number of bits you need to send those.
Lec4-43|And we figured that out on the next slide.
Lec4-44|That no, not that slide.
Lec4-45|What would be optimal would be to use one bit for Bob.
Lec4-48|Then another bit to say which one it is.
Lec4-49|So 50% of the time I'm sending one bit, 50% of the time I'm sending two bits, and on average I'm sending one-and-a-half bits.
Lec4-50|Okay, this should look familiar to those of you doing coding theory and ECE.
Lec4-51|And so entropy is minus p log p.
Lec4-52|So if I take -0.5 log 0.50, 0.5, -2.25, 0.25, that's also 0.5.
Lec4-53|And so that represents how much entropy there is for Ted analyse, this represents how much entropy there is for for Bob.
Lec4-54|And so when I add those three up, I get 1.5.
Lec4-55|Okay? The entropy of the distribution is how many bits I need to encode it.
Lec4-56|Okay? And so I have to sum that up for all the variables and the distribution.
Lec4-57|And then cross entropy is how much I lose by sending the message from one distribution using another one.
Lec4-62|And it's best if the two distributions are the same.
Lec4-63|So what we wanna do is move the probability distribution of Y as close as possible to the probability distribution of p of t.
Lec4-68|You should ask it.
Lec4-71|Yeah, because y is the output of our regression or logistic or softmax regression.
Lec4-76|Okay, So what I didn't get to last time was how it leads to cross entropy for multinomial regression or softmax regression.
Lec4-77|And kind of surprisingly the formula ends up even simpler to some extent anyway.
Lec4-78|So we have more than two categories, so we need more outputs.
Lec4-79|We can just represent it as p, one minus p.
Lec4-82|And if TK soup n is one, then example and is from category k and zero otherwise.
Lec4-83|And that's called a one-hot encoding.
Lec4-85|The targets are zero except for the category that the target is in.
Lec4-86|That's called one-hot encoding.
Lec4-87|These days we used to call it localist encoding back and when we called all this connectionism, now how to write the likelihood.
Lec4-91|So we can write the probability one pattern as the product of the probabilities of the outputs of the network to the T sub t and a sub k.
Lec4-92|Okay? And so e.g.
Lec4-93|suppose we have, just to make this concrete, suppose we have category three out of four outputs.
Lec4-96|It's an example of one-hot encoding.
Lec4-97|Any questions about that? So that's already got a product and it's now, that's now the probability of one pattern.
Lec4-98|And that's going to mean that that's the probability of one pattern to get the likelihood of all the patterns I have to multiply together.
Lec4-99|Because again, we're assuming they're IID identically, independently distributed.
Lec4-100|So I have to plug in this here and I get that.
Lec4-103|And so now we can take the negative log-likelihood of that because that's what we want to try and minimize the negative log of the likelihood.
Lec4-105|Okay? And when this is again minimized, when all the y's equal, all the t's.
Lec4-109|They don't have to be exactly the same.
Lec4-112|Anything times zero is zero.
Lec4-114|Any other questions? Okay.
Lec4-116|I'll have to do it myself.
Lec4-120|Okay? And again, the idea of maximum likelihood is we're trying to maximize the likelihood of the data.
Lec4-121|Given the parameters, and we're assuming that that's also the probability of the parameters given the data, because we're assuming the prior is equal.
Lec4-122|The prior on the weights that is.
Lec4-124|And this is why last time that, that clicker question was a little tricky because it could have been either one, really.
Lec4-125|But I was looking for the probability of the weights given the data.
Lec4-126|Again, the key idea is you have to figure out the probability distribution of the data or assume usually some parametrized form of the probability distribution of the data.
Lec4-129|Okay? So that's maximum likelihood and I'm done with maximum likelihood.
Lec4-130|So you have a question about that.
Lec4-131|This would be yeah.
Lec4-135|They have an expression for the probability of that, for the parameters of the distribution also.
Lec4-136|And they integrate over that.
Lec4-140|A lot of times we design objective functions in order to get the answer that we want.
Lec4-141|And you have to be careful, because if you tell your super smart AI that you want to cure cancer and it kills all life on the planet that's cured cancer.
Lec4-142|But it wasn't quite the objective function we wanted.
Lec4-145|Maybe we want to cluster the data and a lower-dimensional space.
Lec4-146|So you might want to take the input and map it into a space where inputs that are, the outputs that are close together are in the same category, right? So that's clustering.
Lec4-147|And that's supervised if we know that the data points belong to the same or different categories.
Lec4-148|Although this is a metric learning and it's also the basis for a lot of unsupervised algorithms, as we'll see later.
Lec4-149|So we might want e.g.
Lec4-150|all the pictures of the same person to be mapped to one region of representational space at the output of the network.
Lec4-151|So the output of the network has two numbers, two outputs.
Lec4-152|We'd want all the faces of one person to go to one point in that space and all the face of another person to go and another point.
Lec4-154|And we usually do this with Siamese neural networks.
Lec4-155|So Siamese twins is a dated term for conjoined twins, twins that are physically joined at birth.
Lec4-156|Siamese neural nets are basically identical networks.
Lec4-157|Okay, so that's why this says f and that says F.
Lec4-159|We give it two inputs.
Lec4-160|And this might be the face to me like this, or it might be the face to me like that.
Lec4-161|And I get two outputs.
Lec4-162|And what I wanna do is I want those two outputs to be as close together as possible.
Lec4-167|You haven't done that yet.
Lec4-170|And what we want is that X1, if X1 and X2 are from the same category, then we want these to be as close as possible.
Lec4-171|But if they're from different categories, then I want them to be far away from each other.
Lec4-176|And we want these two people could be mapped to the same output vector because in fact they're one person.
Lec4-177|And what's cool about that is, I have all these different pictures of this guy.
Lec4-178|And what the network has to do is become invariant to those changes.
Lec4-180|It may not go to exactly inside a cluster.
Lec4-181|It could be end up in its own cluster, but it should be invariant to different pictures of the same person.
Lec4-182|So the network has to learn this and variance.
Lec4-184|You would want all of these to be in the same category.
Lec4-185|So they would have to be invariant to differences between these images.
Lec4-186|And that'll generalize to new faces.
Lec4-188|I'm going to use two outputs for this because that allows you to see what you can, what they do.
Lec4-189|And what we're gonna do is anything from the rows should be mapped to the same output.
Lec4-190|Anything from the columns should be mapped to different outputs.
Lec4-192|Oh, just to give you the idea before I show it to you.
Lec4-193|Basically what we're trying to do is if they're from the same category, we want to minimize the squared error between the two outputs.
Lec4-194|So we're trying to move them closer together.
Lec4-196|So this is training on m-nest and the different colors correspond to different categories.
Lec4-197|And after this is using a convolutional neural net, and after 200 iterations, things still look pretty clumped.
Lec4-199|And again, this is output one.
Lec4-201|This is output to, and again, I'm plotting them so you can see them.
Lec4-203|And then there's 5,000 iterations and then 50,000 iterations.
Lec4-204|So what's interesting about this is because you're moving similar things close together.
Lec4-205|That also applies to the categories.
Lec4-206|I'm trying to push things that are the same close together.
Lec4-207|And so similar inputs like 0.6 will be near one another, one in seven or be near one another, etc.
Lec4-209|And these aren't linearly separable.
Lec4-211|And see how well it does.
Lec4-212|This isn't linearly separable because it's in 2D.
Lec4-213|You can never separate the threes from everything else.
Lec4-215|there are a lot of advanced, I really like Siamese networks.
Lec4-216|There's a lot of advantages to them.
Lec4-217|Because the main one from my point of view is because you're training on pairs of data points, then you're basically amplifying your dataset.
Lec4-219|Those are just two outputs of this Siamese network.
Lec4-220|And so there's no fixed target ever.
Lec4-221|It's just t1 to t2.
Lec4-222|If I give it two ones, I want the two outputs for those two ones to be as close together as possible.
Lec4-225|One point on that.
Lec4-230|Remember they're the same network and both.
Lec4-236|It's some image of a zero.
Lec4-237|This is the same network.
Lec4-238|These are the same.
Lec4-239|Here's X2 and that's equal to somebody else's zero.
Lec4-240|So I want them to be the same.
Lec4-241|And basically I have just two outputs, Y1 and Y2, Y1 and Y2.
Lec4-247|So this is the output space of the network.
Lec4-248|It's two-dimensional, so you can look at it from two different networks.
Lec4-252|I mean, I guess these days we could do three, but after three and then after five, as your first homework shows, all your intuitions go to hell.
Lec4-255|And I'm pointing it out which ones are which without, so you can see that without the legend.
Lec4-256|So I've trained on this 60,000 examples from m-nest and all of the zeros are going to that red cluster.
Lec4-257|So I'm fostered the data sufficient Y2, so I can show it to you.
Lec4-258|I mean, one dimension.
Lec4-262|I haven't told you what the loss function is, except that I did say it's square root error.
Lec4-264|You're trying to minimize the distance between all the red points and maximize the distance between the different colors up to some margin usually.
Lec4-265|So the cool thing about this is again, because you have your training on pairs of points.
Lec4-267|And data points to n squared to train the network.
Lec4-268|Actually, it's, it's how many? How big the categories are.
Lec4-274|I'm giving them into zeros, right? And I've got two outputs, Y1 and Y2.
Lec4-275|Know there are the outputs of the network.
Lec4-276|And I'm trying to minimize the distance between the vector Y1, Y2, and then this guy.
Lec4-278|I've never had so many questions.
Lec4-280|You're really trying to understand it.
Lec4-286|You have two outputs, so there's, there's two-dimensions in the output.
Lec4-287|The output of the network has just two numbers.
Lec4-288|And now I'm plotting those numbers.
Lec4-290|You minimize the loss.
Lec4-291|So this is, this is output of the network one.
Lec4-292|This is output of the network to.
Lec4-293|So I got to point.
Lec4-294|Each airport is a two-dimensional point and the red guys are the two-dimensional point for the zeros.
Lec4-297|You've got these nice convex clusters.
Lec4-298|Was just so we can look at this picture, it makes sense.
Lec4-301|That up to the network.
Lec4-302|Yeah, that's the cool thing.
Lec4-303|She said, Do we leave what Y1 and Y2 are up to the network, right? We didn't actually give it a target.
Lec4-304|It's whatever the network started.
Lec4-305|It's the initial random weights that set the network to put things in the space.
Lec4-306|And so, yeah, we don't give it actually targets.
Lec4-307|We just say we want these to be similar and these other ones to be separate.
Lec4-310|We're taking we trained the network.
Lec4-311|What we're trying to do is I work with this guy at Scripps Institute of Oceanography, Bill Guerrero wick.
Lec4-312|And he's a natural products researcher.
Lec4-314|It means like you go diving in the sea.
Lec4-315|You pull up some lobster spit or something, and then you put it in your NMR machine and you get these dots.
Lec4-316|On a page that correspond to bonds between a hydrogen and a carbon, you're trying to find out what this molecule is so that you can use it to cure cancer maybe.
Lec4-317|And we were trying to, if I can cluster molecules according to how similar they are.
Lec4-318|Then by giving this network these dots on a page as a computer vision thing.
Lec4-326|So another thing about this is you don't need to know the number of categories in advance because you're not doing softmax regression.
Lec4-329|And that gives you a big clue as to what the structure of the molecule is if it's like these other molecules.
Lec4-331|And then given a new face, most of the faces of that guy or that woman or that person would be mapped into a similar point in the space, so it generalizes.
Lec4-332|Okay? So I promised I would tell you what the objective function is.
Lec4-333|This is the objective function.
Lec4-335|And for some reason they chose one to be different.
Lec4-337|So this part of the objective function goes away and you only have this part.
Lec4-338|If the, if the target is zero, then y is zero, then this goes away and you only have this part.
Lec4-339|So what is this part doing? It's just one times this.
Lec4-341|So you're trying to minimize the squared distance between them.
Lec4-342|This is just another notation for the squared error.
Lec4-343|Squared distance, okay? This one.
Lec4-346|So we want to minimize that.
Lec4-347|But if they're different, then we want to minimize this, which is a minus w.
Lec4-349|It's pushing them apart.
Lec4-350|But in this case, we have a margin that says, okay, we're not going to push them infinitely apart.
Lec4-353|What your objective is just zero, so there's no, no error there.
Lec4-354|And this is based on a spring analogy where you're letting it push.
Lec4-360|Is it technically considered a classifier? Yeah.
Lec4-361|This is also called contrastive learning.
Lec4-362|If you've heard that this is what it is.
Lec4-363|And so it turns out you can use this and unsupervised learning by e.g.
Lec4-364|taking an image from your mini-batch and warping it a little bit by taking a different crop or skewing it a little bit.
Lec4-365|And then you can say they're the same because you know they're the same.
Lec4-366|Then you take two different images from your training set and use them for different.
Lec4-369|Yeah, So you could be wrong.
Lec4-375|They tried to make it sensitive to shape by randomizing the color.
Lec4-377|Backprop will glom onto any thing it can to solve the problems.
Lec4-378|There was a paper I reviewed for Europe's few years ago where they had one pixel in m-nest and the pixel told you what category it was.
Lec4-380|And the network totally ignored the rest of the image and just concentrated on that pixel.
Lec4-381|So it was doing great, it's perfect.
Lec4-382|And then you give it a five, but the pixel says it's four.
Lec4-383|It'll say it's for backdrops very.
Lec4-385|Not you don't want if there's an actress who has a beauty mark that's marked down her face with makeup.
Lec4-386|And it focuses on that.
Lec4-387|And then they remove the beauty mark and it doesn't know who it is.
Lec4-394|No, Y1 and Y2 are always 1.0 in this not in this part of the network.
Lec4-395|No, no, it's okay.
Lec4-398|I mean, these are not collapsing to zero.
Lec4-400|Oh, that's because of the margin.
Lec4-401|You can only push them apart so far and it stops worrying about it.
Lec4-403|They could be anywhere in the space.
Lec4-405|You're shaking your head.
Lec4-406|No, but you mean yes.
Lec4-408|Here's a clicker question.
Lec4-414|Did anybody get one of those old-fashioned clickers off eBay? Does it work? Okay.
Lec4-416|Going we're up to 41, going, going going oh, yeah.
Lec4-422|Change it back to what you had.
Lec4-424|Going, going, going gone.
Lec4-426|You can share results.
Lec4-427|It's not showing it.
Lec4-430|There were a couple of people said a and B in some sense there.
Lec4-432|Like I took these 28 by 28 MNIST images and turn them into two-dimensions.
Lec4-433|And I did that with supervised learning and it clustered them.
Lec4-434|Okay, but E is the best answer.
Lec4-439|Oops, here's another one.
Lec4-445|So maximum likelihood is kind of a Meta objective function.
Lec4-446|You want to adjust your parameters to maximize the likelihood of the data you see.
Lec4-447|But the particular form of the function you get depends on the kind of distribution you're modeling.
Lec4-448|And a Gaussian distribution leads to SSE and Bernoulli distribution leads to cross entropy.
Lec4-449|A multinomial distribution with one-hot encoding leads to cross entropy for multiple variables.
Lec4-450|And other kinds of objective functions are possible.
Lec4-451|Okay, so that's the end.
Lec4-452|We're going to do backprop.
Lec4-454|So I can MLP.
Lec4-455|Yeah, But that, so a typical way to do dimensionality reduction is that you have some input, maybe some pictures.
Lec4-456|And you go that put that through a narrow channel of hidden units and you ask it to reproduce the input on the output.
Lec4-457|So this is in a sense, unsupervised because the target is the same as the input.
Lec4-460|With one hidden layer.
Lec4-461|Then 92 we did three hidden layers.
Lec4-462|And now they're really big stuff.
Lec4-470|We gave it a set of input, output examples.
Lec4-471|For tomato, the function.
Lec4-473|We only corrected it when it was wrong.
Lec4-475|And it was really slow because learning on some patterns would screw up learning and other patterns.
Lec4-478|I will post them, but they're nearly identical to what was already there.
Lec4-480|Until we can write it this way with an actual delta.
Lec4-482|Networks activation function must be differentiable everywhere.
Lec4-483|Okay? Okay, going, we're up to 47 clickers going, going, going, gone.
Lec4-488|77% of you said B.
Lec4-489|Remember the perceptron? The threshold unit is not differentiable everywhere.
Lec4-490|Again, there is an objective function you can take a derivative of to get the perceptron rule, but I haven't shown you that.
Lec4-498|And in fact, another example is relative units which are like this.
Lec4-499|And they're not differentiable where the hinges.
Lec4-500|So again, the great thing about perceptrons is that anything that can compute, it can learn to compute.
Lec4-501|But lots of things weren't computable.
Lec4-502|This is the cover of the Perceptrons book.
Lec4-503|These are examples of XORs.
Lec4-504|Essentially it's like, is this a closed surface or a, or an open one? But Minsky and Papert said, if you had hidden units, you could compute any Boolean function.
Lec4-505|And that's easily demonstrated.
Lec4-506|You can, given a line of the truth table, you can come up with essentially a perceptron that would just turn on for that line of the truth table.
Lec4-507|I'll leave that as an exercise for the reader.
Lec4-508|So if you can have one hidden unit for every line in the truth table, you can just turn on or turn off the output.
Lec4-511|If you have three-dimensions, you need eight hidden units.
Lec4-512|If you've got four, you need 16.
Lec4-513|So you need an exponential number of hidden units, um, by that construction, but in fact you don't need that many.
Lec4-514|They also said, But no learning rule exists for such multilayer networks and we don t think one will ever be discovered to be fair to Minsky and pampered.
Lec4-515|But they really said was, there's no learning rule with this kind of guarantee that a perceptron has.
Lec4-517|And he said, Yeah, you still haven't done it because backprop is not guaranteed to learn every function that can compute.
Lec4-519|He's dead now so we don't have to listen to me anymore.
Lec4-521|And Chesapeake Bay went over the side of this sailboat.
Lec4-522|Some say it was a grad student who would marry him, but okay.
Lec4-524|Okay, that you can apply some nonlinear functions to the inputs and then do a perceptron to sort of like designing your own hidden units.
Lec4-526|if you use X1 times X2 and XOR, you can train a network, single layer network to do XOR.
Lec4-527|Here, you're figuring out your own features in that case, and that's what computer vision did for decades.
Lec4-531|It turned out to have been discovered in the same year, the Perceptrons book came out by Bryson and Ho.
Lec4-539|So AI people didn't know about it.
Lec4-541|And he showed his thesis to Steve Gross burg at Boston University.
Lec4-544|I didn't do that, but it's not an interesting chapter six, I did that chapter and what he said was not uninteresting, was backprop.
Lec4-545|And so wherever else put it down.
Lec4-546|Then in 198053, different people rediscovered it or groups.
Lec4-547|Rumelhart, Hinton, and Williams at UCSD.
Lec4-548|They were in the Institute for Cognitive Science here.
Lec4-549|Hinton and Williams were post-docs.
Lec4-550|Rumelhart was my post-doc advisor and starting in 85.
Lec4-551|And I was a real letdown from Hinton, I think.
Lec4-553|And David Parker at MIT and undergrad who patented it.
Lec4-555|And it works a lot like the perceptron algorithm.
Lec4-556|You randomly choose an input output pattern.
Lec4-557|You present the input, you get an output, you give it a teaching signal, and then you propagate the error backwards through the network.
Lec4-558|And that's why it's called backpropagation and change the connection strengths according to the error.
Lec4-559|So it looks like this.
Lec4-560|The actual algorithm uses the chain rule of calculus to figure out what the error signal for the hidden units ought to be and go through that if we have time today.
Lec4-561|But the cool thing about it is that it's actually intuitive.
Lec4-562|So if you're using cross-entropy as your air, then the output units used the same delta rule that we've been talking about.
Lec4-565|And this guy sums that up and then multiply that times its slope.
Lec4-566|So you're basically running the network backwards here, taking the deltas and you're propagating them backwards.
Lec4-569|And if you think about it, this makes a lot of sense.
Lec4-571|This is called the credit assignment problem.
Lec4-573|If you have a big error and I have a small weight for you, I don't care as much.
Lec4-574|Or if you have a small error and I have a big weight, I don't care as much.
Lec4-575|If you have a small error in a small weight, I don't give a shit.
Lec4-576|So that's how it works.
Lec4-577|And in fact, one of the interesting things about backprop is it's relatively robust to programming errors.
Lec4-579|The error went down.
Lec4-580|Then it started going up again.
Lec4-581|And Rumelhart said, there's a bug.
Lec4-583|I use a lot with my grad students.
Lec4-584|I'm quoting him a lot.
Lec4-585|And it turned out we stared at the code for like an hour.
Lec4-586|Paul Monroe and I.
Lec4-587|And after about an hour we figured out we forgot to multiply it by the slope.
Lec4-588|So that's why it didn't work.
Lec4-590|Let's talk about forward propagation first.
Lec4-591|And it's fairly straightforward.
Lec4-595|And then we sum that up and I'm going to call that a again, that's my notation.
Lec4-596|This the net input to the unit.
Lec4-597|It's a weighted sum.
Lec4-599|Then you apply your usually non-linear activation function g to that for the hidden units.
Lec4-601|And this could be a logistic function, a tan h or ReLu.
Lec4-603|And we have a bias term here.
Lec4-604|And then we just do that again.
Lec4-605|We do the weighted sum of the hidden unit activations.
Lec4-606|We get a net input to the outputs.
Lec4-607|Then we apply our nonlinear function to that activation function.
Lec4-609|Just jamming a little salami into this.
Lec4-618|So how do we do backprop? It's still a gradient descent.
Lec4-619|Okay? Still the same rule we're trying to go downhill in the error.
Lec4-620|We're going to change our weights so that we're making the error smaller.
Lec4-622|And, but now we need to take into account the hidden units.
Lec4-624|Because remember for the Delta, the Delta times the input on that line.
Lec4-625|So we're going to need something to say.
Lec4-626|What the input on that line as if it's a hidden unit or what the input on that line is, if it's an actual input.
Lec4-628|But if I as an input, it's still z.
Lec4-630|And then we sum all those up and we get an AJ that is the net input to that unit.
Lec4-632|It'll be a little mysterious, but it's just the chain rule.
Lec4-635|Basically, we're going to take the partial derivative of the error with respect to the weighted sum of the inputs.
Lec4-636|And then the partial derivative of the weighted sum of the inputs with respect to this weight.
Lec4-638|Then this was t minus y.
Lec4-639|Okay? So just to recall that this, this guy is the input on that line, That's what we've done a couple of times already.
Lec4-640|So that shouldn't be too mysterious.
Lec4-641|And that's because every term in this sum is zero.
Lec4-642|Except when, in this case, L equals j equals I.
Lec4-646|So this is the input on that line.
Lec4-648|So we still have an input on that line term.
Lec4-649|And that's going to be true whether you're a hidden unit or an output unit.
Lec4-650|Okay? Now we're gonna do a little trick.
Lec4-651|We're gonna define delta as minus the gradient of J with respect to the weighted sum of the inputs.
Lec4-653|You plug that into the, into the schema for gradient descent and you get plus delta z, right? Because it's minus this minus, minus is a plus.
Lec4-656|The j is the delta of the unit that the weight is going into.
Lec4-658|And we change this guy according to delta times z.
Lec4-661|So gradient descent says we do that.
Lec4-662|And we know that for output units with the right objective function and output activation function, we get this t minus y thing.
Lec4-664|And that's again, the stupid thing we did for a few years.
Lec4-668|So you have to take into account.
Lec4-669|So I'm this guy, I'm in the middle of the network.
Lec4-672|And so it's the sum using the chain rule, it's the sum over all those guys.
Lec4-673|The derivative of the error with respect to their input.
Lec4-674|And how their input changes as my input changes.
Lec4-677|So this is the idea you should have in your mind.
Lec4-678|We're talking about one of these weights here.
Lec4-680|We're talking to you about this guy right here.
Lec4-681|Remember, when we're changing the weight, we need the delta of j.
Lec4-683|We need the delta of that unit j.
Lec4-684|And so we need the delta of this guy in order to change the weight coming into it.
Lec4-685|And so we're going to take how the error changes as all these change times, how all these change as my input changes.
Lec4-686|Still got 5 min, I bet I can do it.
Lec4-687|Okay? So moving right along, I'm gonna do another cool trick.
Lec4-688|This guy here is pretty close to how I define delta.
Lec4-690|And then I'm left with this guy.
Lec4-691|And using the chain rule here, I'm going to take how does my input changes? Your input changes.
Lec4-692|My input changes is your output changes.
Lec4-693|How does your output changes? Your input changes.
Lec4-694|This is gonna be the slope term.
Lec4-695|Z is g of a.
Lec4-696|So we're going to have this guy which will be a slope term.
Lec4-699|Sum over I of wi k times z with respect to z j.
Lec4-700|This is gonna be very similar to what happened with some of the weighted sum of the inputs, the input, except this time.
Lec4-701|What happens is that, that derivative becomes just w j k, because it's w1, z1.
Lec4-702|Whenever it's when I equals j, that this is non-zero.
Lec4-703|So that's just this way.
Lec4-706|Because every term in the sum is zero except when I equals j.
Lec4-711|Delta for the outputs is just T minus Y depending on the object, might be something else depending on the objective function and the activation function.
Lec4-712|But that's what it is for most of what we do.
Lec4-714|And then if it's a hidden unit, we take the weighted sum of the deltas of the guys I'm connected to by the, times the weight I'm connected to them by.
Lec4-715|So again, if this is big and this is big, I get a lot of blame for this because I have a big way to them.
Lec4-716|So the matter of error I get depends on the weight I am connected to them by.
Lec4-717|So we have a recursive version of Delta.
Lec4-718|Again, this is the picture to have in mind.
Lec4-719|We take this weighted sum of these deltas and multiply them times the slope.
Lec4-724|You can't get rid of the sub terms.
Lec4-725|This is true no matter where you are in the network.
Lec4-726|This is your, how you compute delta.
Lec4-728|So you're just taking the transpose of the weights and multiplying it times this delta vector to get that weighted sum quickly.
Lec4-729|Another thing to notice about this is that backprop is a linear operation.
Lec4-730|We're just taking a weighted sum times the scalar.
Lec4-732|So it's a linear operation.
Lec4-733|So backprop online backprop works as follows.
Lec4-734|You present a pattern in the network.
Lec4-737|You propagate the deltas backwards through the network.
Lec4-741|They propagated the deltas and then they changed the weights here.
Lec4-742|And then they propagated them on those changed weights, which is not how you're supposed to do it.
Lec4-748|So the slope is the slope up the activation function.
Lec4-750|We're just taking the derivative of a non-linear function.
Lec4-751|If it's not differentiable at all, then you're screwed.
Lec4-754|So it's really, really easy to compute the slope.
Lec4-757|You're watching my undergrad course.
Lec4-761|And I said you should take some other AI course to do research or something.
Lec4-762|Did you get an a? I got it.
Lec4-763|You got to see.
Lec4-765|So yeah, you can audit it.
Lec4-766|I think it'd be good for you.
Lec4-767|Take it and you're doing a lot of the same stuff, but to get them to do it again.
Lec4-768|And hopefully it'll be better this time.
Lec4-770|If you want to.
Lec4-772|Yeah, I need to add use.
Lec6-6|This is a different dataset, but also illustrates the point.
Lec6-7|This is again, principal components analysis of these images.
Lec6-8|And just let me just mention why this worked well for me.
Lec6-10|So That's not very This one.
Lec6-12|So if I have one day, even if this is just two-dimensions of a very high-dimensional space, I can't do PCA of this, right? I only got to a point.
Lec6-13|But if I have two points, then I can do PCA of that and get that line.
Lec6-14|That would be the first principle component.
Lec6-20|If I have three points, then I have maybe two principal components.
Lec6-21|So the number of principal components you can have is the maximum of the number of points you have minus one.
Lec6-22|And the dimension, or I should say dimension minus one probably too.
Lec6-23|But anyway, you mentioned one.
Lec6-24|This one is good.
Lec6-27|And this is the one where Paul Ekman and Wally freezing who invented facial action units or the idea of them.
Lec6-28|Trained subjects to move just the right facial muscles to make the six basic expressions.
Lec6-29|You do PCA of that.
Lec6-30|And I think there's the devil in here somewhere and maybe the Virgin Mary.
Lec6-31|But anyway, so you have, for this dataset, you can't have more than about 14 times eight principal components even though it's 100 by 130 dimensional.
Lec6-32|And again, you can see what we actually give the network is not the images, but the projection onto these principal components.
Lec6-33|So this slider is how much of that principal component, that's an, that's a scalar.
Lec6-35|And if I give it all 20 of these, then I've reduced my 100, 530 dimensional input, which is 13,000 pixels, down to just 20.
Lec6-38|Yeah, that's what I was just explaining.
Lec6-39|So again, if I just have one point, I can't have a principal component.
Lec6-40|Even if it's at one of these images, it's 13,000 pixels.
Lec6-43|Well, it's the mean also in the mode and the minimum and the maximum.
Lec6-44|But if I have two points, then I have at least one principal component.
Lec6-45|So two minus one is one.
Lec6-46|Usually write one plus one is two.
Lec6-49|And if I have three points, then I can have something like two principal components.
Lec6-50|So that's why it's the max of this and this.
Lec6-61|Thank you for pointing that out.
Lec6-62|I'm always happy to be corrected, sort of.
Lec6-63|So again, if you change the first principal component, that's going to mean that's going to make the most changes.
Lec6-64|If I change this last one down here, you should be able to not see very much change a little bit, but not as much as you do if you change the first principal component projection.
Lec6-67|So what I'm going to show you now is doing backprop using these numbers as, as the inputs to the network.
Lec6-70|And due to software, right, I can't make any more principal component things.
Lec6-71|I have to figure out what went wrong.
Lec6-72|But luckily I had some saved.
Lec6-73|So I just loaded our cafe 40.
Lec6-74|And now the system knows that there's 40 inputs because there's 40 principal components and projecting the data onto.
Lec6-76|So I'm going to use those as targets.
Lec6-79|And now I can train it.
Lec6-80|And it very quickly learns learns this problem.
Lec6-81|It's not that hard.
Lec6-83|When I'm trying to recognize facial expressions, the identity of the person is noise with respect to this classification.
Lec6-86|In the group T-shirt for that year.
Lec6-88|But anyway, what this is, this the input, what this is, is a mathematical representation of what the neural network thinks he looks like.
Lec6-89|So it's basically taking the hidden unit activations, projecting them back onto the principal components, and then creating an image for the principal components.
Lec6-90|You gave a very short leash because you chewed up here other one.
Lec6-94|So this is anger that you can I don't know if you can read from the back, but that says anger there.
Lec6-95|Whoops, wait, what happened? This is discussed and it looks nothing like her, but it's got all the markers, the features here of disgust.
Lec6-96|Disgust is basically you make your nose to this and you get these heavy creases here.
Lec6-99|And it's random, which it is.
Lec6-104|I can't have my eyebrows up and be disgusted.
Lec6-106|And so you can think of this as the template for fear.
Lec6-107|You can see the whites of the eyes.
Lec6-108|It looks nothing like her.
Lec6-110|In fact, there's probably more.
Lec6-111|There's some variation due to the identity of the person, but I can make the network do it in a much smaller dimensional space by having it go through five hidden units instead.
Lec6-112|Now there'll be much more.
Lec6-113|It only has five dimensions to represent each one and they'll, they'll look more like each other.
Lec6-114|And this is one of the students that did a lot of this work because now there's an Oregon.
Lec6-115|Again, you can see it looks nothing like him.
Lec6-116|Because again, identity is noise with respect to that.
Lec6-117|You can also set some number of validation and test data and train it again.
Lec6-118|And you can see here that the red is the test and the black is the is the holdout.
Lec6-120|And I can do that multiple times.
Lec6-121|So this is your cross-validation thing.
Lec6-122|And you can see that at least sometimes the holdout set tracks, the not very much here.
Lec6-123|Let me myself more hidden units.
Lec6-125|So the holdout set tracks the test set.
Lec6-127|And it tells you when you're starting to overfit the data, you may not have gotten much overfitting on your homework assignment because it's just a single layer network, doesn't have a lot of ways to overfit.
Lec6-128|But with a hidden layer, it's easier to overfit the data.
Lec6-129|And so this tells me if I do this a few times, gee, maybe I should stop training after about ten epochs, not 200, until I can just change this to ten.
Lec6-130|And that's, that seems about right.
Lec6-132|There's about 20 of these images that aren't in the training set anymore.
Lec6-135|So I can't, Let's see.
Lec6-136|This is she looks angry and it says she's sad.
Lec6-137|That's disgusted, fear, happy.
Lec6-139|Said neutral, and says she's sad.
Lec6-142|So it's learned a representation that's useful for the task.
Lec6-143|Now, I can change the task.
Lec6-144|I can make the task identity.
Lec6-145|And now it knows there are, there are ten output units.
Lec6-146|Now I can train it again and I could give it more than that.
Lec6-149|Now look at what the representation looks like as I go from that looks like her, right.
Lec6-151|So it's learned a representation, again in the service of the task.
Lec6-152|And changes again a little bit as the image changes, but it's basically the same representation, even over all these different expressions.
Lec6-155|Then any questions so far? So this is a demo of learning representations in the service of the task.
Lec6-156|And now we'll do genders.
Lec6-157|And as I gave you a trigger warning last time, there's only two genders in this dataset.
Lec6-158|And I can learn gender with one hidden unit pretty well.
Lec6-159|So it's basically a perceptron problem.
Lec6-162|That doesn't look like her.
Lec6-164|It doesn't change until I switch to a guy.
Lec6-165|As I switched to a different guy.
Lec6-166|It doesn't change at all because I'm just recognizing gender across all the people.
Lec6-171|You can get MATLAB from the UCSD software distribution.
Lec6-173|So it's free for people instead of somebody having to pay.
Lec6-175|And again, that's useful if we don't know what the features ought to be.
Lec6-177|I didn't know what features necessarily you should use for facial expression recognition.
Lec6-178|Then it turns out that this simple network that wasn't trained to fit any data about humans.
Lec6-180|Network very similar to this, I should say, explains a whole lot of behavioral results about how people see facial expressions.
Lec6-181|And what's, I think a little surprising about it as the network has no access to the surrounding culture.
Lec6-182|It doesn't feel anything.
Lec6-184|And yet it finds that just like with people fear and surprise are often confused, anger and disgust are confused.
Lec6-185|It, it has about the same rank order difficulty with each of the facial expressions as people do.
Lec6-186|And if you look at That's if you think about the confusion matrix where there's happy, sad, afraid.
Lec6-191|And again, it's just, it's not feeling anything.
Lec6-192|It doesn't have emotions.
Lec6-195|I thought it was cool.
Lec6-197|That's all on backprop.
Lec6-198|So now you're all ready to do your programming assignments.
Lec6-199|It's going to get put out today.
Lec6-202|Oh, I still haven't looked at it.
Lec6-203|Had to go to the belly up last night, unfortunately.
Lec6-205|So a few notes on improving generalization.
Lec6-206|This is a rather short part of this.
Lec6-207|So how do you deal with overfitting? I'm one of the things the best way to deal with overfitting.
Lec6-215|Was that it was unable to detect joys face because she was black and it was biased towards white people.
Lec6-217|Or it gets biased towards what's in the training set.
Lec6-218|So it's overfit to its training set, which is just mostly white people.
Lec6-219|And it was more guys than women.
Lec6-220|And so the worst it performed was on black women.
Lec6-221|So the other way to get more data as to make it up and a way to make it up.
Lec6-222|There's a few ways to make it up, but standard way is to just take the image and take different crops of the image that moves things around in the image gives us a slightly different context.
Lec6-223|You can also warp the image a little bit.
Lec6-224|If it doesn't change the category of what inside the image.
Lec6-225|You can usually do a mirror image, except if you're trying to recognize text.
Lec6-226|And you can flip, you can.
Lec6-227|Sometimes people rotate that 90 degrees.
Lec6-230|You can add noise like Gaussian noise, zero mean Gaussian noise to the pixels.
Lec6-233|But if you take one of these networks, it's been trained on ImageNet.
Lec6-234|And you take a cat and you give it elephant skin filling in the cat.
Lec6-235|It'll say it's an elephant.
Lec6-236|Looks nothing like an elephant, but it's got elephant texture.
Lec6-237|So in order to get it to be more sensitive to shape, one of the things you can do is randomize the colors that helps some.
Lec6-238|So you can fill in the shape with other things.
Lec6-239|So it's trying to pay attention to the shape.
Lec6-240|But it's interesting because it suggests that we don't have the right architectural priors for networks, that they don't pay attention to shape.
Lec6-241|Another idea is Occam's razor.
Lec6-242|William of Ockham, way back when said, if you have two explanations of the same phenomenon, then you should pick the simpler one.
Lec6-245|And it's got to stop.
Lec6-248|Another way is dropout is invented by Geoff Hinton's group.
Lec6-249|You take some the hidden layer and you randomly turned some things off during training and early stopping, which you have unfortunately a lot of experience with.
Lec6-251|You saw why early stopping is a good thing.
Lec6-252|In my demonstration moments ago where the holdout set tract, the test set.
Lec6-256|So the model is a hypothesis about the function you're trying to fit.
Lec6-257|And we want to make that hypothesis as simple as possible by minimizing the weights e.g.
Lec6-259|This is L2 regularization.
Lec6-260|So you add that as something you're trying to minimize.
Lec6-262|And the derivative of this is to w.
Lec6-263|And then you multiply that by lambda.
Lec6-264|And so what that does when you're trying to minimize it, it's got a minus sign in front of it.
Lec6-265|So you're going to subtract a small amount off the wait.
Lec6-266|Is that a question or a stretch? Okay.
Lec6-268|You came here from Hogwarts.
Lec6-270|Don't don't spell me.
Lec6-271|That makes the weight smaller.
Lec6-275|So you're making the weight smaller at a constant amount.
Lec6-276|Dave Rumelhart idea was to use a function that looked like this.
Lec6-277|And it turns out this penalizes small weights and tries to drive them to zero while leaving big weights alone.
Lec6-279|So you can see L2 regularization is going to penalize big weights more.
Lec6-280|L1 regularization is going to penalize all weights the same.
Lec6-284|So when w is big, this is going to go close to zero or one, sorry.
Lec6-289|And so it's going to actually eliminate some weights.
Lec6-290|L1 regularization tends to actually drive ways to zero.
Lec6-291|And so it really prunes your network.
Lec6-296|Any questions on this? Before? You can imagine lots of other functions you could use, but these are the main ones people use.
Lec6-300|And one thing that does is make the hidden units more independent of one another because you can't depend on the guy next to you who've taken care of part of the problem because he might not be there next time.
Lec6-302|So assume the dropout rate is 0.5.
Lec6-303|So we're probabilistically going to turn off half the units.
Lec6-305|If I have four hidden units, this is the mask.
Lec6-306|I would multiply the output of this one by zero, this one by zero, this one by one, this one by one.
Lec6-307|So you can think of this as a mask on the hidden unit activations.
Lec6-309|So you really looking at not just one model, but all for all six of these.
Lec6-310|And by the way, after using dropout, then things like Py Torch dropout is just to switch, you just turn it on or off.
Lec6-311|Then it automagically cuts the outputs of the hidden units in half.
Lec6-312|If you use 0.5.
Lec6-313|Because now I've got four times as much input or sorry, twice as much input to the next layer up.
Lec6-314|So I better scale that back.
Lec6-315|And 0.5 is just.
Lec6-318|Choose two, okay, I've chosen two different ones to turn off.
Lec6-321|You're not really doing that.
Lec6-322|You'd have to train it some multiple of e to the 29.
Lec6-323|Actually expect to see all of them at some point.
Lec6-324|But you can try, I guess, good luck with that.
Lec6-326|You can add noise to the inputs.
Lec6-327|You can add Gaussian noise to the hidden unit activations.
Lec6-328|And what this does is it makes the model not depend on any particular exact value of a hidden unit.
Lec6-330|So to improve generalization, there's early stopping getting more data or making more data.
Lec6-331|Another way to make more data's to train a GAN.
Lec6-332|But we're not going to talk about that.
Lec6-334|Add noise, the input or the model or even the output.
Lec6-337|Where did it go? It's right there.
Lec6-343|It's the first chapter of this book.
Lec6-345|Somebody just made that up.
Lec6-346|Essentially, it did follow from previous work, but still was that a question back there? I thought I saw your hand up.
Lec6-347|Well, one way of talking about simple is, you know, like how heavy is it? If you have smaller weights? G, zip your model, you'll have a smaller file.
Lec6-348|Right? So if you think about another way of talking about this is minimum description length, risks, and then came up with this idea as a model of complexity.
Lec6-349|So how many bits does it take to describe your model? And driving some weights to zero makes the model smaller.
Lec6-357|Certainly driving some weights to zero makes the model smaller, right? You don't have to write down as many numbers.
Lec6-358|You can use some compression algorithms and make the model smaller.
Lec6-364|That makes some sense.
Lec6-369|The Slavic name, I think.
Lec6-371|Anyway, there's a great blog out there.
Lec6-372|And one of them is about how to train a neural network.
Lec6-373|And we'll talk about that later.
Lec6-374|But for this lecture, this is from the first chapter with Gian lacuna is the first author.
Lec6-375|So some of these tricks are ancient at this point.
Lec6-376|But they illustrate a lot of issues and they motivate batch normalization, which is a more modern version of some of these ideas.
Lec6-379|You compute the gradient, you add it to a running average of the gradients.
Lec6-380|And if you've seen all the examples now you change the weights and then you go back to step one.
Lec6-381|This is following the true gradient for your dataset.
Lec6-382|You're gonna go exactly.
Lec6-384|If this is the weight and that's the way the error surface is a function of those.
Lec6-390|Once you've figured out which way is downhill, you can search along that direction until you find the bottom, rather than using a learning rate.
Lec6-392|And that makes the model kind of stiff.
Lec6-393|It doesn't generalize as well.
Lec6-395|Where you get one example.
Lec6-396|You compute the gradient, change the weights.
Lec6-397|And if you've seen all the examples, shuffle them so you're not doing exactly the same sequence the second time.
Lec6-398|And this, because if the true downhill is here, and that leads to a local minimum.
Lec6-399|But I changed my weights based on one example.
Lec6-400|I might be able to get around that local minimum because it's adding some stochasticity in expectation.
Lec6-401|You're going to be going down the right way.
Lec6-404|So you can learn a lot about the problem before you've seen them all.
Lec6-405|It tends to get better solutions and have better generalization.
Lec6-406|For the reason I just said.
Lec6-407|You can adapt to a changing environment.
Lec6-408|If you are getting examples on the fly, people start using the European seven instead of regular seven.
Lec6-409|It'll adapt to that.
Lec6-410|And it can be used for very large datasets because you don't have to go through all of them at once.
Lec6-411|So batch learning is good because it's well understood by mathematicians.
Lec6-412|There's a fair amount of theory involved.
Lec6-415|Conjugate gradient is kind of a one-and-a-half order technique.
Lec6-416|It's not really second-order, but conjugate gradient.
Lec6-417|If you know what you have is a bowl.
Lec6-419|And conjugate gradient makes that assumption.
Lec6-421|The thing about batch learning again is it can lead to very large weights which make the model and not generalize very well.
Lec6-423|You'd like to get a big enough mini-batch to have a good sample of the data.
Lec6-425|So you get a better sample of what the function is.
Lec6-426|And you don't wait until the very end of the dataset to change your weights.
Lec6-427|Some people, I was talking to a friend of mine the other day said some people just call this batch now, but it's not real batch.
Lec6-428|And this can be very efficient on GPUs.
Lec6-429|One of the things about batch learning is you're running all the examples through the network with the same weight.
Lec6-430|So you can use data parallelism in a minibatch.
Lec6-432|And then you can do that in parallel.
Lec6-433|That's still called stochastic gradient descent because it's still as a stochastic component.
Lec6-434|It's not, you're picking some subset of the data and so that it's not going down the true gradient.
Lec6-439|You can say that.
Lec6-440|Right? Okay, So shuffling the examples.
Lec6-441|This is for SGD.
Lec6-442|It wouldn't make any difference at all in batch learning if you shuffle the examples because you look at the mall anyway.
Lec6-443|And what that means is what we hope to do with that as get a good sample of the data.
Lec6-447|You want to make sure that the mini-batch is diverse.
Lec6-448|And a third possible heuristic here, but little.
Lec6-450|Some things are mislabeled.
Lec6-452|You wouldn't want to necessarily trained more in those outliers.
Lec6-453|Or it might be that you need to learn something about the simple examples before you learn things about the hard ones.
Lec6-454|But it can improve things like is a very rare phoneme in English.
Lec6-455|And so you might want to train more on.
Lec6-456|Another example of this is de-biasing the dataset.
Lec6-457|So you could, you can take your small sample of black people, do a whole bunch of modifications to those to generate new data and try and balance the categories in the dataset.
Lec6-461|I mean, where do I say that? Okay.
Lec6-464|There is something called ImageNet real where somebody actually went through and made sure all the labels are right.
Lec6-466|But it can be useful in some situations.
Lec6-467|But first, a little intuition.
Lec6-470|They're going to be 0-255, right? Let's what if all the inputs to one hidden unit are positive.
Lec6-471|So we take this weighted sum and then we run it through the activation function.
Lec6-472|Where do you, what could be wrong with that? There's a clicker question coming up.
Lec6-474|But we might as well, I guess just to get some participation points, yes.
Lec6-477|I haven't showed you the question you need to answer.
Lec6-480|Read that question again and reconsider your answer.
Lec6-481|Some people are reconsidering.
Lec6-482|This is where it had sent telling me that percent.
Lec6-484|Let's let's stop here and talk to your neighbor.
Lec6-491|We're ready to try again.
Lec6-495|Answer is not C or E or D.
Lec6-500|There's more than that.
Lec6-501|More clickers out there than that.
Lec6-502|Okay? Some people are not going to get all their participation points.
Lec6-503|Okay? Going, going, going gone.
Lec6-504|Okay, between true and false.
Lec6-510|What is wrong with all positive inputs? Remember the weight change rule, this the way change rule.
Lec6-518|We're just trying to learn a set of weights that solves the problem for us.
Lec6-525|I will reveal the answer.
Lec6-527|Delta is part of the weight change roles.
Lec6-528|So you're multiplying delta times this input to change this way, delta times this input to change this weight.
Lec6-529|All the way changes are gonna be either raising the weights, are lowering the rates, depending on the sign of delta.
Lec6-534|And that's what this network has to do.
Lec6-536|So my weight changes.
Lec6-537|I have to go back.
Lec6-539|And that's what this has to do called tacking.
Lec6-542|So I'm just assuming, I should say what this is a picture of.
Lec6-545|This is the bold green dot is the bottom of the bowl that I'm trying to get to, right? It's the error surface and the green dot is the optimal location from here.
Lec6-546|I'm trying to get there.
Lec6-547|But I can only move like this.
Lec6-552|They can all get smaller, they can all get bigger.
Lec6-556|You would really like is independent information.
Lec6-557|And you can't get independent information, but you can get decorrelated information by doing PCA.
Lec6-558|So with PCA, I might have two-dimensions here, one being size and one being adiposity.
Lec6-559|And that's your new word for today.
Lec6-560|Fatness, adipose tissue, I've got a whole lot of it.
Lec6-562|Every way down here is lighter on average for their height.
Lec6-567|Remember what the weight change rule is involves x.
Lec6-569|Some other ones that vary between -1.1.
Lec6-570|What's going to happen? If these are both important to learn about what's going to happen.
Lec6-576|And again, if they're equally important and I will take a long time.
Lec6-578|In fact, on average there are about equally positive and negative.
Lec6-580|You're basically projecting the data onto the eigenvectors of the covariance matrix of the data.
Lec6-581|And then this step isn't part of PCA, but you could.
Lec6-582|In PCA, the eigenvalue and eigenvector is the variance of along that eigenvector.
Lec6-583|So you take the square root of that to get the standard deviation and you divide by the standard deviation.
Lec6-584|And now I've widened the data.
Lec6-587|So presumably faster learning.
Lec6-588|So this last step I mentioned is not part of PCA, but it's good for neural nets because it basically makes every, all the variables roughly the same size.
Lec6-589|The scoring, on the other hand, shifts the mean of the inputs of the variables to zero.
Lec6-590|But it skips that point and goes directly to the whitening.
Lec6-591|You can't throw away any dimensions with Z scoring, but it's much cheaper.
Lec6-593|I assume you all know what Z scoring is.
Lec6-594|It means you, instead of the data that you got, you take the data and subtract the mean of the data and divide by the standard deviation.
Lec6-595|And that makes all your variables have zero mean and unit standard deviation.
Lec6-597|And that's, there's two kinds of normalization in neural nets.
Lec6-598|One called layer normalization, where you go across a layer and you make layer means zero and unit standard deviation across all the activations.
Lec6-599|And then there's batch normalization.
Lec6-600|So another way to normalize your data is to normalize each variable.
Lec6-602|But for images that's going to tend to amplify noise because e.g.
Lec6-603|maybe you have slight differences in some of the black areas of m-nest.
Lec6-604|But it's going to take that black pixel and make it zero mean and unit standard deviation.
Lec6-605|And that's just noise.
Lec6-606|You've much, it's much better to do the scoring across the image.
Lec6-607|First of all, it's cheaper.
Lec6-609|And it tends to normalize if you have some images that are very bright and some images that are very dim, it normalizes the brightness and it also normalizes the contrast.
Lec6-611|Mostly all people do is subtract the mean of the dataset.
Lec6-616|Layer normalization is like what I just described.
Lec6-617|There's also, I think in your career, your assignment, you did min-max normalization.
Lec6-619|I hope Z scoring work better.
Lec6-620|I don't know, maybe not for such a small dataset and easy problem.
Lec6-622|Side note, number one, a linear auto encoder essentially does PCA.
Lec6-623|So I haven't really talked about an autoencoder much yet, but I did an image autoencoder in 1986, probably before your grandparents were born.
Lec6-624|So what an autoencoder is, again, is in our case, we took in an image patch and usually it was like eight by eight or 16 by 16.
Lec6-625|And you run that through a narrow channel of hidden units.
Lec6-626|And then you try and reproduce what you see on the output.
Lec6-627|And this was, you know, we had one hidden layer back then.
Lec6-628|And basically, what happens is that what the network is gonna do is if there are five units here, each unit.
Lec6-629|And PCA provably minimizes the squared error of the data.
Lec6-630|The number of components you keep.
Lec6-636|Which makes sense here because it's a regression problem.
Lec6-637|You're trying to predict pixels, so you'd have a linear output.
Lec6-645|All the way down.
Lec6-649|The first five dimension of the subspace.
Lec6-650|They won't line up perfectly with the principal components.
Lec6-651|There won't be the first principal component, the second, the third.
Lec6-652|But like if these are the principal components in 2D, that will span that space.
Lec6-653|So they'll spread the variance a little bit among, among themselves.
Lec6-655|You should use a neural net to do it and gradient descent, rather than trying to find the covariance, the eigenvalues of a huge covariance matrix.
Lec6-656|There are ways of making this find exactly the principal components.
Lec6-657|It's called the generalized heavy an algorithm.
Lec6-658|And you just train one unit first, and that becomes the first principle component.
Lec6-659|Then you subtract its activations from the next guy and train the next guy, and that becomes the second principal component.
Lec6-661|And taking the input times that weights is giving you the coordinate on that vector.
Lec6-662|Just like principal component.
Lec6-663|Side note number two, seems to contradict all this.
Lec6-665|So those concerns seemed to be not as important in deep networks, but batch normalization, which essentially Z-scores the inputs to your unit.
Lec6-669|There are decorrelated and now we put them into a sigmoid.
Lec6-670|What happens? What will the activations be? Sigmoid goes 0-1.
Lec6-672|That's bad, right? So we need to change the Sigmoid to have the same properties as before, because these are inputs the next layer up.
Lec6-674|which has negative and positive outputs is better.
Lec6-677|So it's 1.7 159 times tan h of two-thirds x y this sigmoid.
Lec6-679|Then, first of all, 0.0 plus one and minus one plus one, minus one n -1 h and hoops.
Lec6-680|And it has an effective gain of about one over the linear range.
Lec6-681|And the second derivative is maximum at x equals one where it's starting to curve.
Lec6-682|The slope is changing fastest there.
Lec6-685|We'll figure this out someday.
Lec6-686|Okay? So another thing to consider is initializing the weights.
Lec6-687|So you want to initialize the weights so that these properties have just talked about are achieved at the next level up.
Lec6-688|We need them to be random and not zero like we do with regression.
Lec6-691|Imagine you're using sigmoidal units, but they're not all like that.
Lec6-692|But why can't the weights start at zero? Let's for the moment, assume a logistic hidden unit function.
Lec6-693|I might ask you what will happen with a tan h on a test, but okay.
Lec6-694|So what's the activation of a sigmoid at zero? It's 0.5.
Lec6-696|And they're going to have exactly the same activation, 0.5.
Lec6-697|On the other hand.
Lec6-698|And at the output level, when you get there, the outputs are going to generally have different targets.
Lec6-699|So they'll have different deltas.
Lec6-700|And now I've got 0.5 across here, that's the input on that line.
Lec6-701|I'm going to have different deltas up here.
Lec6-702|And the weights change according to delta times the input.
Lec6-704|This guy is going to have a different set of weights because it's delta is different, but they're gonna be exactly the same because it's multiplied times 0.5 to change.
Lec6-705|And similarly here, the incoming activations from each hidden unit will be the same.
Lec6-707|All be the same because they all have the red, green, and black weights.
Lec6-709|And so we're gonna get a similar kind of thing where the outgoing weights from each input will be the same.
Lec6-710|So this input would have the same weights to everybody here.
Lec6-712|And what's going to happen is the hidden units are all going to compute the same feature.
Lec6-713|So you really going to have one hidden unit.
Lec6-715|So again, we want the weighted sum of the inputs to be in the linear range of the sigmoid.
Lec6-716|So we get the biggest bang for the buck than the gradients will be largest.
Lec6-717|If we hit the rails of the sigmoid, we get zeros.
Lec6-719|Can learn any linear part of the mapping before any non-linear parts.
Lec6-720|So a lot of mappings will have linear components, especially if you're doing a regression.
Lec6-721|So we want the net inputs to the units to be zero mean and unit standard deviation with funny tan h.
Lec6-724|We also want this to be true for the outputs, the first hidden layer.
Lec6-726|That is, this is the weighted sum of the inputs to all the units.
Lec6-728|This is just the expression for variance.
Lec6-729|So the variance of the input is one, because the standard deviation is one.
Lec6-732|So we end up with the variance of the weights.
Lec6-733|And so the standard deviation of the weighted sum of the inputs will be the square root of the variance of the weights, which has, since they're independent there, it'd be the square root of the sum of the squares.
Lec6-734|And so we've figured out that the standard deviation or the standard deviation of the, of the weighted sum of the inputs is this.
Lec6-736|Because when you plug that back in, you got one.
Lec6-737|It turns out this all had to do with the funny tan h and all this other stuff.
Lec6-740|So here's an example of Xavier initialization noted as the square root of the fan and the fan out.
Lec6-742|So yeah, So that's it.
Lec6-745|I couldn't get it going.
Lec6-746|You couldn't get it to work for the second question.
Lec6-755|I just wanted to talk about ago eBay.
Lec6-757|Hold on a second.
Lec6-765|Where's the person who asked the question? Yeah.
Lec6-766|Usually, what people do is they have a little constant in in the function.
Lec6-767|So that like for softmax, you want to have a little constant in the bottom.
Lec6-769|It depends where it happens, but like taking a log e.g.
Lec6-770|you add a little 0.0 001 to never taking log of zero.
Lec6-773|I would look at.
Lec6-777|That shouldn't be zero.
Lec7-1|Okay, let's get started.
Lec7-6|So weights can only change in one of two directions.
Lec7-7|They can only go this way, but not this way.
Lec7-8|Okay? So that's going to slow things down.
Lec7-9|Correlated inputs or a PCA or as the scoring.
Lec7-11|Starting with all zero weights gives you just one feature.
Lec7-12|And then we started to talk about weight initialization.
Lec7-13|And I went through this little thing very quickly.
Lec7-15|If we initialize the weights that means zero, then the net inputs to the units that is a across all the x's and w's.
Lec7-16|The variance of that.
Lec7-17|We've got the weights are zero mean and unit standard deviation.
Lec7-18|So this is one.
Lec7-19|This is the expectation of the weights is zero.
Lec7-20|The expectation of the inputs is zero.
Lec7-21|So we just end up with the variance of W.
Lec7-22|Okay? So the standard deviation of the weighted sum of the inputs is the square root of that.
Lec7-23|And if we, and because the weights have mean zero, then the, the variance of W is just the sum of the squares of the weights because again, mean is zero.
Lec7-24|And so we figured that out.
Lec7-26|Small, they are just how many there are.
Lec7-27|If you have a small number of weights coming in, each one is going to have a bigger effect.
Lec7-28|If you have a large number of weights, each one is going to have a smaller effect.
Lec7-29|And so for each particular unit in the network, which May 1 layer may have the same fan-in, but different units in different layers may have different fan ends.
Lec7-30|So if we set the standard deviation of the initial weights to be one over the square root of that.
Lec7-31|And you figure out what that is, you get that the standard deviation is one.
Lec7-32|So we wanted our things to be zero mean and unit standard deviation.
Lec7-33|And this makes the, by initializing the weights this way we get standard deviation of one.
Lec7-34|Okay? And so if the inputs have been normalized to zero mean and unit standard deviation, and we use that funny, tan H.
Lec7-35|If we draw the weights from this distribution, then we get zero mean and unit standard deviation of the inputs, which will, for this tan h will make the output zero mean and unit standard deviation.
Lec7-36|So we've normalized everything.
Lec7-37|Okay? Alright, so we get normalized inputs to the next layer up that way.
Lec7-40|The standard ways of initializing weights in in PyTorch and TensorFlow.
Lec7-43|You're shaking your head.
Lec7-45|But they can't both be right? It's exclusive or not.
Lec7-47|Anyway, both of these have a square root of the fan-in and fan-out in this case.
Lec7-48|And just the fan in, in that case.
Lec7-50|But all of this careful initialization won't survive.
Lec7-53|So that's where batch normalization comes in.
Lec7-54|And batch normalization uses all of those insights.
Lec7-55|But dynamically during training, during network training.
Lec7-56|Let's see, that was 40,500 citations as of 11:00 last October.
Lec7-57|So what this does is it normalizes all of the inputs in the network.
Lec7-58|That is, those net inputs.
Lec7-59|The A's, the weighted sum of the inputs on a per unit basis over each mini-batch.
Lec7-60|So what batch normalization does is it basically z-scores each variable.
Lec7-62|What batch normalization does is z-score that weighted sum of its inputs over the mini-batch.
Lec7-64|And that's for each.
Lec7-66|So you can think of batch normalization and they do in these neural net platforms.
Lec7-67|It's, it's a layer.
Lec7-68|Actually, if it's a neural network layer, it's a kind of a weird layer, but it's a layer and it does this normalization over the mini-batch.
Lec7-69|So anything after this layer, all the units beyond this layer will have zero mean and unit standard deviation inputs over the mini-batch.
Lec7-71|But suppose that's not what you want or not what the network wants, which is trying to change the weights in order to find features that solve the task.
Lec7-72|It also has a step where it allows it to undo this.
Lec7-73|So the first thing it does is when I say input variable, again, I mean the weighted sum of the inputs to a unit.
Lec7-76|And again, this could be at any layer of the network.
Lec7-78|So you get, you get a new weighted sum of the inputs.
Lec7-79|And then you can change it by a scalar, multiplication by a scalar and addition of a beta.
Lec7-80|So these are now adaptive parameters that can be learned through the magic of gradient descent.
Lec7-81|And this whole thing starting with this and this are differentiable.
Lec7-82|So you can be, this batch normalization layer can be backpropagated through.
Lec7-83|And this is straight out of the paper.
Lec7-84|It doesn't exactly use our notation.
Lec7-85|So let me use my pointer.
Lec7-87|I really should redo this in our notation.
Lec7-88|I'll do that before I post these slides.
Lec7-90|And then it's got these two parameters.
Lec7-91|And the output is the batch normalization of that.
Lec7-92|Okay, So over the mini-batch, so first you find the mean of the weighted sum of the inputs.
Lec7-93|You subtract that from each weighted sum of the inputs squared average, that's the variance.
Lec7-96|So this is just a little thing to avoid divide by zero.
Lec7-97|Then the final weighted sum of the inputs of the units is this times gamma plus beta.
Lec7-99|Okay? So you can undo this by the appropriate choice of Gamma and Beta.
Lec7-100|And so presumably gamma and beta are initialized something like 1.0.
Lec7-101|Okay? And you can insert this anywhere you want it in Py Torch.
Lec7-104|So I went through this whole thing about zero mean and unit standard deviation.
Lec7-105|And that was all about initializing the network.
Lec7-106|Now we can do it while we're learning.
Lec7-107|The soft underbelly of this is that now when I go to test set, I don't have a mini-batch.
Lec7-109|When this came out.
Lec7-111|People out there in network land also tried to do it to the outputs of a layer.
Lec7-112|And sometimes that seemed to work better.
Lec7-120|And in the middle of a network that's called layer normalization.
Lec7-123|It's so it's the title of the paper is is avoiding covariate shift.
Lec7-124|And nobody seems to know what that means.
Lec7-125|Internal covariate shift, sweat, they call it.
Lec7-127|I don't think it has.
Lec7-129|So I think for all the reasons we talked about leading up to this, That's the intuition you should use.
Lec7-133|Now I've told you everything I knew about that.
Lec7-135|So we're going to first, we're in the last two things here.
Lec7-137|What did you do? He lost it.
Lec7-138|He can't reach it.
Lec7-142|All of his Coursera lectures are now on YouTube from this relatively older course that he gave.
Lec7-143|So what momentum does is, instead of using the gradient to change the weights, we keep a running average of the gradient over the minibatches.
Lec7-144|And we use that to change the way that, that kind of, you know, we're gonna get a lot of variation if we're doing stochastic gradient descent over the, over the different examples.
Lec7-145|And this is going to keep track of the general direction that the gradient is going.
Lec7-147|Then the other thing is to use separate adaptive learning rates for each weight in the network.
Lec7-148|So now, instead of just storing all the weights, you're storing all the weights.
Lec7-149|And for every weight, you're storing a learning rate.
Lec7-150|So now you've just doubled the amount of memory you need.
Lec7-151|You got to consider that.
Lec7-152|And we're going to adjust the learning rate based on the consistency of the gradient.
Lec7-155|That could mean that we're oscillating in a bowl like this.
Lec7-156|And we might want to lower the learning rate.
Lec7-157|But if the learning rate over multiple mini-batches is always positive, if the gradient over multiple, multiple mini-batches is positive, we may want to increase the learning rate and go faster down in that direction.
Lec7-158|So first some intuition.
Lec7-159|So remember what the error surface looks like for a linear network.
Lec7-160|So if we're doing linear regression, remember that the mean squared error cost function is a quadratic.
Lec7-161|And so it looks like this.
Lec7-162|This is a cross-section for one weight here, right? In 2D that looks like an ellipse.
Lec7-164|This is the error surface.
Lec7-165|And again, it's going to be a quadratic bowl for Mean Squared Error.
Lec7-167|The, the error surface can be approximated by a quadratic.
Lec7-168|So for real error surface, we're going to have local minima.
Lec7-169|We're going to have plateaus.
Lec7-171|So for here, on this flat spot, it's going to take a long time to get off the plateau because we don't have much of a gradient, it's near zero.
Lec7-172|Then there's saddle point.
Lec7-173|So I doubt a lot of you are horseback riders, but you know what a saddle looks like.
Lec7-174|And a saddle point is where you've come down this way.
Lec7-175|And now there's two ways to go here in the gradient is basically zero in both of those directions.
Lec7-176|Because you're on a flat spot in the middle of the saddle.
Lec7-177|But again, locally a piece of a quadratic is usually a good approximation.
Lec7-178|Okay, here's a clicker question.
Lec7-180|I click on that and nothing happens.
Lec7-181|So we'll go to I don't know why that's happening.
Lec7-182|I tried unplugging and plugging this back in.
Lec7-184|Yeah, maybe it is gonna do something now.
Lec7-185|I'm getting a little.
Lec7-188|Okay, I got it.
Lec7-201|Should be pretty easy.
Lec7-202|The answer is not E.
Lec7-203|54, people, 56, 58.
Lec7-204|You just got here, you're turning on your clicker.
Lec7-205|You are hoping to get in there.
Lec7-208|Anybody else want to vote? Going, going, going gone.
Lec7-210|What's the answer class? True? Okay, good.
Lec7-214|I usually pull out a piece of paper at this point.
Lec7-215|I don't have a piece of paper.
Lec7-220|Get more than one.
Lec7-221|You've got some notes and one of them, this here.
Lec7-224|So gutter essentially going downhill.
Lec7-225|And you're here on the Earth's surface, which, so the minimum is down here somewhere, right? But if you're up here, which way does the gradient point? It points this way.
Lec7-227|And then it's, the gradient is pointing this way.
Lec7-228|So you're gonna go bump, bump, bump, bump.
Lec7-230|And very slowly in a direction you do want to go.
Lec7-231|Okay? Momentum fixes that.
Lec7-232|So if I take this gradient which is, let's say positive and this one's negative and this one's positive and one's negative.
Lec7-233|I keep a running average of those.
Lec7-234|I'm going to get something that points this way because they're all going down this way, but just zigzagging.
Lec7-235|So keeping a running average of that is going to point this way.
Lec7-236|And I'm going to learn faster.
Lec7-237|And that's what momentum does.
Lec7-238|Okay? Unless this is a circle and then everything's great.
Lec7-239|But that's usually going to happen.
Lec7-240|The gradients big in the direction.
Lec7-244|So what we want to achieve is to move quickly in the directions with consistent gradients and go slowly in the direction of ones with inconsistent gradients.
Lec7-245|So what I mean by consistent is if I'm going back-and-forth this way, if you take I've got a small consistent going this way.
Lec7-246|And I have an inconsistent amount.
Lec7-248|So imagine you've got a ball which has momentum or inertia.
Lec7-249|And it's, a ball is not going to do that.
Lec7-250|It's going to eventually settle down and go straight down.
Lec7-251|So that's the idea.
Lec7-252|So the ball starts out by following the gradient as it does here.
Lec7-253|But if we keep averaging, these will end up going down in the direction you want to go.
Lec7-254|Okay? So that's the intuition.
Lec7-255|It's damping these kinds of oscillations.
Lec7-256|It, it builds up speed and the direction with a small but consistent gradients.
Lec7-257|So again, keep in mind this guy.
Lec7-260|After all, he did come up with the Boltzmann machine at some point.
Lec7-261|Points out that this really velocity, momentum.
Lec7-262|But these are the equations.
Lec7-263|You take some fraction of the previous gradient, this guy, and you subtract from it the current gradient.
Lec7-264|So you're combining these two by alpha and eta, the learning rate, the momentum term in momentum usually has a number like 0.9, 0.99 in front of it.
Lec7-265|So that's what alpha would be.
Lec7-267|And that's the way change.
Lec7-268|So we take some fraction of the previous running average and we add in the current gradient.
Lec7-269|So that's, that's an exponentially decaying average of the gradient.
Lec7-270|Because if you plug in the old one here, you'll get alpha squared, et cetera.
Lec7-273|I'm just plugging in that, which is the old way change, right? So again, we're keeping a running average of the weight changes.
Lec7-274|And to think about this, what we're gonna do now is imagined that the error surface is a plane.
Lec7-275|And so the gradient is always the same.
Lec7-276|Okay? To get some more intuition.
Lec7-277|Okay? So if you take this equation here and you, you plug in that, the gradient is always the same and solve it as a recurrence relation.
Lec7-279|Oops, you get this.
Lec7-280|So this is the momentum term, this is the learning rate.
Lec7-281|So if this is 0.99, say, then this term is 0.01.
Lec7-283|So he recommends, he has a different recommendation than I do, which I would just start with 0.99 or something and keep it that way.
Lec7-284|He suggests starting with a small momentum and increasing it.
Lec7-286|And most people nowadays use something like Adam, is there optimizer? Adam stands for adaptive momentum.
Lec7-287|So it takes into account these kinds of considerations.
Lec7-288|We're learning much faster and we're learning at a rate that would normally oscillate if we didn't have momentum.
Lec7-291|And it has this weird thing about it.
Lec7-292|That we take this running average of the gradient and we move in that direction.
Lec7-293|Okay? Then we subtract off the gradient from where we were.
Lec7-294|If we add this into the gradient and then we do this, this is from where we were.
Lec7-295|And that motivates Nesterov momentum.
Lec7-296|So Nesterov was a mathematician or was a mathematician and I don't know if he's still alive, but he's working in the field of optimization.
Lec7-297|And so our technique computes the gradient at the current location and then takes a big jump in the direction of the accumulated gradient.
Lec7-299|It seems like we could do better if we took this long jump and then computed the gradient there and moved.
Lec7-300|So you make a first big jump in the direction of the accumulated gradient.
Lec7-301|Then you measure the gradient where you are and adjust it.
Lec7-302|So it's better to correct a mistake after you've made it as the intuition.
Lec7-303|So this is a picture of the Nesterov momentum.
Lec7-304|And so this is the, the jump with the accumulated gradient.
Lec7-308|And now we're going to compute the gradient at that location.
Lec7-310|Compare this to standard momentum, where you compute the gradient where you are, and then you take a big jump.
Lec7-311|So it seems like this is better and it turns out it generally is.
Lec7-316|In a multiple layer network.
Lec7-317|The appropriate learning rates can vary widely.
Lec7-318|And this is again part of the intuition for the weight initialization.
Lec7-320|The effect of the weights is going to vary.
Lec7-321|And this is an old intuition that gradients can get small and the early layers of a network that's if you have sigmoidal units.
Lec7-323|So if you have big weights, if you have a lot of weights and you have to high learning rate, you're going to change the net input quite a bit.
Lec7-324|So the way that you do this is you have a global learning rate that you set by hand.
Lec7-325|And then you adapt it by multiplying it times the gain factor that's empirically determined for every way.
Lec7-326|So this is the idea.
Lec7-327|So here's the learning rate that I've set by hand.
Lec7-328|G is my adaptive part.
Lec7-330|Right? Now we're going to increase it if the gradient for that weight doesn't change sign if we keep adding to it.
Lec7-331|We're gonna decrease it if the gradient for that weight is changing sign.
Lec7-333|In that case, we're going to increase the learning rate by an additive factor.
Lec7-334|Otherwise we're going to decrease it by a multiplicative factor.
Lec7-335|So if the gradient was totally random, and you repeat this process over and over again, it's going to hover around one.
Lec7-336|So you can try this at home.
Lec7-338|And every time it comes up negative subtract, multiply it by 0.95.
Lec7-339|And if you do this, you can write a little program to do it, right? And you could see what happens.
Lec7-340|It should hover around one.
Lec7-342|And then again, irritated, tingly empirical.
Lec7-343|You could try and limit these gains to some range.
Lec7-346|Okay? And then you can combine this with momentum by using the agreement in sign between the momentum and learning rate.
Lec7-347|And that's what Robbie Jacobs did back in 1989.
Lec7-350|Momentum does not care about the alignment of the axes.
Lec7-351|What the heck does he mean by that? Took me awhile to figure that out.
Lec7-352|Let's go look at this.
Lec7-353|These are the axes.
Lec7-355|This the error surface.
Lec7-356|Adaptive weight changes are only going to deal with these axes because they're changing the learning rate for await.
Lec7-357|Okay? Momentum doesn't care about that.
Lec7-360|That's, that's the story I'm sticking to it.
Lec7-365|The idea was that the magnitude of the gradient can be very different for different weights and can change during learning.
Lec7-366|That makes it hard to choose a single global learning rate.
Lec7-367|And for full batch learning, we can deal with this by only using this sign of the gradient.
Lec7-368|So this is resilient backprop.
Lec7-370|So what does that mean? What it means is we're essentially dividing the gradient by its magnitude.
Lec7-371|So it has this weird thing where this time it's increases the step size multiplicatively if the signs of the last to agree and it decreases it multiplicatively if they disagree.
Lec7-373|This is a heuristic algorithm.
Lec7-374|It only applies to full batch learning, so it doesn't work with mini batches.
Lec7-376|So suppose I have a weight that gets a plus 0.1 on nine mini batches and a -0.9 on the tenth mini batch.
Lec7-377|So it should stay about where it is.
Lec7-378|But if I use our prop for this, I'm going to do something quite different.
Lec7-379|I'm going to increase it by eight because I'm only increasing it by the sine.
Lec7-381|Increase the weight nine times and decrease at about the same amount.
Lec7-382|And the weight would grow a lot, which it shouldn't.
Lec7-383|So is there some way to combine the robustness var prop and the efficiency of minibatches and the effect of averaging of gradients over mini-batches.
Lec7-385|Rms prop, I should say.
Lec7-386|Our prop is equivalent to dividing the gradient by the size of the gradient, right? That gives you the sign plus one or minus one.
Lec7-388|So why not force the number we divide by to be similar over adjacent minibatches.
Lec7-389|That's the intuition between the intuition behind RMS prop.
Lec7-390|We keep a moving average of the squared gradient of each way.
Lec7-393|And that makes the learning work much better.
Lec7-395|And it was a technique for having a totally adaptive learning rate.
Lec7-396|And it had a term in it which was division by the square root of the mean, square root of the, of the gradient.
Lec7-397|So these ideas kind of converged.
Lec7-398|Okay, So summary for learning methods for neural nets at grain Geoff Hinton.
Lec7-399|For small datasets, you could use full batch learning or large minibatches.
Lec7-400|You can use one of these second-order techniques.
Lec7-402|And you should use gradient descent with momentum.
Lec7-404|Try whatever you on the Kuhn is doing at the moment.
Lec7-407|And all of those may have different techniques that work with them.
Lec7-410|Some require very accurate weights, some not so much.
Lec7-412|Any questions about that stuff? It's a little this is the grad version in the undergrad.
Lec7-415|That was a year after AlexNet won the ImageNet Large Scale Visual Recognition thing.
Lec7-419|I think this is right.
Lec7-420|They added a chapter.
Lec7-421|Some point, it might be six and it might be 8.9.
Lec7-425|And it's 11, 40 by 648.
Lec7-426|So it's got 745,000 pixels.
Lec7-427|So suppose I just did what you guys are doing and took the network and then fed it into a bunch of hidden units.
Lec7-428|How many parameters, what I need for that? Well, 74 million input to hidden weights, that seems a bit much.
Lec7-429|So we're going to take into account, we're going to use what's called an architectural prior.
Lec7-430|We're going to take what we know about the visual world and some of what we know about how the brain works.
Lec7-431|And we're going to design our network to have those properties to reflect those properties.
Lec7-432|And most people give you two properties are three properties.
Lec7-433|I give you four properties.
Lec7-434|So the first one everybody uses is nearby pixels correlate the most with nearby pixels.
Lec7-435|So these two pixels are very similar.
Lec7-436|But they're very different from these pixels.
Lec7-437|Right? The world is irregular place it's relatively smooth.
Lec7-438|So there's local consistency.
Lec7-439|But on the other hand, this is a slight lie.
Lec7-441|So I have a better way of saying this, but just to try and get this idea across.
Lec7-442|Imagine I took all of your vacation pictures.
Lec7-443|Are all of the pictures on your cell phone.
Lec7-444|And I looked at pixel right here in the picture.
Lec7-445|So you can i, over all your pictures, I compute the mean and the standard deviation and maybe the covariance with pixels around it.
Lec7-446|I do that over here for another pixel.
Lec7-447|The story is that the mean, the standard deviation, the covariance of those two will be relatively similar.
Lec7-448|Now that's not completely true.
Lec7-449|You look up at the sky, you get a different set of correlations then when you look down at the grass, right? But all assumptions are wrong usually, but some are longer than others.
Lec7-451|So then when you assume you make an ass of you and me, that's what my daughter keeps telling me.
Lec7-454|The third thing is that doesn't depend on where it is in the picture.
Lec7-456|So I can translate tests in the image, and she's still tests.
Lec7-458|So we all have where most of us have two hands and then we have an elbow and upper arm and body and we're made of parts.
Lec7-460|They were actually in Chapter eight of the Old Testament, which I don't know if I've posted yet under readings.
Lec7-461|Chapter eight of the parallel distributed processing books that came out in 1986 and changed cognitive science forever.
Lec7-462|So this is his zip code recognizer for some reason.
Lec7-464|I don't know if it's still used there, but they have letters and numbers and their codes.
Lec7-466|So if this is like five-by-five, it has 25 plus the bias parameters, 25 numbers.
Lec7-467|So that's a local receptive field and it's like what happens in your retina.
Lec7-469|All your, your, the light sensitive cells in your retina only respond to very small part of your visual field.
Lec7-470|But they're replicated across the image.
Lec7-472|So this guy, and this is a patch of the responses of neurons that all have the same five-by-five set of weights.
Lec7-473|So you're computing the same feature everywhere in the network.
Lec7-474|So you're essentially, you can think of that five-by-five patch awaits as a kernel and you're convolving the image with it, you're running that all over the image and computing it's the response.
Lec7-476|The next guy is another feature, the next guy is another feature.
Lec7-477|So he's got six different features.
Lec7-479|We've got 28 by 28 times six.
Lec7-480|So that's some large number, like thousands.
Lec7-481|But there's only six times 25 different weights.
Lec7-483|So that's, that's, that's a convolutional layer.
Lec7-484|If you're in double E.
Lec7-486|That's a convolution, but we're doing what we're calling a convolution is really correlation.
Lec7-488|This is a feature map, thank you.
Lec7-490|And we've got six different feature maps.
Lec7-492|Then the next layer up is a pooling layer.
Lec7-493|So it might take a two-by-two patch here and it could average it, or it could take the maximum.
Lec7-494|Maximum has been used a lot in deep learning because it seems to work well.
Lec7-495|But averaging is better for avoiding artifacts.
Lec7-497|But now I've reduced by taking a two-by-two patch.
Lec7-498|I've taken this 28 by 28 thing and turned it into a 14 by 14 thing.
Lec7-502|But if it was color, we'd have R, G, and B planes.
Lec7-503|And this five-by-five patch would actually be five by five by three.
Lec7-504|That means there's three channels for m-nest, there's one channel for color images.
Lec7-505|There are three channels.
Lec7-506|Now we've got six channels for six different feature maps.
Lec7-507|And this is a little, it's a little misleading because this doesn't just take a patch from this feature map.
Lec7-508|It goes all the way through.
Lec7-509|So it's a volume, rectangular volume.
Lec7-510|Parallel pipette, whatever you call that.
Lec7-511|It's if it's got like a five-by-five patch, it's five by five by six.
Lec7-512|So it's got a volume of weights.
Lec7-513|And it's applying that all over this set of thing because now we've got six channels.
Lec7-514|Okay? And okay, and so I get, and he increases the number of features now to 16 and that turns out to be a good thing to do.
Lec7-515|And now I've got 16 feature maps.
Lec7-516|And then I reduce that by sub-sampling by taking a two-by-two patch and either averaging are taking the max.
Lec7-517|There's another way of doing it called network and network where you actually have a little neural network between here and here.
Lec7-518|A little three-layer network that learns its weights adaptively.
Lec7-522|Then typically have a fully connected layer, in this case two and then ten outputs for the ten digits.
Lec7-523|So it's a little surprising that we have a letter and then we're talking about digits.
Lec7-524|So this spatial pooling is what gives you some translation invariance.
Lec7-525|Why is that? So here's the spatial pooling.
Lec7-526|I'm taking a two-by-two patch here.
Lec7-527|And I'm reducing that to one number by taking the max.
Lec7-528|So if, if there's a feature here that's firing the most out of this two-by-two patch.
Lec7-529|Then it shifts a little bit.
Lec7-530|The feature will be here.
Lec7-531|Alright? And so if I take the max, I'll get the same response.
Lec7-532|So translation and variance comes about as a combination of using the same features everywhere and pooling them.
Lec7-534|As I go deeper into the network, the size of the receptive field.
Lec7-536|But if this guy is listening to five-by-five of these, and it's going to have a bigger part of the input that drives it.
Lec7-537|That's called the receptive field.
Lec7-539|So the deeper the network, the more translation invariance this builds up over the network.
Lec7-540|And that's true in our brains as well.
Lec7-541|There'll be neurons that deepen your head.
Lec7-543|Because faces big, your receptive fields get bigger as you go deeper in the net.
Lec7-544|And so this might, this is representing a part of the input.
Lec7-545|And as we go deeper, we're combining those features to get the whole thing.
Lec7-547|So in say a face recognition network, we might have features here that respond to our eyes, some that respond to the mouth, et cetera.
Lec7-548|And as we go deeper, we're going to combine those to recognize the whole face.
Lec7-550|It has a dual meaning in neuroscience.
Lec7-553|How much of what out there drives the neuron.
Lec7-554|And the second part is the, the features in that part of the visual world that stimulate that neuron.
Lec7-555|So we're going to use the first meaning as receptive field.
Lec7-556|And the second one we'll call the features or the kernel or the filter.
Lec7-560|You're seeing the site? Yeah.
Lec7-562|So you change the input in a larger thing, you're going to change the response to that neuron.
Lec7-563|So this is another view from the Stanford site.
Lec7-565|So again, you have this little block of features of numbers.
Lec7-566|And again they're just numbers.
Lec7-568|Not any different than what we've been doing.
Lec7-571|And then we could have, say in this case five different features.
Lec7-572|Five different kernels will get five different responses.
Lec7-573|And this will be all over the image depending on how many of these there are, et cetera.
Lec7-574|So the blue is a convolutional block.
Lec7-575|It has five neurons and each one computes a different feature.
Lec7-576|Then these five are replicated across the image.
Lec7-580|I haven't clicked on this link lately.
Lec7-582|Last quarter or two quarters ago.
Lec7-588|So here's the input.
Lec7-590|Each one of these is computing a different feature and you can see they're different.
Lec7-591|This one seems to respond to white patches.
Lec7-592|This one seems to respond to black on top or white on top, black on the bottom.
Lec7-593|This one seems to respond mostly to solid black things, but not edges where it's black and white, etc.
Lec7-594|So there's six different features.
Lec7-595|How are these little patches set? How do we decide what our kernels ought to be? Class? I said we've got, say, these five-by-five patches.
Lec7-598|if, you know, if we're doing max pooling here where we have four numbers and we just picks them, pick the maximum.
Lec7-599|We can backpropagate through that, just pretending there's a one to the maximum guy.
Lec7-600|So we have to keep track of when we're doing backprop.
Lec7-601|We have to keep track of where the maximum was.
Lec7-603|And so if we're doing average pooling, that's like having equal weights on all 4.25, 0.25, 0.25.
Lec7-604|We keep them the same and we backpropagate through it.
Lec7-606|Okay, and then this layer here is just the pooling of this layer.
Lec7-607|So they should look very much the same.
Lec7-608|Then these are the 16 features and they start to look harder to tell what they are.
Lec7-609|This is the reduction of them and this is the fully connected layer.
Lec7-610|So you can't see anything there because it's beyond our groups.
Lec7-623|But let's look at this one instead.
Lec7-624|Still got 10 min.
Lec7-626|This the Stanford site.
Lec7-629|This is an example of some features learned it the first layer of a convolutional neural network.
Lec7-630|And they resemble the features in, right.
Lec7-632|So these would be, each one of these would be computed across the whole image.
Lec7-635|That's a joke for people that watch old movies.
Lec7-641|They do red, green, and blue, yellow.
Lec7-642|But that's not what I wanted to show you.
Lec7-646|Okay, so this is just showing there we go.
Lec7-647|The computation of a convolution.
Lec7-648|Okay? So these are the three channels, red, green, and blue.
Lec7-650|And so there's a three-by-three patch for this one.
Lec7-651|There's a three-by-three patch for this one and a three-by-three patch for that one.
Lec7-652|So that's one filter.
Lec7-654|So you'd go zero times minus one is 0002 times 00,222.2 times one is two, so that's four.
Lec7-656|Then six minus one is five.
Lec7-657|And then two times one is two, that's seven.
Lec7-659|The bias is 007 plus 07.
Lec7-660|I got it right.
Lec7-664|Notice here that we're taking this three-by-three thing and stuffing two across two steps each time, that's called the stride.
Lec7-665|And depending on the size of your filter, you might need padding around the outside.
Lec7-667|And this is just showing how each one of these is computed.
Lec7-670|People don't generally, but go ahead.
Lec7-671|It'd be great to see that.
Lec7-672|You can have, I mean, what people tend to.
Lec7-674|So you'd get features at different scales that can start to get a little bit expensive.
Lec7-676|But what people sometimes do is they use what are called dilated filters.
Lec7-677|So if I have a three-by-three filter.
Lec7-678|I can have a five-by-five filter by taking those nine numbers and putting one up here in the corner as zero and then the number of zeros and then the number.
Lec7-679|So I've got those first row three numbers, but I've just and big and it had zeros in-between.
Lec7-680|That's called a dilated filter.
Lec7-681|And you can get features at different scales that way.
Lec7-683|And now it's 316, got 4 min.
Lec7-686|So when you record location sorry.
Lec7-687|If you have a two locations randomly pick one.
Lec7-690|Don't look at me like that.
Lec7-695|I was just here.
Lec7-697|There's a clicker question in the last 3 min.
Lec7-698|Okay? When we convolve a filter with the window of the input image, we take the a cross-product to learn filter with the window, be the dot-product learn filter with the window.
Lec7-700|Which of these do we do? Okay, Looking good, looking very good.
Lec7-706|Which one is it? A, B, C, or D? B.
Lec7-711|So you're all pretty wiggly.
Lec7-712|I'm going to let you go because you're not paying any attention to anymore.
Lec9-2|You can hear me.
Lec9-6|I just wanted to clear up something.
Lec9-8|Seem a little bit confused about this gradient approximation thing.
Lec9-9|I just want to make clear what we're talking about here.
Lec9-11|So remember the schema for gradient descent is this.
Lec9-12|So this is the gradient.
Lec9-14|So the gradient is equal to the negative of the Delta times the input on that line.
Lec9-15|Right? When you're comparing that, that's what you should be computing when you're trying to compare the gradient of your algorithm with the gradient of the numerical gradient.
Lec9-17|And so that means that this guy is the negative of that.
Lec9-22|And we get a value for the error at that setting of W and a value for the error at this setting of W.
Lec9-23|And so that change is the rise over the run which is two epsilon.
Lec9-24|So that's, that's where we're getting.
Lec9-25|That's what you should be comparing.
Lec9-31|Okay, So the numeric approximation is just computing the rise over the run as an approximation.
Lec9-32|That is Lynn, that J is linear.
Lec9-33|A linear approximation of the gradient, right? Yeah, we good.
Lec9-35|So you compute this for one, you can just compute this for one pattern, even with the initial random weights.
Lec9-36|And then you backpropagate from that pattern and look at what it says for changing that way.
Lec9-37|And those two should be roughly equal up to four digits if epsilon is 0.01.
Lec9-39|So this should look familiar.
Lec9-40|This is we did.
Lec9-42|That's about where we ended up.
Lec9-43|Are there any questions left over from last time? Okay.
Lec9-45|And then we did batch norm, a way that isn't where I thought we were ending up.
Lec9-49|And there was a question during my office hours yesterday about this one that clear to everybody.
Lec9-59|The midterm, I believe is Saturday.
Lec9-60|And I'll be giving a review session on Friday night and I will record that so you can watch it the next morning if you miss it.
Lec9-63|So come to class.
Lec9-69|So you can start, this is the full AlexNet.
Lec9-70|It's got 12345678 layers have kept the softmax, which you should.
Lec9-73|Okay, and then retrain.
Lec9-75|We only have one fully-connected layer to the output that drops 16 million parameters.
Lec9-76|Why? Why does that one layer dropped 16 million parameters? Sorry.
Lec9-78|I'm not sure I caught that.
Lec9-81|So I don't know if that's what you were saying, but it's 16 million parameters.
Lec9-83|It only dropped the performance 1% or so.
Lec9-85|And now you're dropping 50 million parameters and you get a 6% drop in performance.
Lec9-86|So that's also not as important.
Lec9-87|Now, here we were dropping layers 3.4 and that only drops 1 million parameters.
Lec9-88|So that tells you something about why convolutions are good things.
Lec9-89|We had two layers that we removed and we only dropped 1 million parameters.
Lec9-90|And that's because of the wonderfulness of convolutional networks.
Lec9-91|They share parameters across the whole input, right? So you've got one.
Lec9-92|Each feature is just one set of parameters and then over, overlaid over the next convolved with the previous layer.
Lec9-94|Okay? So that's part of the beauty of that.
Lec9-95|And that was only a 3% drop.
Lec9-96|But when they moved, remove two of them, we add 3,456.7.
Lec9-97|Now we've just got four layers and there's a 33% drop.
Lec9-98|So depth is important.
Lec9-106|That's all I can imagine he did.
Lec9-107|I haven't read this paper.
Lec9-108|This is his slide, but I assume he retrain the network from scratch.
Lec9-113|And I talked last time about transfer learning and what you do.
Lec9-114|Basically, if you have a new problem, you could take this network that's been pre-trained on ImageNet and just bolt-on another softmax layer for your new problem.
Lec9-115|And the simplest thing to do is just train that single layer of weight.
Lec9-116|And that generally gets you a long way towards doing very well on that dataset.
Lec9-117|And the reason why you do this again is if you have a small dataset because you can't train a deep network without enough examples.
Lec9-120|Except these are mostly used in SVM.
Lec9-123|And then you, those are your features.
Lec9-126|It will separate the data and stop, right? And maybe it just barely got cut off.
Lec9-127|One of the dataset, one of the categories.
Lec9-128|So any that could give rise to a fair amount of error because you're very close to that category.
Lec9-129|And if anything from that category goes over the line, you've got a mistake.
Lec9-130|So what a support vector machine does is it finds the separating hyperplane that as far as possible from the dataset.
Lec9-131|So imagine I've got two sets of data here.
Lec9-133|And it's the distance to the nearest vectors.
Lec9-134|Those are called the support vectors.
Lec9-135|They support this essentially, right? With things that are at right angles to it.
Lec9-137|So this is connecting a support vector machine to layer one and trying to use that to count it.
Lec9-138|So if you don't know what a support vector machine and didn't get it from what I just said, which is entirely plausible.
Lec9-139|Just imagine it's a softmax layer.
Lec9-140|And this is just bolting on a softmax to layer one and trying to categorize things.
Lec9-144|And it's a lot simpler.
Lec9-145|So you can see that the deeper you go in the network, the better the features appear to be, at least for this dataset.
Lec9-146|Remember what I said about Sandy's project? Recognizing digits, it actually got better, going shallower.
Lec9-148|Sorry, it actually got better the shallower we went, but it still wasn't very good.
Lec9-149|And we had to essentially train a network from scratch.
Lec9-150|That was five hidden layers to recognize these letters that are these words.
Lec9-151|And that's because when you try to recognize the category, like we call these things chairs.
Lec9-152|Right? But we don't think about it at all.
Lec9-153|If I was a office furniture salesman, I'd probably have a different name for all three of these four of these chairs.
Lec9-156|That means we're taking this set of similar looking things and mapping them into small region of space in the network so that we can cut it off with a simple hyperplane know.
Lec9-158|And so they're taking written letters and squishing them down.
Lec9-160|So you're essentially becoming a word expert as it were.
Lec9-161|Okay? That all make sense.
Lec9-163|And these their structure is, is reliant on those principles.
Lec9-164|So locality, I can't, I can't even read my own slides when I'm looking at them on my laptop.
Lec9-165|So pixels depend on nearby pixels.
Lec9-166|So if you look at an image and you grab a patch of pixels, they will be highly correlated with one another.
Lec9-167|And we use small receptive fields.
Lec9-168|And then we use the statistics of the pixels in an image don't vary much as you look at the statistics over a whole set of images at different locations in the image.
Lec9-171|Objects don't change identity based on location.
Lec9-172|So we want translation invariance and the combination of using the same features across the image along with spatial pooling gives you some translation invariance.
Lec9-173|And objects are made of parts, so receptive fields get larger the deeper you go, encompassing more of the image.
Lec9-174|And usually you want something small like three-by-three patches of features.
Lec9-179|Depth allows features of features of features to be learned, which allows us to untangle.
Lec9-180|If you look at images of, picture of say, Donald Trump and Joe Biden, they share much more than they don't share.
Lec9-181|The pixels in the middle will be bright skin colored.
Lec9-182|And so there's a lot of correlation between those.
Lec9-183|And somehow the network has to separate those so that they are linearly separable before you get to the output.
Lec9-184|They deal with all sorts of problems that previous computer vision systems couldn't, like.
Lec9-185|Lighting changes and scale and rotation, all those sorts of things.
Lec9-188|And Joe Biden and sending them over here by the final layer.
Lec9-190|In fact, there have been recent results showing that basically these networks find the global minimum, which is a big surprise.
Lec9-191|And they're better as long as you have some way of passing the gradient back.
Lec9-192|And I haven't talked about this yet.
Lec9-193|The next lecture but resonates, overcome this problem.
Lec9-194|We'll see that in a few minutes.
Lec9-195|And it's often best to just reuse a pre-trained network to compute the features of the image.
Lec9-197|Okay? So I want the best answer to this, right? So think about it a bit.
Lec9-199|I'll give you a few more seconds to answer, and it's not E.
Lec9-203|58 of you voted, 61.
Lec9-216|And having a lot of parameters with a one hidden layer network that's very, very wide, won't solve a bunch of problems.
Lec9-217|Okay? That's it for that one.
Lec9-219|But the architecture is what really matters.
Lec9-220|Okay? Okay, convolution, that's part two.
Lec9-223|The structure of some of these deep models like resonates and Google, and how to visualize the features.
Lec9-224|In your first assignment.
Lec9-225|You could visualize the features by just looking at the weights but deepen a network it's responding to features of features of features.
Lec9-226|So how do you visualize what is responding to them? And then talk about some adversarial examples and training tips.
Lec9-228|So here's 5123456, different, 12345, different things.
Lec9-230|We've got some crocodiles, we've got an African gray parrot and entertainment center.
Lec9-235|Now, take that very long vector and compare their distance to one another.
Lec9-236|What this is showing is if there's no shift, obviously, it's the same.
Lec9-237|So the distance is zero.
Lec9-238|But very quickly, as you shifted up or shifted down, the distance between the initial representation in the new one is very different.
Lec9-239|The distance is high.
Lec9-240|Now somewhere in much later in the network though, there's a much smaller shifts.
Lec9-241|So look, this is normalized to some distance.
Lec9-242|I'm not sure what.
Lec9-243|But the y-axis and the lower left, that's the distance between the first layer as we shift things up and down.
Lec9-244|The y-axis is ten.
Lec9-246|I don't know exactly how they did it.
Lec9-247|This is like layer seven.
Lec9-249|So by that layer we've got a lot more.
Lec9-251|It doesn't change a lot.
Lec9-254|That would be identical in both cases, the weights don't change.
Lec9-255|We're just setting, sending these images through the network and recording the activations.
Lec9-256|So the feature maps.
Lec9-257|Take the feature maps and treat them as string them out in one long vector and now show it translated image, take the feature map, stretch it out into one long vector, and compute the distance between those two.
Lec9-259|That's the lower right panel.
Lec9-260|The upper-right panel is the, is the softmax layer.
Lec9-262|It's biased against Africa.
Lec9-264|It's still getting the classification correct because we're talking about 1,000 different categories.
Lec9-265|So even if the activation for that, so that's the activation of the lawn mower of the ship to the crocodile, the gray and the entertainment center output in the green line being the African gray.
Lec9-272|So probably the most.
Lec9-280|And so they changed a lot as you change the image.
Lec9-281|But deepen the network somewhere like corresponding to our Jennifer Aniston neuron.
Lec9-282|They're not changing hardly at all.
Lec9-284|Okay? This is mostly due to the architecture of the network.
Lec9-285|It's got this prior built-in to be translation invariant.
Lec9-288|This is we go from left to right.
Lec9-289|We're just scaling it up.
Lec9-290|And the first layer of the network, there's a big shift in the distance.
Lec9-291|It goes up to now 11.
Lec9-292|Like This is Spinal Tap.
Lec9-296|And we keep hold of that.
Lec9-297|And then as we shifted up and down, we're measuring the distance between the activations for the original one and the new one.
Lec9-299|I highly recommend This is Spinal Tap.
Lec9-301|And they're very stupid.
Lec9-302|And one of them says, everybody else's AMP goes up to ten, but mine goes up to 11.
Lec9-304|So just goes up to 11 in the next slide.
Lec9-306|But the rest are the distance and the activations between the image on the left and the other images as they get bigger and bigger.
Lec9-307|And so it gets farther and farther away.
Lec9-308|Okay? And then here is the same thing from deepen the network, where we're looking at the distance to the original image versus the scaled up image in the feature space.
Lec9-309|Right? So that only goes up to 0.7 again.
Lec9-310|And so they're all represented very well by that time, but that's not built into the network.
Lec9-312|This is learned from the data.
Lec9-315|I'm sorry, Casey, I didn't bring anything for you today.
Lec9-316|Then look up at the top.
Lec9-317|That's the final softmax layer.
Lec9-318|And it's got those three categories that are already knows pretty well.
Lec9-319|And the two that it doesn't know well, it actually does a slightly better when they're made bigger, at least at first.
Lec9-322|And so we're taking the original image and rotating it through 360 degrees.
Lec9-324|So at that layer, it's, it's, it's still much smaller than the first layer.
Lec9-325|It's 1.2 as opposed to 0.7 on the earlier ones.
Lec9-326|But that's still enough to really mess it up at the softmax layer.
Lec9-327|But one of them seems to be kind of beating, right? Like the entertainment center.
Lec9-328|It seems to like get recognized at a few other places.
Lec9-331|Ask me you ask your question.
Lec9-332|A lot less than what? Yeah.
Lec9-333|But then look at the output.
Lec9-336|I mean, it's still doing kind of okay.
Lec9-341|But when the entertainment center it keeps.
Lec9-345|So it's when the screen comes back into register, is upright and square.
Lec9-348|There are scale invariant because of the training.
Lec9-349|They've been trained to recognize big and little dogs, etc.
Lec9-350|Because things are various distances and pictures.
Lec9-351|And it's not rotation invariant.
Lec9-354|So the basic idea here is we're going to have convolutional layers.
Lec9-355|We're going to have two streams.
Lec9-357|That's pretty standard for a lot of networks.
Lec9-358|Convolution, Convolution, convolution, fully connected output.
Lec9-359|Okay? But he added a lot of things that made this work.
Lec9-360|It included a lot of innovations like dropout.
Lec9-362|So data augmentation, one of the common ways to augment your data.
Lec9-364|But the second best way is to make more data out of the data you already have.
Lec9-365|And so all they do is take different crops.
Lec9-366|So it changes the context around the thing you're trying to recognize.
Lec9-368|Obviously, you could rotate it all 360 and train it to recognize things upside down without changing the architecture at all.
Lec9-369|The other things they do is color randomization.
Lec9-371|These things tend to be focused on texture.
Lec9-372|So if you take a cat and you fill in everything It's for with elephant skin.
Lec9-373|It thinks it's an elephant.
Lec9-374|So by varying the color, you make it so it can't depend too much on, on that color.
Lec9-375|You can also put different textures on things to make it less sensitive to texture and be more sensitive to shape.
Lec9-376|And the obvious one is mirror flip.
Lec9-377|And so those are various ways to augment the data.
Lec9-378|You can add some Gaussian noise.
Lec9-379|So it's not, doesn't focus in on any particular pixel that would have defeated.
Lec9-380|The thing that the paper I reviewed for Europe's said where they had one pixel that told you what digit it was and it focus completely on that one pixel rather than the actual shape of the characters.
Lec9-384|Some of the most primitive plants just repeat their stems over and over again to get taller, get closer to the sun.
Lec9-385|And here he's just basically taking a single bit of architecture.
Lec9-387|I didn't change the batteries on this.
Lec9-390|So as you go from top to bottom or from left to right, I should say.
Lec9-391|They're basically just adding more of the same.
Lec9-392|So up the top it's adding some more convolutional layers, adding more convolutional layers, especially in the middle and the bottom, until it just gets deeper and deeper.
Lec9-393|Now, it's harder to optimize because you have to propagate the error pretty far back in the network.
Lec9-395|But if you look at the error in this table here, it goes from 10.4 down to eight for top 5-year in 29.6, ten to 25.5.
Lec9-396|So adding more layers appears to be better.
Lec9-397|You would think that's very contrary to the prevailing VC dimension theory at that point, which said, you don't want to have too many parameters.
Lec9-398|It seems like having over parameterizing these things seems to work really well, which is pretty surprising and is one of the things people are trying to understand right now.
Lec9-401|This is the one I've mentioned a couple of times, where instead of max pooling, they put a couple of layers.
Lec9-405|Global average pooling is another idea where at the final layer you have these planes right? Of, of features.
Lec9-406|But you get them to specialize in one particular category by having like this purple guy only connected to the elephant output, e.g.
Lec9-407|and this other guy only connected to the cat output.
Lec9-408|And then you take all of these features and just average them.
Lec9-409|That's gap for global average pooling.
Lec9-410|And then those debts, what goes into those softmax.
Lec9-411|And because of backprop, because of the way this is connected, these learned to be the pattern for that category.
Lec9-415|You can see where the peak activation is.
Lec9-416|And go back to the original image and see what it's looking at.
Lec9-417|To make this decision.
Lec9-418|And so you can see things like this.
Lec9-421|And you can map because it's all spatially organized.
Lec9-422|You can go back to the original input just by working out where that activation is coming from.
Lec9-423|And you get these kind of salience maps, like where it's paying attention to.
Lec9-424|So in the left image, trying to recognize brushing teeth.
Lec9-425|And so there's a lot of big hotspot over her mouth where she's holding a toothbrush.
Lec9-426|And over here there's two people brushing their teeth.
Lec9-427|So there are two spots.
Lec9-429|So it's focusing on the tree and the guy is wearing a hat.
Lec9-430|And similarly here, what you find with these things is e.g.
Lec9-432|It turns out that it recognizes, doesn't recognize barbells by themselves very well.
Lec9-433|It recognizes barbells with hands on them because hands are always there with barbells in pictures anyway.
Lec9-439|And maybe they're 14 by 14.
Lec9-441|So there's 36 features at this level times 1,000 is 36,000, but you've connected each one to only one.
Lec9-442|Output, becomes the activation map for elephant, or the activation map for brushing teeth.
Lec9-444|But I do have my hearing aids in.
Lec9-451|Oh, but this is just the weighted I mean, it's weighted by averaging, right? So this is just a weighted sum to the elephant output.
Lec9-453|And if they're bigger, urine, a softmax, elephant gets suppressed.
Lec9-459|So if the elephant is here, it's going to have peak activation here.
Lec9-460|If the elephants over here, it's going to have peak activation here.
Lec9-461|And you can figure out where that is in the image.
Lec9-464|And you're playing that over the original image, stretching it out.
Lec9-465|So it's 224 by 224 again.
Lec9-475|I give the network an elephant.
Lec9-476|I get an output.
Lec9-478|I get some activation for elephant.
Lec9-479|I just take all of those activations and average them together.
Lec9-481|So it's a kind of average pooling.
Lec9-483|And because of the way it's connected, again, each one of these is going to try and be an elephant detector.
Lec9-484|Because it's trying to, it's learned that when it's an elephant, you backpropagate to here and then from here back into the rest of the network.
Lec9-489|That, that doesn't mean that make sense.
Lec9-490|So it's not, it's not like over multiple.
Lec9-492|The most error if it's not saying elephant, right? And so the elephant output unit is propagating error back to all of these equally.
Lec9-496|Is that better? Less shaking of the hand? Yeah.
Lec9-500|Yeah, I guess network and network actually came up with this idea because as Nin at the top, and I don't think they mean Nine Inch Nails.
Lec9-506|The high ones there'll be red, the low ones will be blue.
Lec9-507|And then smooth it out and stretch it out over the image.
Lec9-508|Okay? And that's, that's what this is.
Lec9-510|I'm just, I'm just taking the activations here and turning them into a heat map.
Lec9-511|And they're not, I mean, they can be distributed.
Lec9-512|Unlike us who can pay attention to two things at once or three things at once? Yeah.
Lec9-513|When they stretch it out and do I mean yeah, this is six-by-six, the original image at 02:24 by 224.
Lec9-514|So I have to multiply it by 36 or something like that.
Lec9-516|And I have to have to upscale it.
Lec9-517|Yesterday in my research group.
Lec9-518|He ran up and barked at the speaker who was very annoying.
Lec9-550|You have a separate feature map for every category.
Lec9-551|No, it's done through the magic of backpropagation.
Lec9-552|They learned to be the elephant guy.
Lec9-553|Like the elephant in the room.
Lec9-555|And each one is basically doing a one-by-one convolution into the previous layers.
Lec9-556|One-by-one convolution is just a vector of weights into all the channels that are coming in.
Lec9-558|And so they're, the architecture is set up.
Lec9-559|So these purple guys are only connected to that purple guy.
Lec9-560|They're not connected to any of the others.
Lec9-561|This middle one is only connected to the middle one.
Lec9-562|The last one is only connected to the last one.
Lec9-563|So through the magic of backprop, they learn they learn to be detectors for that category.
Lec9-570|So there's one feature map per category.
Lec9-571|You can turn it into a heatmap and lay it over the image and see where it's getting its input from.
Lec9-573|So I can plug this into here.
Lec9-574|And plug this into here.
Lec9-578|You could have one-by-one convolutions.
Lec9-580|You've got a set of weights to basically one pixel in the previous layer.
Lec9-581|Multiple channels and you use that same way all over.
Lec9-582|It's really just looking one thing in the feature box.
Lec9-584|here, if I have one-by-one convolutions with 32 filters, I've suddenly gone from 64 by 56 by 56 to 32 by 56 by 56, I've halved the number of numbers by using one-by-one convolutions.
Lec9-586|How many different 64 dimensional vectors are there? No, I mean, unique one-dimensional vectors.
Lec9-587|How many unique convolutions essentially, right? Dum, dum, dum, dum, dum.
Lec9-596|So you've got a 64 dimensional vector going into that feature block down there.
Lec9-597|And I get one feature plane from that.
Lec9-598|I do that again, I get a second.
Lec9-599|I do that again, I get it.
Lec9-610|These are always the same for one feature map.
Lec9-611|That gives me one of these 56 by 56 planes.
Lec9-615|Stop, drop and cover.
Lec9-621|You can have 12, you're going to have ten.
Lec9-625|And hopefully if you've seen the movie Inception, you, you're, you're good, but if you haven't, you should.
Lec9-626|So this is the previous feature map here.
Lec9-627|The previous layer as a pale green.
Lec9-628|And now they're getting features at multiple scales.
Lec9-629|They've got one-by-one features.
Lec9-633|So everywhere there's a one-by-one, you have control over how many numbers you're using for the next layer up.
Lec9-634|And you're getting features at very small scales, like one pixel of the previous feature map.
Lec9-635|Or you're getting bigger scale three-by-three, or bigger five-by-five.
Lec9-640|So you need multiple scales.
Lec9-641|And so they're doing that and then convolving are concatenating all of those into the next layer up.
Lec9-642|They just so you can see the number of filters and the y-axis over on the left, you're going to have more one-by-one, fewer three by threes, and even fewer five-by-five.
Lec9-645|So everywhere there's a you can see it clearly repeats over and over again.
Lec9-647|And what's good about this? So this is 22 layers deep.
Lec9-648|And you still got this issue of backpropagating the gradient.
Lec9-650|So what they do is they put on multiple heads at different parts of the network.
Lec9-652|So there's one there, one there, one there.
Lec9-653|So you're trying to recognize the same object over and over again.
Lec9-654|But what this does is allow a shorter path for the gradient.
Lec9-660|And then they also increase the dimensionality as they got closer to the output.
Lec9-661|I guess the intuition here is I've got 1,000 categories.
Lec9-663|And so this did really well.
Lec9-666|And yet there's only twice as much computation involved.
Lec9-668|Then along comes resonance.
Lec9-669|Where are we? 3137.
Lec9-670|Okay? So the motivation here is up here.
Lec9-672|On the left is the training error, on the right is the test ear.
Lec9-673|And notice that the 20 layer network, which is the yellow line, has lower test error and lower training error.
Lec9-674|So it's very hard to optimize these deep networks because the gradient is probably getting, you're getting a vanishing gradient by the time you get by the input.
Lec9-676|I should say that this is upside down, the inputs on the top, the outputs on the bottom.
Lec9-677|I always have it the other way round.
Lec9-678|But you've got a one-to-one combination connection from the input x to the output x.
Lec9-680|You just copy that up there or down there in this picture.
Lec9-681|And then you have two layers of weights.
Lec9-684|But the idea is, what is the weight on this thing? It's one.
Lec9-687|And these things, now you can have like, you know, we had one of my colleagues had a 10,000 layer network.
Lec9-697|The output should be, okay.
Lec9-702|You take multiple networks, like five networks, and you have them vote for the right answer.
Lec9-703|And you hope that because they started out with different initial random weights, the mistakes they make are going to be uncorrelated.
Lec9-704|So that on one particular example, three might get it right and to get it wrong, but the three went out, right? So simple, actually simple voting works pretty well.
Lec9-705|Not even numerically combining them.
Lec9-708|You can also call these skip connections.
Lec9-709|They skip this part.
Lec9-713|So we're halfway, about halfway through the course.
Lec9-716|Remember that backprop is a linear operation.
Lec9-720|And the linear dynamical system has only two places.
Lec9-721|It can go zero and infinity.
Lec9-722|So you can worry about the infinity by clipping the gradient, you can shorten it to some fixed length, but you can't, you can't deal with the shrinking gradient problem.
Lec9-723|And so what happens is you have smaller and smaller error correction.
Lec9-724|Earlier in the network.
Lec9-726|Now you're able to choose like how many of those? Well, it's an easy it's an easy way to do it.
Lec9-729|And then for each of the I mean, you mentioned e.g.
Lec9-730|if I have three You have three one-by-one convolutions.
Lec9-739|And you subtract that average from all the red values and divide by the standard deviation, normalizing your novel way.
Lec9-741|Yeah, I can imagine that would be a way to do it.
Lec9-747|Say we did this.
Lec9-748|This is the equations for what we did.
Lec9-752|That's good to know.
Lec9-757|I'll try and find a room for it.
Lec9-762|You just have to wander around campus and try and find me.
Lec9-764|At the very end.
Lec9-769|You are an hour or so.
Lec8-0|Okay, let's get started.
Lec8-3|Just to reminder to read Stanford cabinets course webpage.
Lec8-7|And how we're going to take into account some priors based image.
Lec8-8|These are going to be architectural priors.
Lec8-9|Based on what we know about the visual system.
Lec8-10|We're going to use small local receptive fields like Yanis using here.
Lec8-11|And those are replicated across the image.
Lec8-13|The other way I like to say this is they are.
Lec8-14|If a feature is useful in one place, it's probably useful in another.
Lec8-15|So we take the same feature and create a feature map, which is the activations due to one feature being convolved with the image.
Lec8-19|So what we have without spatial pooling is what Geoff Hinton calls equivariance.
Lec8-21|And then translation and variance comes from pooling those signals so that if I move it just a little bit, I'll get, I'll see equivariance at C1 here.
Lec8-22|I'll see probably invariants at S2.
Lec8-23|And then objects are made apart, so receptive fields get larger the deeper you go into the network.
Lec8-25|Yeah, So well, we'll talk about that in the next lecture.
Lec8-27|You can imagine an architectural prior that gives you rotation and variance by taking your features and rotating them and then pulling that.
Lec8-28|I'm not sure exactly.
Lec8-29|To get size and variance, you'd need to expand the features and contract the features and pull across those.
Lec8-30|In the human and primate visual system.
Lec8-33|So scale is just to shift left or right, and rotation is just to shift up or down, rotation in the image plane.
Lec8-34|But translation invariance is lost.
Lec8-35|There's quite a bit of difference between the architectural prior here and the human architectural prior.
Lec8-36|So I'm semi retired.
Lec8-37|I retired in 21 and I'm recalled disservice.
Lec8-38|If you think I'm performing a service for you guys, teaching you something.
Lec8-39|And so I get my pension plus 43% of my salary.
Lec8-40|So I'm getting 130% of my salary.
Lec8-43|So again, this is an example of what this would look like with five different features.
Lec8-45|And then the next layer up where it might pull some of these.
Lec8-47|So you only pull within a plane.
Lec8-48|And then the next layer up would take a big slice out of this image like this one.
Lec8-50|So the sift feature volume now.
Lec8-52|And I said all that last time.
Lec8-56|If I could show you this, you would see is that a network like this is relatively invariant to some size changes, some rotations, and even funny-looking drawn numbers.
Lec8-63|These are two different features.
Lec8-64|Notice they have, they have features for each channel.
Lec8-65|And then they sum all those up and add in the bias and you get the numbers over here.
Lec8-66|This is the, now this is the feature map.
Lec8-68|Then I would apply a non-linear activation function to this.
Lec8-69|So this is our a's are weighted sum of the inputs to the next layer up.
Lec8-71|This is clear to everyone what's going on here.
Lec8-73|The only dumb question is the one that wasn't asked.
Lec8-77|So this guy here corresponds to one of the dots randomly.
Lec8-80|So that's 11 filter or one kernel or one set of weights for one feature map.
Lec8-81|That's the set of weights for the second feature map.
Lec8-82|This is feature map one.
Lec8-84|They must program in C.
Lec8-87|So convolutional networks are now perfectly obvious, right? Okay.
Lec8-88|And then I had this clicker question last time and most of you got it right.
Lec8-93|I mean, this was what, this was 1989.
Lec8-94|What happened 1989-2012 when continents got deep-learning got really popular.
Lec8-95|We'll talk a little bit about that.
Lec8-96|Sometimes people named two things, I named three things.
Lec8-99|The second thing is we got ReLu units, rectified linear units, which allowed the gradient to pass back fairly well.
Lec8-101|And then the third thing was we got a lot of data on the Internet.
Lec8-102|We got pictures everywhere because everyone's taking pictures with their cell phones and uploading them to Facebook or whatever.
Lec8-105|The nice thing about Facebook, by the way, is that they publish all their models.
Lec8-106|They make them available to everyone.
Lec8-108|just published his papers and never actually gives you the source code for anything.
Lec8-110|They worked well for recognizing error, Arabic and Chinese characters.
Lec8-111|They worked well on some simpler benchmarks like CFR ten.
Lec8-113|And traffic sign recognition better than humans.
Lec8-115|And it turns out that's just not enough data to train a network like this.
Lec8-117|That's two orders of magnitude more.
Lec8-118|Then along came AlexNet.
Lec8-122|He just pulled this architecture out of his hat basically.
Lec8-123|But you took the three, red, green, and blue.
Lec8-124|And then he had, I can't see on my slides, these 11 by 11 patches.
Lec8-127|And then he included ReLU units.
Lec8-128|So that was one innovation.
Lec8-129|He also included dropout.
Lec8-130|And so you have these two, basically two networks that they're identical to each other.
Lec8-131|This is actually the figure from the paper.
Lec8-132|The top half is cut off in the paper.
Lec8-133|Must have been trying to squeeze it into the paper because there's a length limit.
Lec8-134|But the network on top is identical to this network.
Lec8-135|And then they communicated various points.
Lec8-137|And then they talk to each other at this layer.
Lec8-140|You put my glasses on it.
Lec8-141|So there's essentially a five layer network.
Lec8-142|It's got the first layer is as these 48 features.
Lec8-143|And there's 55 by 55.
Lec8-144|So if you think of the image coming in as 224 by 22, 24, and you have 11 by 11 kernels.
Lec8-145|On the Stanford web page.
Lec8-147|Then those got max pooled.
Lec8-150|And then two layers of hidden units before the output.
Lec8-151|And ImageNet has 1,000 categories.
Lec8-152|The ImageNet Large Scale Visual Recognition Challenge has 1,000 categories.
Lec8-153|So there's 1,000 softmax outputs.
Lec8-157|A few of those.
Lec8-158|Okay, so now they're very happy because they won the ImageNet Large Scale Visual Recognition Challenge.
Lec8-161|So again, there's sometimes I see 1.2 that says 1.3 million training images, thousand categories, 50,000 test images, and a lot of variation within the images.
Lec8-162|And there's a lot of fine scale categories.
Lec8-163|I sometimes say there's 120 dog breeds, but I've read somewhere else.
Lec8-165|But any network trained on ImageNet is a dog expert, knows a bunch of different dog breeds.
Lec8-166|And so here's and back then, you got they assessed how well you're doing based on whether the right answer was in your top five answers.
Lec8-167|So here, the red means it's correct.
Lec8-168|This is not correct.
Lec8-169|It says reflex camera, which it is it says Polaroid camera, which probably isn't.
Lec8-170|What the label for this was was lens cap.
Lec8-171|So there's just bad labeling to why somebody labeled that lens cap.
Lec8-172|I have no idea.
Lec8-175|But after slug it says zucchini, pretty close.
Lec8-177|And there's a lot of variability in the images in general.
Lec8-178|So sometimes texture matters a lot as in this leopard versus texture mattering.
Lec8-180|Um, I think they meant something else.
Lec8-184|Well, usually they're dark colors.
Lec8-185|Shape is important for bells, but not so important for puzzle pieces.
Lec8-187|So that's between images variability and how much they matter for different categories.
Lec8-188|But even within categories.
Lec8-189|This, how many different breeds of dog here do you think there are? One? Scottish deer hounds, even though here it's sitting next to a grand or whip it or something.
Lec8-190|So these are all Scottish deer hounds.
Lec8-191|And it has to recognize all, put all of them in the same category.
Lec8-197|And then they all started doing deep learning.
Lec8-198|Although there was some resistance.
Lec8-200|But now almost all computer vision is some form of deep learning.
Lec8-202|Ziegler for the forgotten.
Lec8-203|Oops, there goes my, my brain again.
Lec8-205|They took AlexNet architecture and change some of the hyperparameters.
Lec8-208|So sealer was Rob's stomach, which you like me to kill you slowly.
Lec8-210|So it's Google Annette.
Lec8-211|And then Andre neuropathy trained himself for three days and he managed to beat Google.
Lec8-212|And then the next year, ResNet came along.
Lec8-213|And then a combination of Google Annette and resonant.
Lec8-214|And then I don't know who that is.
Lec8-215|Actually keeps getting better and better.
Lec8-217|Sit down all the way down.
Lec8-224|You're supposed to only praise them when they do the right thing, but what happens if he'd never does the right thing? Okay, so AlexNet has eight layers, sorry, I said, I think I said five, but there's five layers of convolutions.
Lec8-225|And then VGG 1919 layers.
Lec8-227|Okay? And they, they do very well at a lot of these computer vision tasks.
Lec8-229|this is recognizing objects in the coco dataset, that's Microsoft's dataset, common images in context.
Lec8-231|So it's getting a lot of these rights.
Lec8-234|She's very occluded, but still it knows there's a person there.
Lec8-235|The mistakes it make makes, makes sense.
Lec8-237|It gets small things like this wine glass and this knife as well as the large things.
Lec8-239|And then Rob student Ziegler started clarify to sell this kind of technology to companies.
Lec8-240|Mostly what they use it for us to tell whether their workers are looking at porn or not.
Lec8-243|Some of those, a lot of those are probably right here.
Lec8-246|But it thinks it's in one of those.
Lec8-247|I don't know which one it is.
Lec8-248|And this one is says Barcelona, Sagrada Familia.
Lec8-249|Anybody know why it says those? Anybody been to Barcelona? No one's been to Barcelona.
Lec8-252|It's a cathedral that's being, had been being built for about the last hundred years.
Lec8-253|So some sides have different, different architecture than others.
Lec8-256|They start with a convolution where you have a learned filter, followed by some non-linearity, followed by spatial pooling of some sort.
Lec8-257|And that gives you a feature map that's the input to the next layer up.
Lec8-258|And we sometimes call that an image.
Lec8-259|It's a kind of deep image in some sense.
Lec8-260|So let's go through the details.
Lec8-261|So the first layer does convolution with these learned filters than a non-linearity and then optional substance sampling.
Lec8-262|So this is gonna be a convolution with a Gabor filter, but you get the idea.
Lec8-263|In fact, I think I explained, I explained Gabor Filters last time.
Lec8-265|So that's what that looks like.
Lec8-267|Then you get a feature map for that particular feature.
Lec8-268|Then you can run a different one over it.
Lec8-269|Get a feature map for that feature.
Lec8-270|And there's many more other ones.
Lec8-271|You could do this exactly what my model of human vision did.
Lec8-273|So hey, I've been doing convolutional networks for a long time.
Lec8-275|It's a local filter in the sense that it's a small patch in the same features computed everywhere.
Lec8-276|So that's the idea of statics statistics.
Lec8-278|So the rectified linear function, remember, for the delta of the hidden units, you can't get rid of the slope term.
Lec8-279|So if you have a rectified linear function like this, the slope is either one or zero.
Lec8-280|That makes it really easy to compute.
Lec8-281|If you're on the positive side, you're going to pass back the Delta without change because the slope is one.
Lec8-283|But there are a lot of other nonlinearities you might use.
Lec8-285|So here's cello and ALU.
Lec8-287|I think Google came up with swish, which is x times the sigmoid.
Lec8-288|So it looks a lot like a ReLu, but it's got a little bit of negative here.
Lec8-289|That's part of the reason for parametric ReLU, remember we said positive and all positive is bad.
Lec8-290|This gives you some negative from this.
Lec8-291|And this a can be learned.
Lec8-292|That's the slope through the magic of gradient descent.
Lec8-293|So here's the, here's the thing people strive for.
Lec8-295|Remember I said you were, if you were this much better, well, this special unit here improves classification accuracy by 0.9% and 0.6%.
Lec8-296|This little tiny differences.
Lec8-298|Fact, why don't I just for fun? I'll stick in short other lecture here.
Lec8-299|Where is the Zheng? He's saying? Where's my search function? Arenas? Where? Where is PowerPoint? There we go.
Lec8-300|I don't see it.
Lec8-303|I'll give it to you later.
Lec8-304|But Richard Zang of Adobe has come up with a better way of doing pooling, that, that works better.
Lec8-307|So what this pooling does is give you some invariance to smaller transformations.
Lec8-308|So at nips in 2010, this is an optimization where you take a filter somewhere in the network.
Lec8-309|This is probably like two layers in.
Lec8-310|And you change the input without changing the output of the filter.
Lec8-311|So that this is what this filter is invariant to.
Lec8-312|So it's a little movie.
Lec8-314|And this second one is invariant to roughly where it is in the image where this edge is.
Lec8-315|Okay? These will see a lot more of in the next, next lecture after this one.
Lec8-316|But you can see anything here.
Lec8-317|So these are nine possible inputs that drive this filter the most.
Lec8-318|These are nine for another filter, another filter, another filter.
Lec8-319|So it's relatively invariant across these inputs.
Lec8-321|So we have pixels are features that are filtered with the learned dictionary, some sort of non-linearity, some kind of spatial pooling, which is optional.
Lec8-323|Then that goes up to the next layer.
Lec8-326|So that's network and network.
Lec8-327|So there's standard way to do pooling is to do max pooling.
Lec8-328|So you have a two-by-two patch of the previous, previous feature map, and you pick the maximum activation.
Lec8-333|And you're going to use the same parameters everywhere for that.
Lec8-334|Then there's batch norm.
Lec8-335|So this is interesting because it shows the results of doing batch norm.
Lec8-337|And this line here is just using batch norm.
Lec8-339|Then the dark blue line is batch storm where you've cranked the learning rate up 30 times.
Lec8-340|And it's tolerant to that.
Lec8-342|The other thing is even a sigmoid, which we know is a bad.
Lec8-343|I'm running out of battery.
Lec8-344|The sigmoid, which we know is not a very good activation function for backprop purposes.
Lec8-345|Even that does pretty well.
Lec8-346|It never reaches inception levels, but that's the red line.
Lec8-348|So that's the story of colonnettes.
Lec8-349|Time for clicker question or three, or four.
Lec8-352|Convolution to learn filters option.
Lec8-359|I would say this is sort of 75%, correct.
Lec8-361|We got 63 votes.
Lec8-362|Anybody else going, going, going gone, okay, I'm going to give it to you.
Lec8-363|48% of you said a, 2%, one person said B, 27% said C, and 24% said D, and 0% said E.
Lec8-366|That's 17 of you said that you would get this wrong on a test.
Lec8-367|I would give both a and D to you because a is the right answer.
Lec8-370|So that's that's okay.
Lec8-376|So that's a is what I would consider the best answer here.
Lec8-378|Processing each pixel of an image with a square convolutional kernel containing all ones would result in a, a feature map brighter than the image, be a feature map.
Lec8-379|I'm resembling a blurred version of the image.
Lec8-380|See the feature map resembling a sharper version of the image.
Lec8-382|I have to start it again, sorry.
Lec8-383|Okay, so it's available for input now.
Lec8-386|Only 42 votes so far I know there's more of you out there.
Lec8-388|I know there's seven more of you.
Lec8-389|Five more like for more of you that didn't vote this time yet.
Lec8-391|Going up and vote.
Lec8-393|Only 60 this time.
Lec8-398|I'll give you a couple of minutes to talk to your neighbor.
Lec8-400|Either you could move back easily.
Lec8-402|You're still chattering, but I'll let you try again.
Lec8-405|What happened here? Okay.
Lec8-410|But only 43, 45, there's about 15 more people.
Lec8-413|Is everybody pushed their button because I'm only getting 50 out of the 60.
Lec8-414|Some of you there are some delay going on.
Lec8-415|Now we're up to 59 606-160-2603.
Lec8-419|Going going going gone.
Lec8-421|So what do you think about a why? Right? You're just adding up the pixels.
Lec8-424|What about B? Why? Yeah.
Lec8-428|What about C? No, because B is right.
Lec8-431|So 59% of you said E, 8% said a, 29% said B.
Lec8-434|Okay? Alright, 11 or two more here.
Lec8-437|Crossing each region of an image with the following kernel would be useful for horizontal edge detection, vertical edge detection, horizontal blur, vertical blurred, gradient detection.
Lec8-442|Still not up to 60 as we were in the previous ones.
Lec8-444|Make sure you vote early and often.
Lec8-445|I want you to participate in all the clicker questions at each session, not just one or two.
Lec8-449|Okay, you're going to lose out.
Lec8-450|Going up, one more, going going gone.
Lec8-455|So you have some dark pixels and some light pixels on one side that match up with one-to-one, you're gonna get a big response from that.
Lec8-456|And the dark pixels, which are smaller numbers, you're just going to add negatives of those.
Lec8-459|So it is kind of a gradient in the brightness that is detecting in one direction but not the other.
Lec8-460|Okay? So this is, this, this is a standard kind of kernel that they use when they were designing them.
Lec8-461|So some of you said C, D, and E, and some of you said a.
Lec8-462|For those of you who said those, Does it make sense now? So bright pixels are going to have high numbers, dark pixels are going to have low numbers.
Lec8-465|You could have a flat thing, they'll get a good response.
Lec8-466|But you're not taking into account the difference in the pixels from one side to the other.
Lec8-468|Oh, I guess I didn't keep the answer there.
Lec8-469|It should be B though.
Lec8-478|There's a pretty high entropy distribution here.
Lec8-480|Talk to your neighbor for a couple of minutes.
Lec8-496|You're being a good boy.
Lec8-497|You being a good point? Yes.
Lec8-502|But I think everything is okay.
Lec8-506|Why do you haven't voted yet? Herding, herding, going, going going.
Lec8-507|And there's like eight of you who haven't voted.
Lec8-508|Okay, keep going, going, going, going, gone.
Lec8-510|So the answer is none of these and I should have taken out the motivates.
Lec8-511|The need for neural architecture and pooling.
Lec8-512|Convolution by itself isn't invariant anything.
Lec8-513|Because if you move the thing underneath it, that, that's imaged translation, right? That's going to change all feature map.
Lec8-515|Not pulling alone, not convolutional loan.
Lec8-516|And since a lot of you said all of these are a only a few of you said E Question.
Lec8-518|Well, it makes it a little translation invariant.
Lec8-519|And again, the deeper you go, the more invariant it is.
Lec8-520|If you have two-by-two pooling and you have a feature here that it's responding to.
Lec8-522|Right? That makes sense.
Lec8-524|And then later in the network, you're pulling together larger things.
Lec8-525|And so you're more invariant to larger things.
Lec8-526|Like no, no, sorry.
Lec8-529|But size is something you have to learn from lots of examples.
Lec8-531|Usually you're going to get rid of that by some normalization or batch norm partway through the network.
Lec8-535|Before I go on there there were so many of you that got that wrong.
Lec8-536|More questions about the last thing.
Lec8-537|Because I'm sure you're still confused.
Lec8-538|We shouldn't we shouldn't I shouldn't go on before.
Lec8-541|This guy is going to have a bigger set of activations.
Lec8-542|This guy is going to have a bigger set of activations and you have to recognize the shape of that.
Lec8-544|Where are you going to do that? Yeah.
Lec8-548|Have three versions of it and share some of the weights between them.
Lec8-549|Right? Although you can't share all the weights because one is bigger than the other, right? So what they do is this thing called dilated convolutions.
Lec8-553|Okay, needed darker one here.
Lec8-556|Okay? So let's say we have these numbers here.
Lec8-557|I'm just making them easy to remember.
Lec8-558|Dilated convolution would use the same numbers, but it would put them here.
Lec8-559|And then zeros everywhere else.
Lec8-560|So it's detecting roughly the same feature at a different scale now.
Lec8-561|So that's called a dilated convolution.
Lec8-562|That's supposed to be a zero.
Lec8-564|And you can do that another step two.
Lec8-567|That's just what people do.
Lec8-568|I mean, if it were me, I would like do an average between them.
Lec8-570|This is what people do.
Lec8-571|And that's more computation.
Lec8-573|I think the previous one.
Lec8-577|You could have three minus ones.
Lec8-580|Brandon's making new clicker questions for me so you can't just look at last year's things and answer them all.
Lec8-584|So imagine these are one current, all a set of weights and I just numbered the weights to make it easier to see what's happening.
Lec8-587|For as long as it's odd.
Lec8-591|But the bias is a learned number.
Lec8-593|It's not going to vary with different brightness, but yeah, something like batch normal will help with that.
Lec8-597|I don't know if you can see that.
Lec8-598|I'm shifting the picture over, right? So at the next layer up, I'm gonna be shifting the feature over.
Lec8-599|Alright, the activation maps can change.
Lec8-604|Could you could just put over it.
Lec8-609|More questions about that.
Lec8-612|Luckily, they quit before they got their PhD.
Lec8-614|You're targeting about this.
Lec8-615|So maximum seems to work well.
Lec8-616|It's cheap to compute.
Lec8-621|Maybe this is just to pour.
Lec8-622|Oh, you can still.
Lec8-629|It's probably just a bad, bad example.
Lec8-630|Do you pull the absolute value? You take the maximum.
Lec8-633|Yeah, I don't know the answer to that question.
Lec8-636|Now we're letting the network learn the features.
Lec8-637|And so we don't have to learn makeup features anymore.
Lec8-638|What we're doing now is making up architectures.
Lec8-641|So we have to design the architecture.
Lec8-642|We could do that through genetic algorithms.
Lec8-644|So we could evolve good architectures.
Lec8-645|So we'd have a bunch of architectures.
Lec8-646|We'd see which ones perform the best and then kinda mix them through crossover.
Lec8-651|The number of layers, number of feature maps, the parameters of the feature maps.
Lec8-652|Cross-validation is one way to try and see which each architecture you get.
Lec8-653|You can test it by cross-validation.
Lec8-655|A better idea though, is to do.
Lec8-657|So imagine that this is my, you know, maybe this is something like number of layers.
Lec8-658|And so it wouldn't go negative.
Lec8-659|This is the number of features in each layer.
Lec8-660|You could do a grid search, but that, you know, the more parameters you have, the more hyperparameters you have to set.
Lec8-663|So what Bengio suggests is doing a random search, just randomly picking spots in this thing, and then seeing which directions make the most difference and focus your search on those directions.
Lec8-665|But you don't have to do that.
Lec8-666|There's already some best practices.
Lec8-669|And this leads to the same size.
Lec8-670|Feature maps is the image.
Lec8-671|And again, if you have like features that with a stride that might go off the end of the image.
Lec8-674|In AlexNet he had one place where he had convolution, convolution and no max pooling in-between.
Lec8-675|If you do just two convolutions, you're basically doing a kind of linear combination of things.
Lec8-676|If you use max and you're using ReLu, you can do XOR if you have two layers of cooling, two layers before pooling.
Lec8-677|And I for I figured this out once.
Lec8-678|I don't remember what I figured out.
Lec8-681|You can use sampling to change the size of your feature maps.
Lec8-683|And that reduces the computation you're doing because you've changed the size of your feature maps.
Lec8-686|Use a pre-trained version of the current best ImageNet network to extract features and then bolt-on and new softmax layer for your problem and fine tune it.
Lec8-687|So this is called transfer learning.
Lec8-688|So you take, and for a couple of years and CV PR, there were a lot of papers about transfer learning.
Lec8-689|So we did some of that.
Lec8-691|But we took image, we took the AlexNet and surge velocity.
Lec8-692|You used to be a computer vision person here and then he went to Cornell Tech.
Lec8-693|His group developed a set of of urban tribes images.
Lec8-694|So they had pictures of people wearing cowboy outfits during line dancing.
Lec8-695|You had pictures of motorcycle gangs, had pictures of golf, people hanging out together.
Lec8-696|And those are different.
Lec8-699|And they turned out to be hard because it's complicated.
Lec8-701|You get a set of features near the output.
Lec8-702|You bolt-on a new classifier on that.
Lec8-705|Immediately, we did better.
Lec8-706|We got 67%, correct.
Lec8-707|My student did a few more tweaks and got it up to 68 or 70.
Lec8-711|And you take in the painting and you say which artists to do is that worked pretty well.
Lec8-713|And Caltech 256, really, what it's good for is if you have a problem that doesn't have a very large training set.
Lec8-717|And then you can immediately get state of the art results.
Lec8-718|But you have to be careful.
Lec8-719|So I had a student who was trying to recognize words.
Lec8-720|We actual printed words.
Lec8-721|So we had like hundreds of fonts.
Lec8-722|We had like 800 words from basic English.
Lec8-723|Basic English is 100 word vocabulary that if you know those 800 words, you can hold a conversation, their actual stories written in basic English.
Lec8-725|Tilted them, made them bigger and smaller, et cetera, to get thousands of examples.
Lec8-726|And then tried AlexNet on it, and it failed miserably.
Lec8-727|Then we tried taking off from an earlier layer than the last layer formed a little better.
Lec8-728|In fact, the earlier we went the better it was.
Lec8-729|And that's because ImageNet has this category where it takes things like signs with text on them and maps them into one category.
Lec8-730|That means it's ignoring the difference between the writing on the signs.
Lec8-731|So it's taking all this writing, kind of glomming it together into one representation.
Lec8-733|We just trained to basically Lynette 5.0 from scratch and it worked great.
Lec8-735|So it can't, it doesn't always work to do transfer learning.
Lec8-737|Fine tuning means that you actually backpropagate into the pre-trained network and tune its features for your problem.
Lec8-738|But the great thing about transfer learning again is you can take a fairly small dataset and do well on it.
Lec8-744|There should be some similarity the problems we've given you so far.
Lec8-748|We last quarter I had 180 students and ten TAs and Tutors.
Lec8-750|I don't mean like having our subject.
Lec8-754|But the written probably, yeah.
Lec8-755|The the issue with that is.
Lec8-757|And so people could get the answers to the next year and sell them to their friends or their enemies.
